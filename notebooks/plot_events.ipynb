{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d65fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.subplots as sp\n",
    "import pytz\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64c241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PSG_FILE = '../data/bishkek_csr/03_train_ready/nasal_files/05-04-2025_nasal.csv'\n",
    "RESPECK_FILE = '../data/bishkek_csr/03_train_ready/respeck/05-04-2025_respeck.csv'\n",
    "LABELS_FILE = '../data/bishkek_csr/03_train_ready/event_exports/05-04-2025_event_export.csv'\n",
    "\n",
    "respeck_df = pd.read_csv(RESPECK_FILE)\n",
    "respeck_df['timestamp'] = pd.to_datetime(respeck_df['alignedTimestamp'], unit='ms')\n",
    "tz = pytz.timezone('Asia/Bishkek')\n",
    "respeck_df['timestamp'] = respeck_df['timestamp'].dt.tz_localize('UTC').dt.tz_convert(tz)\n",
    "respeck_df.set_index('timestamp', inplace=True)\n",
    "\n",
    "psg_df = pd.read_csv(PSG_FILE)\n",
    "psg_df['timestamp'] = pd.to_datetime(psg_df['UnixTimestamp'], unit='ms')\n",
    "tz = pytz.timezone('Asia/Bishkek')\n",
    "psg_df['timestamp'] = psg_df['timestamp'].dt.tz_localize('UTC').dt.tz_convert(tz)\n",
    "psg_df.set_index('timestamp', inplace=True)\n",
    "\n",
    "labels_df = pd.read_csv(LABELS_FILE)\n",
    "labels_df['timestamp'] = pd.to_datetime(labels_df['UnixTimestamp'], unit='ms')\n",
    "tz = pytz.timezone('Asia/Bishkek')\n",
    "labels_df['timestamp'] = labels_df['timestamp'].dt.tz_localize('UTC').dt.tz_convert(tz)\n",
    "labels_df.set_index('timestamp', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3a3cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.subplots as sp\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "def plot_single_event(respeck_df, psg_df, event_time, event_row, buffer_minutes=2):\n",
    "    \"\"\"\n",
    "    Plot a single event with PSG and Respeck data in separate subplots.\n",
    "    \n",
    "    Parameters:\n",
    "    - respeck_df: DataFrame with Respeck sensor data\n",
    "    - psg_df: DataFrame with PSG data  \n",
    "    - event_time: Timestamp of the event\n",
    "    - event_row: Series containing event details\n",
    "    - buffer_minutes: Minutes to show before and after event\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create figure with 2 subplots\n",
    "    fig = sp.make_subplots(\n",
    "        rows=2, cols=1,\n",
    "        subplot_titles=[\n",
    "            f'{event_row.get(\"Event\", \"Event\")} at {event_time.strftime(\"%Y-%m-%d %H:%M:%S\")}',\n",
    "            None\n",
    "        ],\n",
    "        vertical_spacing=0.1\n",
    "    )\n",
    "    \n",
    "    buffer_td = timedelta(minutes=buffer_minutes)\n",
    "    start_time = event_time - buffer_td\n",
    "    end_time = event_time + buffer_td\n",
    "    \n",
    "    # Parse duration\n",
    "    duration = 30.0\n",
    "    duration_val = event_row.get('Duration')\n",
    "    if duration_val is not None:\n",
    "        try:\n",
    "            duration = float(str(duration_val).replace(',', '.'))\n",
    "        except (ValueError, TypeError):\n",
    "            pass\n",
    "    \n",
    "    event_end_time = event_time + timedelta(seconds=duration)\n",
    "    \n",
    "    # Filter data\n",
    "    respeck_window = respeck_df[start_time:end_time]\n",
    "    psg_window = psg_df[start_time:end_time]\n",
    "    \n",
    "    # Plot PSG data (top subplot)\n",
    "    if not psg_window.empty and 'Resp nasal' in psg_window.columns:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=psg_window.index, \n",
    "            y=psg_window['Resp nasal'], \n",
    "            mode='lines', \n",
    "            name='PSG Nasal Resp',\n",
    "            line=dict(color='blue', width=2), \n",
    "            opacity=0.8\n",
    "        ), row=1, col=1)\n",
    "    \n",
    "    # Plot Respeck data (bottom subplot)\n",
    "    if not respeck_window.empty:\n",
    "        if 'breathingSignal' in respeck_window.columns:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=respeck_window.index, \n",
    "                y=respeck_window['breathingSignal'], \n",
    "                mode='lines',\n",
    "                name='Respeck Breathing', \n",
    "                line=dict(color='orange', width=2),\n",
    "                opacity=0.9\n",
    "            ), row=2, col=1)\n",
    "        \n",
    "        if 'x' in respeck_window.columns:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=respeck_window.index, \n",
    "                y=respeck_window['x'], \n",
    "                mode='lines',\n",
    "                name='Respeck X', \n",
    "                line=dict(color='red', width=1),\n",
    "                opacity=0.7\n",
    "            ), row=2, col=1)\n",
    "        \n",
    "        if 'y' in respeck_window.columns:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=respeck_window.index, \n",
    "                y=respeck_window['y'], \n",
    "                mode='lines',\n",
    "                name='Respeck Y', \n",
    "                line=dict(color='green', width=1),\n",
    "                opacity=0.7\n",
    "            ), row=2, col=1)\n",
    "\n",
    "        if 'z' in respeck_window.columns:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=respeck_window.index, \n",
    "                y=respeck_window['z'], \n",
    "                mode='lines',\n",
    "                name='Respeck Z', \n",
    "                line=dict(color='magenta', width=1),\n",
    "                opacity=0.7\n",
    "            ), row=2, col=1)\n",
    "\n",
    "    # Add event markers to both subplots\n",
    "    event_display_name = event_row.get('Event', 'Event')\n",
    "    for r in [1, 2]:\n",
    "        fig.add_vrect(\n",
    "            x0=event_time, x1=event_end_time, \n",
    "            fillcolor=\"red\", opacity=0.2, \n",
    "            layer=\"below\", line_width=0,\n",
    "            annotation_text=f\"{event_display_name} ({duration:.1f}s)\" if r == 1 else \"\",\n",
    "            annotation_position=\"top left\", \n",
    "            row=r, col=1\n",
    "        )\n",
    "        fig.add_vline(\n",
    "            x=event_time, \n",
    "            line_dash=\"dash\", line_color=\"red\", line_width=1.5,\n",
    "            opacity=0.8, \n",
    "            row=r, col=1\n",
    "        )\n",
    "\n",
    "    # Set x-axis ranges and link them\n",
    "    fig.update_xaxes(range=[start_time, end_time], row=1, col=1)\n",
    "    fig.update_xaxes(range=[start_time, end_time], matches='x', row=2, col=1)\n",
    "\n",
    "    # Update axis titles\n",
    "    fig.update_yaxes(title_text=\"PSG Nasal Resp.\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Respeck Data\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"Time\", row=2, col=1)\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        title_text=f\"Analysis of {event_display_name}\",\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1)\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def plot_events_batch(respeck_df, psg_df, osa_events_df, buffer_minutes=2, events_per_plot=5):\n",
    "    \"\"\"\n",
    "    Plot events in batches to avoid subplot limit issues.\n",
    "    \n",
    "    Parameters:\n",
    "    - respeck_df: DataFrame with Respeck sensor data\n",
    "    - psg_df: DataFrame with PSG data\n",
    "    - osa_events_df: DataFrame with event labels\n",
    "    - buffer_minutes: Minutes to show before and after each event\n",
    "    - events_per_plot: Number of events to show per plot\n",
    "    \"\"\"\n",
    "    \n",
    "    if osa_events_df.empty:\n",
    "        print(\"No events found in the data.\")\n",
    "        return\n",
    "    \n",
    "    n_events = len(osa_events_df)\n",
    "    n_batches = math.ceil(n_events / events_per_plot)\n",
    "    \n",
    "    print(f\"Found {n_events} events. Creating {n_batches} batch plots...\")\n",
    "    \n",
    "    for batch_idx in range(n_batches):\n",
    "        start_idx = batch_idx * events_per_plot\n",
    "        end_idx = min(start_idx + events_per_plot, n_events)\n",
    "        batch_events = osa_events_df.iloc[start_idx:end_idx]\n",
    "        \n",
    "        n_events_in_batch = len(batch_events)\n",
    "        \n",
    "        # Create subplot titles\n",
    "        subplot_titles = []\n",
    "        for i, (event_time, event_row) in enumerate(batch_events.iterrows()):\n",
    "            event_display_name = event_row.get('Event', 'Event')\n",
    "            subplot_titles.append(f'{event_display_name} #{start_idx + i + 1} at {event_time.strftime(\"%H:%M:%S\")}')\n",
    "            subplot_titles.append(None)\n",
    "        \n",
    "        # Calculate appropriate vertical spacing\n",
    "        total_rows = n_events_in_batch * 2\n",
    "        max_spacing = 1.0 / (total_rows - 1) if total_rows > 1 else 0.1\n",
    "        vertical_spacing = min(0.03, max_spacing * 0.8)  # Use 80% of max allowed\n",
    "        \n",
    "        # Create subplots\n",
    "        fig = sp.make_subplots(\n",
    "            rows=total_rows, cols=1,\n",
    "            subplot_titles=subplot_titles,\n",
    "            vertical_spacing=vertical_spacing\n",
    "        )\n",
    "        \n",
    "        buffer_td = timedelta(minutes=buffer_minutes)\n",
    "        \n",
    "        for idx, (event_time, event_row) in enumerate(batch_events.iterrows()):\n",
    "            psg_row = idx * 2 + 1\n",
    "            respeck_row = idx * 2 + 2\n",
    "            \n",
    "            start_time = event_time - buffer_td\n",
    "            end_time = event_time + buffer_td\n",
    "            \n",
    "            # Parse duration\n",
    "            duration = 30.0\n",
    "            duration_val = event_row.get('Duration')\n",
    "            if duration_val is not None:\n",
    "                try:\n",
    "                    duration = float(str(duration_val).replace(',', '.'))\n",
    "                except (ValueError, TypeError):\n",
    "                    pass\n",
    "            \n",
    "            event_end_time = event_time + timedelta(seconds=duration)\n",
    "            \n",
    "            # Filter data\n",
    "            respeck_window = respeck_df[start_time:end_time]\n",
    "            psg_window = psg_df[start_time:end_time]\n",
    "            \n",
    "            # Plot PSG data\n",
    "            if not psg_window.empty and 'Resp nasal' in psg_window.columns:\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=psg_window.index, y=psg_window['Resp nasal'], mode='lines', \n",
    "                    name='PSG Nasal Resp', line=dict(color='blue', width=2), \n",
    "                    opacity=0.8, showlegend=(batch_idx == 0 and idx == 0)\n",
    "                ), row=psg_row, col=1)\n",
    "            \n",
    "            # Plot Respeck data\n",
    "            if not respeck_window.empty:\n",
    "                if 'breathingSignal' in respeck_window.columns:\n",
    "                    fig.add_trace(go.Scatter(\n",
    "                        x=respeck_window.index, y=respeck_window['breathingSignal'], \n",
    "                        mode='lines', name='Respeck Breathing', \n",
    "                        line=dict(color='orange', width=2), opacity=0.9, \n",
    "                        showlegend=(batch_idx == 0 and idx == 0)\n",
    "                    ), row=respeck_row, col=1)\n",
    "                \n",
    "                for col_name, color in [('x', 'red'), ('y', 'green'), ('z', 'magenta')]:\n",
    "                    if col_name in respeck_window.columns:\n",
    "                        fig.add_trace(go.Scatter(\n",
    "                            x=respeck_window.index, y=respeck_window[col_name], \n",
    "                            mode='lines', name=f'Respeck {col_name.upper()}', \n",
    "                            line=dict(color=color, width=1), opacity=0.7, \n",
    "                            showlegend=(batch_idx == 0 and idx == 0)\n",
    "                        ), row=respeck_row, col=1)\n",
    "\n",
    "            # Add event markers\n",
    "            event_display_name = event_row.get('Event', 'Event')\n",
    "            for r in [psg_row, respeck_row]:\n",
    "                fig.add_vrect(\n",
    "                    x0=event_time, x1=event_end_time, fillcolor=\"red\", opacity=0.2, \n",
    "                    layer=\"below\", line_width=0,\n",
    "                    annotation_text=f\"{event_display_name} ({duration:.1f}s)\" if r == psg_row else \"\",\n",
    "                    annotation_position=\"top left\", row=r, col=1\n",
    "                )\n",
    "                fig.add_vline(\n",
    "                    x=event_time, line_dash=\"dash\", line_color=\"red\", line_width=1.5,\n",
    "                    opacity=0.8, row=r, col=1\n",
    "                )\n",
    "\n",
    "            # Set x-axis ranges and link them\n",
    "            fig.update_xaxes(range=[start_time, end_time], row=psg_row, col=1)\n",
    "            fig.update_xaxes(range=[start_time, end_time], matches=f'x{psg_row}', row=respeck_row, col=1)\n",
    "\n",
    "            # Update axis titles\n",
    "            fig.update_yaxes(title_text=\"PSG Nasal\", row=psg_row, col=1)\n",
    "            fig.update_yaxes(title_text=\"Respeck\", row=respeck_row, col=1)\n",
    "            if idx == n_events_in_batch - 1:  # Only add x-axis title to last subplot\n",
    "                fig.update_xaxes(title_text=\"Time\", row=respeck_row, col=1)\n",
    "\n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            height=max(800, 300 * n_events_in_batch),\n",
    "            title_text=f\"Events Batch {batch_idx + 1}/{n_batches} (Events {start_idx + 1}-{end_idx})\",\n",
    "            legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1)\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "\n",
    "\n",
    "def plot_events_individual(respeck_df, psg_df, osa_events_df, buffer_minutes=2, max_events=None):\n",
    "    \"\"\"\n",
    "    Plot each event as a separate individual plot.\n",
    "    \n",
    "    Parameters:\n",
    "    - respeck_df: DataFrame with Respeck sensor data\n",
    "    - psg_df: DataFrame with PSG data\n",
    "    - osa_events_df: DataFrame with event labels\n",
    "    - buffer_minutes: Minutes to show before and after each event\n",
    "    - max_events: Maximum number of events to plot (None for all)\n",
    "    \"\"\"\n",
    "    \n",
    "    if osa_events_df.empty:\n",
    "        print(\"No events found in the data.\")\n",
    "        return\n",
    "    \n",
    "    n_events = len(osa_events_df)\n",
    "    if max_events is not None:\n",
    "        n_events = min(n_events, max_events)\n",
    "        osa_events_df = osa_events_df.iloc[:n_events]\n",
    "    \n",
    "    print(f\"Creating {n_events} individual plots...\")\n",
    "    \n",
    "    for idx, (event_time, event_row) in enumerate(osa_events_df.iterrows()):\n",
    "        print(f\"Plotting event {idx + 1}/{n_events}\")\n",
    "        plot_single_event(respeck_df, psg_df, event_time, event_row, buffer_minutes)\n",
    "\n",
    "        # Option 1: Plot events in batches (recommended for many events)\n",
    "# plot_events_batch(respeck_df, psg_df, labels_df[labels_df['Event'] == 'Obstructive Apnea'], \n",
    "#                   buffer_minutes=15, events_per_plot=3)\n",
    "\n",
    "# Option 2: Plot each event individually (good for detailed analysis)\n",
    "# plot_events_individual(respeck_df, psg_df, labels_df[labels_df['Event'] == 'Obstructive Apnea'], \n",
    "#                        buffer_minutes=15, max_events=5)\n",
    "\n",
    "# Option 3: Plot a single specific event\n",
    "# event_idx = 0  # First event\n",
    "# events = labels_df[labels_df['Event'] == 'Obstructive Apnea']\n",
    "# if not events.empty:\n",
    "#     event_time, event_row = list(events.iterrows())[event_idx]\n",
    "#     plot_single_event(respeck_df, psg_df, event_time, event_row, buffer_minutes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c545726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from calculateContinuousBreathFeatures import *\n",
    "from respiratoryFeatures import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e271e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def load_and_process_events(csv_file_path):\n",
    "    \"\"\"\n",
    "    Load and process the event CSV file, handling the comma decimal format.\n",
    "    \n",
    "    Parameters:\n",
    "    - csv_file_path: Path to the CSV file\n",
    "    \n",
    "    Returns:\n",
    "    - events_df: Processed DataFrame with proper timestamps and durations\n",
    "    \"\"\"\n",
    "    # Load the CSV\n",
    "    events_df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    print(f\"Loaded {len(events_df)} events from CSV\")\n",
    "    print(f\"Event types: {events_df['Event'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # Convert Unix timestamp to datetime\n",
    "    events_df['start_time'] = pd.to_datetime(events_df['UnixTimestamp'], unit='ms')\n",
    "    \n",
    "    # Parse duration (handle comma as decimal separator)\n",
    "    events_df['duration_seconds'] = events_df['Duration'].str.replace(',', '.').astype(float)\n",
    "    \n",
    "    # Calculate end time\n",
    "    events_df['end_time'] = events_df['start_time'] + pd.to_timedelta(events_df['duration_seconds'], unit='s')\n",
    "    \n",
    "    # Sort by start time\n",
    "    events_df = events_df.sort_values('start_time').reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Time range: {events_df['start_time'].min()} to {events_df['end_time'].max()}\")\n",
    "    print(f\"Duration range: {events_df['duration_seconds'].min():.1f}s to {events_df['duration_seconds'].max():.1f}s\")\n",
    "    \n",
    "    return events_df\n",
    "\n",
    "def create_labeled_respiratory_data(respeck_df, events_df, breathing_col='breathingSignal'):\n",
    "    \"\"\"\n",
    "    Create a labeled version of respiratory data with precise event timing.\n",
    "    \n",
    "    Parameters:\n",
    "    - respeck_df: DataFrame with respiratory data (must have datetime index)\n",
    "    - events_df: Processed events DataFrame with start_time, end_time columns\n",
    "    - breathing_col: Name of the breathing signal column\n",
    "    \n",
    "    Returns:\n",
    "    - labeled_df: DataFrame with respiratory data and event labels\n",
    "    \"\"\"\n",
    "    print(\"Creating labeled respiratory data...\")\n",
    "    \n",
    "    # Ensure respeck_df has datetime index\n",
    "    if not isinstance(respeck_df.index, pd.DatetimeIndex):\n",
    "        print(\"Converting respeck_df index to datetime...\")\n",
    "        respeck_df.index = pd.to_datetime(respeck_df.index)\n",
    "    \n",
    "    # Create a copy of respiratory data\n",
    "    labeled_df = respeck_df.copy()\n",
    "    \n",
    "    # Initialize all as 'Normal'\n",
    "    labeled_df['event_label'] = 'Normal'\n",
    "    labeled_df['event_duration'] = np.nan\n",
    "    labeled_df['event_id'] = np.nan\n",
    "    \n",
    "    # Check time overlap\n",
    "    respeck_start = labeled_df.index.min()\n",
    "    respeck_end = labeled_df.index.max()\n",
    "    events_start = events_df['start_time'].min()\n",
    "    events_end = events_df['end_time'].max()\n",
    "    \n",
    "    print(f\"Respeck data: {respeck_start} to {respeck_end}\")\n",
    "    print(f\"Events data: {events_start} to {events_end}\")\n",
    "    \n",
    "    # Find overlapping events\n",
    "    overlapping_events = events_df[\n",
    "        (events_df['end_time'] >= respeck_start) & \n",
    "        (events_df['start_time'] <= respeck_end)\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"Found {len(overlapping_events)} events overlapping with respiratory data\")\n",
    "    \n",
    "    # Label each event period\n",
    "    events_applied = 0\n",
    "    for idx, event in overlapping_events.iterrows():\n",
    "        # Find respiratory data points within this event\n",
    "        event_mask = (\n",
    "            (labeled_df.index >= event['start_time']) & \n",
    "            (labeled_df.index <= event['end_time'])\n",
    "        )\n",
    "        \n",
    "        data_points_in_event = event_mask.sum()\n",
    "        \n",
    "        if data_points_in_event > 0:\n",
    "            # Apply event label to this time period\n",
    "            labeled_df.loc[event_mask, 'event_label'] = event['Event']\n",
    "            labeled_df.loc[event_mask, 'event_duration'] = event['duration_seconds']\n",
    "            labeled_df.loc[event_mask, 'event_id'] = idx\n",
    "            events_applied += 1\n",
    "            \n",
    "            if events_applied <= 5:  # Show first 5 for debugging\n",
    "                print(f\"  Event {idx}: {event['Event']} at {event['start_time']} \"\n",
    "                      f\"({data_points_in_event} data points)\")\n",
    "    \n",
    "    print(f\"Successfully applied {events_applied} events to respiratory data\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    label_counts = labeled_df['event_label'].value_counts()\n",
    "    print(f\"\\nLabel distribution:\")\n",
    "    for label, count in label_counts.items():\n",
    "        percentage = (count / len(labeled_df)) * 100\n",
    "        print(f\"  {label}: {count:,} samples ({percentage:.1f}%)\")\n",
    "    \n",
    "    return labeled_df\n",
    "\n",
    "def analyze_event_coverage(labeled_df, events_df):\n",
    "    \"\"\"\n",
    "    Analyze how well events are covered in the respiratory data.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Event Coverage Analysis ===\")\n",
    "    \n",
    "    # Get unique events that were actually applied\n",
    "    applied_events = labeled_df[labeled_df['event_label'] != 'Normal']['event_id'].unique()\n",
    "    applied_events = applied_events[~np.isnan(applied_events)]\n",
    "    \n",
    "    print(f\"Events successfully applied: {len(applied_events)} out of {len(events_df)}\")\n",
    "    \n",
    "    # Analyze coverage by event type\n",
    "    coverage_stats = []\n",
    "    for event_type in events_df['Event'].unique():\n",
    "        type_events = events_df[events_df['Event'] == event_type]\n",
    "        applied_type_events = events_df.loc[applied_events][\n",
    "            events_df.loc[applied_events]['Event'] == event_type\n",
    "        ]\n",
    "        \n",
    "        coverage_stats.append({\n",
    "            'Event_Type': event_type,\n",
    "            'Total_Events': len(type_events),\n",
    "            'Applied_Events': len(applied_type_events),\n",
    "            'Coverage_Rate': len(applied_type_events) / len(type_events) * 100 if len(type_events) > 0 else 0,\n",
    "            'Total_Duration': type_events['duration_seconds'].sum(),\n",
    "            'Applied_Duration': applied_type_events['duration_seconds'].sum() if len(applied_type_events) > 0 else 0\n",
    "        })\n",
    "    \n",
    "    coverage_df = pd.DataFrame(coverage_stats)\n",
    "    coverage_df = coverage_df.sort_values('Coverage_Rate', ascending=False)\n",
    "    \n",
    "    print(\"\\nCoverage by event type:\")\n",
    "    print(coverage_df.to_string(index=False))\n",
    "    \n",
    "    return coverage_df\n",
    "\n",
    "def extract_features_by_precise_labels(labeled_df, feature_extraction_func, \n",
    "                                     segment_length_minutes=5):\n",
    "    \"\"\"\n",
    "    Extract features using precise event labels (no time windows needed).\n",
    "    \n",
    "    Parameters:\n",
    "    - labeled_df: DataFrame with precise event labels\n",
    "    - feature_extraction_func: Your respiratory feature extraction function\n",
    "    - segment_length_minutes: Length of segments to analyze\n",
    "    \n",
    "    Returns:\n",
    "    - features_list: List of feature dictionaries\n",
    "    \"\"\"\n",
    "    print(f\"Extracting features using precise labels...\")\n",
    "    \n",
    "    all_features = []\n",
    "    segment_length = pd.Timedelta(minutes=segment_length_minutes)\n",
    "    \n",
    "    # Get all unique event periods\n",
    "    event_periods = []\n",
    "    \n",
    "    # Group consecutive samples with the same label\n",
    "    labeled_df['label_group'] = (labeled_df['event_label'] != labeled_df['event_label'].shift()).cumsum()\n",
    "    \n",
    "    for group_id, group_data in labeled_df.groupby('label_group'):\n",
    "        if len(group_data) < 50:  # Skip very short segments\n",
    "            continue\n",
    "            \n",
    "        event_type = group_data['event_label'].iloc[0]\n",
    "        start_time = group_data.index.min()\n",
    "        end_time = group_data.index.max()\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        event_periods.append({\n",
    "            'event_type': event_type,\n",
    "            'start_time': start_time,\n",
    "            'end_time': end_time,\n",
    "            'duration': duration,\n",
    "            'sample_count': len(group_data)\n",
    "        })\n",
    "    \n",
    "    print(f\"Found {len(event_periods)} distinct event periods\")\n",
    "    \n",
    "    # Extract features for each period\n",
    "    for i, period in enumerate(event_periods):\n",
    "        try:\n",
    "            # Get data for this period\n",
    "            period_data = labeled_df[\n",
    "                (labeled_df.index >= period['start_time']) & \n",
    "                (labeled_df.index <= period['end_time'])\n",
    "            ]\n",
    "            print(period_data)\n",
    "            \n",
    "            if 'breathingSignal' not in period_data.columns or len(period_data) < 50:\n",
    "                continue\n",
    "            \n",
    "            # Extract features\n",
    "            features = feature_extraction_func(\n",
    "                period_data['timestamp'].values,\n",
    "                period_data['breathingSignal'].values\n",
    "            )\n",
    "            \n",
    "            if features is not None:\n",
    "                # Add metadata\n",
    "                features['event_type'] = period['event_type']\n",
    "                features['event_time'] = period['start_time']\n",
    "                features['period_duration'] = period['duration']\n",
    "                features['sample_count'] = period['sample_count']\n",
    "                \n",
    "                all_features.append(features)\n",
    "                \n",
    "                if i % 50 == 0:\n",
    "                    print(f\"  Processed {i+1}/{len(event_periods)} periods...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing period {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Successfully extracted features for {len(all_features)} periods\")\n",
    "    return all_features\n",
    "\n",
    "def visualize_event_timeline(labeled_df, sample_hours=2):\n",
    "    \"\"\"\n",
    "    Visualize the event labeling over time.\n",
    "    \"\"\"\n",
    "    print(f\"Creating timeline visualization for first {sample_hours} hours...\")\n",
    "    \n",
    "    # Sample data for visualization\n",
    "    start_time = labeled_df.index.min()\n",
    "    end_time = start_time + pd.Timedelta(hours=sample_hours)\n",
    "    sample_data = labeled_df[(labeled_df.index >= start_time) & (labeled_df.index <= end_time)]\n",
    "    \n",
    "    if len(sample_data) == 0:\n",
    "        print(\"No data available for visualization period\")\n",
    "        return\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10), sharex=True)\n",
    "    \n",
    "    # Plot respiratory signal\n",
    "    ax1.plot(sample_data.index, sample_data['breathingSignal'], 'b-', alpha=0.7, linewidth=0.5)\n",
    "    ax1.set_ylabel('Breathing Signal')\n",
    "    ax1.set_title(f'Respiratory Signal with Event Labels ({sample_hours} hours)')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot event labels\n",
    "    unique_labels = sample_data['event_label'].unique()\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, len(unique_labels)))\n",
    "    label_colors = dict(zip(unique_labels, colors))\n",
    "    \n",
    "    # Create colored background for events\n",
    "    for label in unique_labels:\n",
    "        if label == 'Normal':\n",
    "            continue\n",
    "        label_data = sample_data[sample_data['event_label'] == label]\n",
    "        if len(label_data) > 0:\n",
    "            ax1.scatter(label_data.index, label_data['breathingSignal'], \n",
    "                       c=[label_colors[label]], alpha=0.6, s=1, label=label)\n",
    "    \n",
    "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Plot label timeline\n",
    "    label_numeric = pd.Categorical(sample_data['event_label']).codes\n",
    "    ax2.scatter(sample_data.index, label_numeric, c=[label_colors[label] for label in sample_data['event_label']], \n",
    "               s=2, alpha=0.8)\n",
    "    ax2.set_ylabel('Event Type')\n",
    "    ax2.set_xlabel('Time')\n",
    "    ax2.set_yticks(range(len(unique_labels)))\n",
    "    ax2.set_yticklabels(unique_labels)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print event statistics for this period\n",
    "    print(f\"\\nEvent statistics for {sample_hours}-hour sample:\")\n",
    "    event_stats = sample_data['event_label'].value_counts()\n",
    "    for label, count in event_stats.items():\n",
    "        duration_minutes = count * (sample_data.index[1] - sample_data.index[0]).total_seconds() / 60\n",
    "        print(f\"  {label}: {count:,} samples ({duration_minutes:.1f} minutes)\")\n",
    "\n",
    "def process_existing_events_df(events_df):\n",
    "    \"\"\"\n",
    "    Process your existing events DataFrame to add proper timestamps.\n",
    "    \n",
    "    Parameters:\n",
    "    - events_df: Your existing events DataFrame\n",
    "    \n",
    "    Returns:\n",
    "    - processed_events_df: DataFrame with start_time, end_time columns\n",
    "    \"\"\"\n",
    "    print(f\"Processing existing events DataFrame with {len(events_df)} events\")\n",
    "    \n",
    "    # Create a copy to avoid modifying original\n",
    "    processed_df = events_df.copy()\n",
    "    \n",
    "    # Convert Unix timestamp to datetime (assuming milliseconds)\n",
    "    processed_df['start_time'] = pd.to_datetime(processed_df['UnixTimestamp'], unit='ms').dt.tz_localize('UTC').dt.tz_convert(tz)\n",
    "    \n",
    "    # Parse duration (handle comma as decimal separator)\n",
    "    processed_df['duration_seconds'] = processed_df['Duration'].str.replace(',', '.').astype(float)\n",
    "    \n",
    "    # Calculate end time\n",
    "    processed_df['end_time'] = processed_df['start_time'] + pd.to_timedelta(processed_df['duration_seconds'], unit='s')\n",
    "    \n",
    "    # Sort by start time\n",
    "    processed_df = processed_df.sort_values('start_time').reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Event types: {processed_df['Event'].value_counts().to_dict()}\")\n",
    "    print(f\"Time range: {processed_df['start_time'].min()} to {processed_df['end_time'].max()}\")\n",
    "    print(f\"Duration range: {processed_df['duration_seconds'].min():.1f}s to {processed_df['duration_seconds'].max():.1f}s\")\n",
    "    \n",
    "    return processed_df\n",
    "\n",
    "def comprehensive_precise_analysis(respeck_df, events_df, feature_extraction_func):\n",
    "    \"\"\"\n",
    "    Run complete analysis using precise event timing with existing events DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - respeck_df: Your respiratory DataFrame\n",
    "    - events_df: Your existing events DataFrame \n",
    "    - feature_extraction_func: Your respiratory feature extraction function\n",
    "    \"\"\"\n",
    "    print(\"=== Comprehensive Precise Event Analysis ===\")\n",
    "    \n",
    "    # Step 1: Process existing events DataFrame\n",
    "    print(\"\\n1. Processing existing events DataFrame...\")\n",
    "    processed_events_df = process_existing_events_df(events_df)\n",
    "    \n",
    "    # Step 2: Create labeled respiratory data\n",
    "    print(\"\\n2. Creating labeled respiratory data...\")\n",
    "    labeled_df = create_labeled_respiratory_data(respeck_df, processed_events_df)\n",
    "    \n",
    "    # Step 3: Analyze coverage\n",
    "    print(\"\\n3. Analyzing event coverage...\")\n",
    "    coverage_df = analyze_event_coverage(labeled_df, processed_events_df)\n",
    "    \n",
    "    # Step 4: Visualize timeline\n",
    "    print(\"\\n4. Creating timeline visualization...\")\n",
    "    visualize_event_timeline(labeled_df, sample_hours=2)\n",
    "    \n",
    "    # Step 5: Extract features with precise labels\n",
    "    print(\"\\n5. Extracting features with precise labels...\")\n",
    "    features_list = extract_features_by_precise_labels(labeled_df, feature_extraction_func)\n",
    "    \n",
    "    return {\n",
    "        'labeled_df': labeled_df,\n",
    "        'events_df': processed_events_df,\n",
    "        'coverage_df': coverage_df,\n",
    "        'features_list': features_list\n",
    "    }\n",
    "\n",
    "# Usage example with your existing events_df:\n",
    "\n",
    "# Import your feature extraction function\n",
    "\n",
    "# Run comprehensive precise analysis using your existing events_df\n",
    "results = comprehensive_precise_analysis(\n",
    "    respeck_df=respeck_df,  # Your respiratory DataFrame\n",
    "    events_df=labels_df,    # Your existing events DataFrame\n",
    "    feature_extraction_func=calculate_TS_breathFeatures\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d44e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the missing timestamp column and re-run feature extraction\n",
    "\n",
    "print(\"=== Fixing Feature Extraction ===\")\n",
    "\n",
    "# Get your labeled_df from results\n",
    "labeled_df = results['labeled_df'].copy()\n",
    "\n",
    "# Add the timestamp column from the index\n",
    "labeled_df['timestamp'] = labeled_df.index\n",
    "print(\"✅ Added 'timestamp' column from index\")\n",
    "\n",
    "# Verify the fix\n",
    "print(f\"Now has 'timestamp' column: {'timestamp' in labeled_df.columns}\")\n",
    "print(f\"Sample timestamps: {labeled_df['timestamp'].head(3).tolist()}\")\n",
    "\n",
    "# Now let's re-run the feature extraction with the corrected DataFrame\n",
    "def extract_features_by_precise_labels_fixed(labeled_df, feature_extraction_func, \n",
    "                                           segment_length_minutes=5):\n",
    "    \"\"\"\n",
    "    Extract features using precise event labels with proper timestamp handling.\n",
    "    \"\"\"\n",
    "    print(f\"Extracting features using precise labels...\")\n",
    "    \n",
    "    all_features = []\n",
    "    \n",
    "    # Group consecutive samples with the same label\n",
    "    labeled_df['label_group'] = (labeled_df['event_label'] != labeled_df['event_label'].shift()).cumsum()\n",
    "    \n",
    "    # Get all unique event periods\n",
    "    event_periods = []\n",
    "    for group_id, group_data in labeled_df.groupby('label_group'):\n",
    "        if len(group_data) < 50:  # Skip very short segments\n",
    "            continue\n",
    "            \n",
    "        event_type = group_data['event_label'].iloc[0]\n",
    "        start_time = group_data.index.min()\n",
    "        end_time = group_data.index.max()\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        event_periods.append({\n",
    "            'event_type': event_type,\n",
    "            'start_time': start_time,\n",
    "            'end_time': end_time,\n",
    "            'duration': duration,\n",
    "            'sample_count': len(group_data)\n",
    "        })\n",
    "    \n",
    "    print(f\"Found {len(event_periods)} distinct event periods\")\n",
    "    \n",
    "    # Show breakdown by event type\n",
    "    period_counts = {}\n",
    "    for period in event_periods:\n",
    "        event_type = period['event_type']\n",
    "        period_counts[event_type] = period_counts.get(event_type, 0) + 1\n",
    "    \n",
    "    print(\"Periods by event type:\")\n",
    "    for event_type, count in sorted(period_counts.items()):\n",
    "        print(f\"  {event_type}: {count} periods\")\n",
    "    \n",
    "    # Extract features for each period\n",
    "    successful_extractions = 0\n",
    "    for i, period in enumerate(event_periods):\n",
    "        try:\n",
    "            # Get data for this period\n",
    "            period_data = labeled_df[\n",
    "                (labeled_df.index >= period['start_time']) & \n",
    "                (labeled_df.index <= period['end_time'])\n",
    "            ]\n",
    "            \n",
    "            if 'breathingSignal' not in period_data.columns or len(period_data) < 50:\n",
    "                continue\n",
    "            \n",
    "            # Extract features using the timestamp column\n",
    "            features = feature_extraction_func(\n",
    "                period_data['timestamp'].to_numpy(),\n",
    "                period_data['breathingSignal'].values\n",
    "            )\n",
    "            \n",
    "            if features is not None:\n",
    "                # Add metadata\n",
    "                features['event_type'] = period['event_type']\n",
    "                features['event_time'] = period['start_time']\n",
    "                features['period_duration'] = period['duration']\n",
    "                features['sample_count'] = period['sample_count']\n",
    "                \n",
    "                all_features.append(features)\n",
    "                successful_extractions += 1\n",
    "                \n",
    "                if i % 100 == 0:\n",
    "                    print(f\"  Processed {i+1}/{len(event_periods)} periods... ({successful_extractions} successful)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            if i < 5:  # Only show first 5 errors\n",
    "                print(f\"Error processing period {i} ({period['event_type']}): {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Successfully extracted features for {successful_extractions} out of {len(event_periods)} periods\")\n",
    "    \n",
    "    # Show successful extractions by event type\n",
    "    success_counts = {}\n",
    "    for features in all_features:\n",
    "        event_type = features['event_type']\n",
    "        success_counts[event_type] = success_counts.get(event_type, 0) + 1\n",
    "    \n",
    "    print(\"Successful extractions by event type:\")\n",
    "    for event_type, count in sorted(success_counts.items()):\n",
    "        print(f\"  {event_type}: {count} periods\")\n",
    "    \n",
    "    return all_features\n",
    "\n",
    "# Test on a small sample first\n",
    "print(\"\\n=== Testing Feature Extraction on Sample ===\")\n",
    "sample_data = labeled_df.head(1000)\n",
    "sample_data['timestamp'] = sample_data.index\n",
    "\n",
    "try:\n",
    "    test_features = calculate_TS_breathFeatures(\n",
    "        sample_data['timestamp'].to_numpy(),\n",
    "        sample_data['breathingSignal'].values\n",
    "    )\n",
    "    \n",
    "    if test_features is not None:\n",
    "        print(\"✅ Feature extraction test successful!\")\n",
    "        print(f\"Extracted features: {list(test_features.keys())}\")\n",
    "    else:\n",
    "        print(\"❌ Feature extraction returned None\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Feature extraction failed: {e}\")\n",
    "    print(\"This might be due to signal quality or length issues\")\n",
    "\n",
    "# Now run the full feature extraction\n",
    "print(\"\\n=== Running Full Feature Extraction ===\")\n",
    "features_list = extract_features_by_precise_labels_fixed(\n",
    "    labeled_df, \n",
    "    calculate_TS_breathFeatures\n",
    ")\n",
    "\n",
    "print(f\"\\n=== Results ===\")\n",
    "print(f\"Total features extracted: {len(features_list)}\")\n",
    "\n",
    "# Update the results\n",
    "results['features_list'] = features_list\n",
    "results['labeled_df'] = labeled_df  # Updated with timestamp column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f622ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add these functions after your existing code\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "def aggregate_breath_features(features_list):\n",
    "    \"\"\"\n",
    "    Convert list of feature dictionaries to aggregated DataFrame for analysis.\n",
    "    Handles time-series data by computing statistical summaries.\n",
    "    \"\"\"\n",
    "    print(f\"=== Aggregation Debug Info ===\")\n",
    "    print(f\"Number of feature dictionaries: {len(features_list)}\")\n",
    "    \n",
    "    if len(features_list) == 0:\n",
    "        print(\"❌ ERROR: features_list is empty!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Check first few feature dictionaries\n",
    "    print(f\"Keys in first feature dict: {list(features_list[0].keys())}\")\n",
    "    if len(features_list) > 0:\n",
    "        print(f\"Sample event_type: {features_list[0].get('event_type', 'MISSING')}\")\n",
    "    \n",
    "    aggregated_features = []\n",
    "    \n",
    "    for features in features_list:\n",
    "        event_features = {\n",
    "            'event_type': features['event_type'],\n",
    "            'event_time': features['event_time'],\n",
    "            'period_duration': features.get('period_duration', np.nan),\n",
    "            'sample_count': features.get('sample_count', np.nan)\n",
    "        }\n",
    "        \n",
    "        # Aggregate time-series features to event-level statistics\n",
    "        for feature_name, values in features.items():\n",
    "            if feature_name in ['event_type', 'event_time', 'period_duration', 'sample_count', 'timestamp', 'breathingSignal']:\n",
    "                continue\n",
    "            \n",
    "            # Handle different data types\n",
    "            if hasattr(values, 'values'):  # pandas Series\n",
    "                values = values.values\n",
    "            \n",
    "            # Convert to numpy array and handle different cases\n",
    "            if isinstance(values, (list, np.ndarray, pd.Series)):\n",
    "                try:\n",
    "                    # Convert to numpy array\n",
    "                    values_array = np.array(values, dtype=float)\n",
    "                    \n",
    "                    # Remove NaN and infinite values\n",
    "                    values_clean = values_array[np.isfinite(values_array)]\n",
    "                    \n",
    "                    if len(values_clean) > 0:\n",
    "                        # Basic statistics\n",
    "                        event_features[f'{feature_name}_mean'] = np.mean(values_clean)\n",
    "                        event_features[f'{feature_name}_std'] = np.std(values_clean)\n",
    "                        event_features[f'{feature_name}_median'] = np.median(values_clean)\n",
    "                        event_features[f'{feature_name}_min'] = np.min(values_clean)\n",
    "                        event_features[f'{feature_name}_max'] = np.max(values_clean)\n",
    "                        event_features[f'{feature_name}_range'] = np.max(values_clean) - np.min(values_clean)\n",
    "                        \n",
    "                        # Percentiles\n",
    "                        event_features[f'{feature_name}_q25'] = np.percentile(values_clean, 25)\n",
    "                        event_features[f'{feature_name}_q75'] = np.percentile(values_clean, 75)\n",
    "                        event_features[f'{feature_name}_iqr'] = np.percentile(values_clean, 75) - np.percentile(values_clean, 25)\n",
    "                        \n",
    "                        # Additional statistics for time-series\n",
    "                        event_features[f'{feature_name}_skew'] = pd.Series(values_clean).skew()\n",
    "                        event_features[f'{feature_name}_kurtosis'] = pd.Series(values_clean).kurtosis()\n",
    "                        \n",
    "                        # Variability measures\n",
    "                        if np.mean(values_clean) != 0:\n",
    "                            event_features[f'{feature_name}_cv'] = np.std(values_clean) / np.abs(np.mean(values_clean))\n",
    "                        else:\n",
    "                            event_features[f'{feature_name}_cv'] = 0\n",
    "                            \n",
    "                        # Count of valid values\n",
    "                        event_features[f'{feature_name}_count'] = len(values_clean)\n",
    "                        \n",
    "                        # For breath-level features, add trend analysis\n",
    "                        if feature_name in ['auc_values', 'breath_durations', 'inhalation_durations', 'exhalation_durations']:\n",
    "                            if len(values_clean) >= 2:\n",
    "                                # Trend analysis (linear slope)\n",
    "                                x = np.arange(len(values_clean))\n",
    "                                slope = np.polyfit(x, values_clean, 1)[0] if len(values_clean) > 1 else 0\n",
    "                                event_features[f'{feature_name}_trend'] = slope\n",
    "                                \n",
    "                                # Rate of change\n",
    "                                rate_of_change = np.mean(np.diff(values_clean)) if len(values_clean) > 1 else 0\n",
    "                                event_features[f'{feature_name}_rate_change'] = rate_of_change\n",
    "                                \n",
    "                except (ValueError, TypeError) as e:\n",
    "                    continue\n",
    "                    \n",
    "            elif isinstance(values, (int, float)) and np.isfinite(values):\n",
    "                # Single value features\n",
    "                event_features[feature_name] = values\n",
    "        \n",
    "        aggregated_features.append(event_features)\n",
    "    \n",
    "    return pd.DataFrame(aggregated_features)\n",
    "\n",
    "def statistical_comparison_multi_event(features_df):\n",
    "    \"\"\"\n",
    "    Perform statistical comparisons between each event type and normal.\n",
    "    \"\"\"\n",
    "    if 'Normal' not in features_df['event_type'].values:\n",
    "        print(\"No 'Normal' samples found for comparison\")\n",
    "        return None\n",
    "    \n",
    "    # Get numerical columns\n",
    "    numerical_cols = features_df.select_dtypes(include=[np.number]).columns\n",
    "    numerical_cols = [col for col in numerical_cols if col not in ['period_duration', 'sample_count']]\n",
    "    \n",
    "    # Get normal data\n",
    "    normal_data = features_df[features_df['event_type'] == 'Normal']\n",
    "    \n",
    "    comparison_results = []\n",
    "    event_types = [et for et in features_df['event_type'].unique() if et != 'Normal']\n",
    "    \n",
    "    print(f\"\\n=== Statistical Comparisons vs Normal (n={len(normal_data)}) ===\")\n",
    "    \n",
    "    for event_type in event_types:\n",
    "        event_data = features_df[features_df['event_type'] == event_type]\n",
    "        print(f\"\\n{event_type} (n={len(event_data)}) vs Normal:\")\n",
    "        \n",
    "        for col in numerical_cols:\n",
    "            normal_vals = normal_data[col].dropna()\n",
    "            event_vals = event_data[col].dropna()\n",
    "            \n",
    "            if len(normal_vals) > 1 and len(event_vals) > 1:\n",
    "                # Perform t-test\n",
    "                t_stat, p_value = stats.ttest_ind(event_vals, normal_vals, equal_var=False)\n",
    "                \n",
    "                # Calculate effect size (Cohen's d)\n",
    "                pooled_std = np.sqrt(((len(event_vals) - 1) * event_vals.var() + \n",
    "                                    (len(normal_vals) - 1) * normal_vals.var()) / \n",
    "                                   (len(event_vals) + len(normal_vals) - 2))\n",
    "                cohens_d = (event_vals.mean() - normal_vals.mean()) / pooled_std if pooled_std > 0 else 0\n",
    "                \n",
    "                comparison_results.append({\n",
    "                    'event_type': event_type,\n",
    "                    'feature': col,\n",
    "                    'event_mean': event_vals.mean(),\n",
    "                    'event_std': event_vals.std(),\n",
    "                    'normal_mean': normal_vals.mean(),\n",
    "                    'normal_std': normal_vals.std(),\n",
    "                    'mean_difference': event_vals.mean() - normal_vals.mean(),\n",
    "                    't_statistic': t_stat,\n",
    "                    'p_value': p_value,\n",
    "                    'cohens_d': cohens_d,\n",
    "                    'significant': p_value < 0.05,\n",
    "                    'effect_size_magnitude': 'Large' if abs(cohens_d) > 0.8 else 'Medium' if abs(cohens_d) > 0.5 else 'Small'\n",
    "                })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_results)\n",
    "    \n",
    "    # Show significant results for each event type\n",
    "    for event_type in event_types:\n",
    "        event_results = comparison_df[comparison_df['event_type'] == event_type]\n",
    "        significant_results = event_results[event_results['significant']].sort_values('p_value')\n",
    "        \n",
    "        if len(significant_results) > 0:\n",
    "            print(f\"\\nSignificant features for {event_type} ({len(significant_results)} total):\")\n",
    "            for _, row in significant_results.head(5).iterrows():\n",
    "                direction = \"↑\" if row['mean_difference'] > 0 else \"↓\"\n",
    "                print(f\"  {direction} {row['feature']}: p={row['p_value']:.4f}, \"\n",
    "                      f\"effect={row['cohens_d']:.3f} ({row['effect_size_magnitude']})\")\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "def plot_event_comparisons(features_df, comparison_features=None):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations comparing all event types vs normal.\n",
    "    \"\"\"\n",
    "    if comparison_features is None:\n",
    "        # Select top features based on variance\n",
    "        numeric_cols = features_df.select_dtypes(include=[np.number]).columns\n",
    "        feature_variance = features_df[numeric_cols].var().sort_values(ascending=False)\n",
    "        comparison_features = feature_variance.head(12).index.tolist()\n",
    "    \n",
    "    # Get event type counts\n",
    "    event_counts = features_df['event_type'].value_counts()\n",
    "    print(f\"Sample sizes: {event_counts.to_dict()}\")\n",
    "    \n",
    "    # Create comprehensive comparison plots\n",
    "    n_features = len(comparison_features)\n",
    "    n_cols = 3\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 6 * n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = [axes] if n_cols == 1 else axes\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, feature in enumerate(comparison_features):\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "            \n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Box plot with all event types\n",
    "        box_plot = sns.boxplot(data=features_df, x='event_type', y=feature, ax=ax)\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "        ax.set_title(f'{feature}')\n",
    "        ax.set_xlabel('')\n",
    "        \n",
    "        # Add sample sizes to x-axis labels\n",
    "        new_labels = []\n",
    "        for label in ax.get_xticklabels():\n",
    "            event_name = label.get_text()\n",
    "            count = event_counts.get(event_name, 0)\n",
    "            new_labels.append(f'{event_name}\\n(n={count})')\n",
    "        ax.set_xticklabels(new_labels)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(comparison_features), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_top_significant_features(comparison_df, features_df, top_n=10):\n",
    "    \"\"\"\n",
    "    Plot the top most significant features across all event types.\n",
    "    \"\"\"\n",
    "    if comparison_df is None:\n",
    "        print(\"No comparison results available\")\n",
    "        return\n",
    "    \n",
    "    # Get top significant features by effect size\n",
    "    significant_features = comparison_df[comparison_df['significant']].nlargest(top_n, 'cohens_d')\n",
    "    \n",
    "    if len(significant_features) == 0:\n",
    "        print(\"No significant features found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Plotting top {len(significant_features)} most significant features:\")\n",
    "    \n",
    "    # Create subplots\n",
    "    n_features = len(significant_features)\n",
    "    n_cols = 2\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = [axes] if n_cols == 1 else axes\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, (_, row) in enumerate(significant_features.iterrows()):\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "            \n",
    "        ax = axes[i]\n",
    "        feature = row['feature']\n",
    "        event_type = row['event_type']\n",
    "        \n",
    "        # Create comparison plot for this specific feature and event type\n",
    "        plot_data = features_df[features_df['event_type'].isin(['Normal', event_type])]\n",
    "        \n",
    "        sns.boxplot(data=plot_data, x='event_type', y=feature, ax=ax)\n",
    "        \n",
    "        # Add statistics to title\n",
    "        direction = \"↑\" if row['mean_difference'] > 0 else \"↓\"\n",
    "        ax.set_title(f'{direction} {feature} ({event_type})\\n'\n",
    "                     f'p={row[\"p_value\"]:.4f}, effect={row[\"cohens_d\"]:.3f}')\n",
    "        ax.set_xlabel('')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(significant_features), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Now run the analysis with your existing results:\n",
    "\n",
    "# Now run the aggregation and comparison\n",
    "if len(features_list) > 0:\n",
    "    print(\"\\n=== Running Analysis ===\")\n",
    "    \n",
    "    # Import the analysis functions if not already loaded\n",
    "    # (The aggregate_breath_features and statistical_comparison_multi_event functions from before)\n",
    "    \n",
    "    # Aggregate features\n",
    "    features_df = aggregate_breath_features(features_list)\n",
    "    print(f\"Created aggregated features DataFrame: {features_df.shape}\")\n",
    "    \n",
    "    # Run statistical comparisons\n",
    "    comparison_results = statistical_comparison_multi_event(features_df)\n",
    "    \n",
    "    # Create visualizations\n",
    "    if comparison_results is not None:\n",
    "        print(\"\\nCreating visualizations...\")\n",
    "        plot_event_comparisons(features_df)\n",
    "        plot_top_significant_features(comparison_results, features_df, top_n=8)\n",
    "    \n",
    "    # Save results\n",
    "    features_df.to_csv('aggregated_respiratory_features_fixed.csv', index=False)\n",
    "    if comparison_results is not None:\n",
    "        comparison_results.to_csv('statistical_comparison_results_fixed.csv', index=False)\n",
    "    \n",
    "    print(\"✅ Analysis complete!\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No features were extracted. Check the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6b195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def create_osa_binary_classification(features_df):\n",
    "    \"\"\"\n",
    "    Create binary OSA vs Non-OSA classification from your features DataFrame.\n",
    "    \"\"\"\n",
    "    print(\"=== Creating OSA Binary Classification ===\")\n",
    "    \n",
    "    # Create binary labels: OSA vs Non-OSA\n",
    "    features_df_binary = features_df.copy()\n",
    "    features_df_binary['is_osa'] = (features_df_binary['event_type'] == 'Obstructive Apnea').astype(int)\n",
    "    features_df_binary['binary_label'] = features_df_binary['is_osa'].map({1: 'OSA', 0: 'Non-OSA'})\n",
    "    \n",
    "    # Show class distribution\n",
    "    class_counts = features_df_binary['binary_label'].value_counts()\n",
    "    print(f\"Class distribution:\")\n",
    "    print(f\"  OSA: {class_counts.get('OSA', 0)} samples\")\n",
    "    print(f\"  Non-OSA: {class_counts.get('Non-OSA', 0)} samples\")\n",
    "    print(f\"  OSA percentage: {(class_counts.get('OSA', 0) / len(features_df_binary) * 100):.1f}%\")\n",
    "    \n",
    "    # Show what's included in Non-OSA\n",
    "    non_osa_breakdown = features_df[features_df['event_type'] != 'Obstructive Apnea']['event_type'].value_counts()\n",
    "    print(f\"\\nNon-OSA breakdown:\")\n",
    "    for event_type, count in non_osa_breakdown.items():\n",
    "        print(f\"  {event_type}: {count}\")\n",
    "    \n",
    "    return features_df_binary\n",
    "\n",
    "def statistical_comparison_osa_vs_rest(features_df_binary):\n",
    "    \"\"\"\n",
    "    Statistical comparison: OSA vs Non-OSA (everything else).\n",
    "    \"\"\"\n",
    "    print(\"\\n=== OSA vs Non-OSA Statistical Analysis ===\")\n",
    "    \n",
    "    # Get numerical columns\n",
    "    numerical_cols = features_df_binary.select_dtypes(include=[np.number]).columns\n",
    "    numerical_cols = [col for col in numerical_cols if col not in ['period_duration', 'sample_count', 'is_osa']]\n",
    "    \n",
    "    # Separate OSA vs Non-OSA\n",
    "    osa_data = features_df_binary[features_df_binary['binary_label'] == 'OSA']\n",
    "    non_osa_data = features_df_binary[features_df_binary['binary_label'] == 'Non-OSA']\n",
    "    \n",
    "    print(f\"OSA samples: {len(osa_data)}\")\n",
    "    print(f\"Non-OSA samples: {len(non_osa_data)}\")\n",
    "    \n",
    "    comparison_results = []\n",
    "    \n",
    "    for col in numerical_cols:\n",
    "        osa_vals = osa_data[col].dropna()\n",
    "        non_osa_vals = non_osa_data[col].dropna()\n",
    "        \n",
    "        if len(osa_vals) > 1 and len(non_osa_vals) > 1:\n",
    "            # Perform t-test\n",
    "            t_stat, p_value = stats.ttest_ind(osa_vals, non_osa_vals, equal_var=False)\n",
    "            \n",
    "            # Calculate effect size (Cohen's d)\n",
    "            pooled_std = np.sqrt(((len(osa_vals) - 1) * osa_vals.var() + \n",
    "                                (len(non_osa_vals) - 1) * non_osa_vals.var()) / \n",
    "                               (len(osa_vals) + len(non_osa_vals) - 2))\n",
    "            cohens_d = (osa_vals.mean() - non_osa_vals.mean()) / pooled_std if pooled_std > 0 else 0\n",
    "            \n",
    "            comparison_results.append({\n",
    "                'feature': col,\n",
    "                'osa_mean': osa_vals.mean(),\n",
    "                'osa_std': osa_vals.std(),\n",
    "                'non_osa_mean': non_osa_vals.mean(),\n",
    "                'non_osa_std': non_osa_vals.std(),\n",
    "                'mean_difference': osa_vals.mean() - non_osa_vals.mean(),\n",
    "                't_statistic': t_stat,\n",
    "                'p_value': p_value,\n",
    "                'cohens_d': cohens_d,\n",
    "                'significant': p_value < 0.05,\n",
    "                'effect_size_magnitude': 'Large' if abs(cohens_d) > 0.8 else 'Medium' if abs(cohens_d) > 0.5 else 'Small'\n",
    "            })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_results)\n",
    "    \n",
    "    # Sort by significance and effect size\n",
    "    significant_results = comparison_df[comparison_df['significant']].sort_values('p_value')\n",
    "    \n",
    "    print(f\"\\nSignificant features for OSA detection ({len(significant_results)} total):\")\n",
    "    for _, row in significant_results.head(10).iterrows():\n",
    "        direction = \"↑\" if row['mean_difference'] > 0 else \"↓\"\n",
    "        print(f\"  {direction} {row['feature']}: p={row['p_value']:.4f}, \"\n",
    "              f\"effect={row['cohens_d']:.3f} ({row['effect_size_magnitude']})\")\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "def plot_osa_classification_features(features_df_binary, comparison_df, top_n=12):\n",
    "    \"\"\"\n",
    "    Plot top discriminative features for OSA classification.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Plotting Top {top_n} OSA Classification Features ===\")\n",
    "    \n",
    "    # Get top significant features\n",
    "    if comparison_df is not None:\n",
    "        significant_features = comparison_df[comparison_df['significant']].copy()\n",
    "        significant_features['abs_cohens_d'] = significant_features['cohens_d'].abs()\n",
    "        significant_features = significant_features.nlargest(top_n, 'abs_cohens_d')\n",
    "        feature_names = significant_features['feature'].tolist()\n",
    "    else:\n",
    "        # Fallback to variance-based selection\n",
    "        numeric_cols = features_df_binary.select_dtypes(include=[np.number]).columns\n",
    "        feature_variance = features_df_binary[numeric_cols].var().sort_values(ascending=False)\n",
    "        feature_names = feature_variance.head(top_n).index.tolist()\n",
    "    \n",
    "    # Create comparison plots\n",
    "    n_features = len(feature_names)\n",
    "    n_cols = 3\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 6 * n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = [axes] if n_cols == 1 else axes\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, feature in enumerate(feature_names):\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "            \n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Box plot: OSA vs Non-OSA\n",
    "        sns.boxplot(data=features_df_binary, x='binary_label', y=feature, ax=ax)\n",
    "        \n",
    "        # Add statistics to title if available\n",
    "        if comparison_df is not None:\n",
    "            feature_stats = comparison_df[comparison_df['feature'] == feature]\n",
    "            if not feature_stats.empty:\n",
    "                row = feature_stats.iloc[0]\n",
    "                direction = \"↑\" if row['mean_difference'] > 0 else \"↓\"\n",
    "                ax.set_title(f'{direction} {feature}\\np={row[\"p_value\"]:.4f}, effect={row[\"cohens_d\"]:.3f}')\n",
    "            else:\n",
    "                ax.set_title(feature)\n",
    "        else:\n",
    "            ax.set_title(feature)\n",
    "        \n",
    "        ax.set_xlabel('')\n",
    "        \n",
    "        # Add sample sizes\n",
    "        osa_count = len(features_df_binary[features_df_binary['binary_label'] == 'OSA'])\n",
    "        non_osa_count = len(features_df_binary[features_df_binary['binary_label'] == 'Non-OSA'])\n",
    "        ax.set_xticklabels([f'Non-OSA\\n(n={non_osa_count})', f'OSA\\n(n={osa_count})'])\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(feature_names), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def machine_learning_osa_classification(features_df_binary):\n",
    "    \"\"\"\n",
    "    Build and evaluate machine learning models for OSA classification.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Machine Learning OSA Classification ===\")\n",
    "    \n",
    "    # Prepare features and labels\n",
    "    feature_cols = features_df_binary.select_dtypes(include=[np.number]).columns\n",
    "    feature_cols = [col for col in feature_cols if col not in ['period_duration', 'sample_count', 'is_osa']]\n",
    "    \n",
    "    X = features_df_binary[feature_cols].fillna(0)\n",
    "    y = features_df_binary['is_osa']\n",
    "    \n",
    "    print(f\"Features used: {len(feature_cols)}\")\n",
    "    print(f\"Total samples: {len(X)}\")\n",
    "    print(f\"OSA samples: {y.sum()}\")\n",
    "    print(f\"Non-OSA samples: {len(y) - y.sum()}\")\n",
    "    \n",
    "    if len(np.unique(y)) < 2:\n",
    "        print(\"❌ Cannot perform classification: only one class present.\")\n",
    "        return None\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "    rf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(rf, X_train_scaled, y_train, cv=5, scoring='roc_auc')\n",
    "    print(f\"\\nCross-validation AUC scores: {cv_scores}\")\n",
    "    print(f\"Mean CV AUC: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "    \n",
    "    # Test performance\n",
    "    y_pred = rf.predict(X_test_scaled)\n",
    "    y_pred_proba = rf.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    print(f\"\\n=== Test Set Performance ===\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Non-OSA', 'OSA']))\n",
    "    \n",
    "    # AUC Score\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    print(f\"AUC Score: {auc_score:.3f}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"                Predicted\")\n",
    "    print(f\"Actual    Non-OSA    OSA\")\n",
    "    print(f\"Non-OSA      {cm[0,0]:3d}    {cm[0,1]:3d}\")\n",
    "    print(f\"OSA          {cm[1,0]:3d}    {cm[1,1]:3d}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': rf.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop 10 Most Important Features for OSA Classification:\")\n",
    "    for _, row in feature_importance.head(10).iterrows():\n",
    "        print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_score:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve for OSA Classification')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_importance.head(15)\n",
    "    plt.barh(range(len(top_features)), top_features['importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('Top 15 Features for OSA Classification')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'model': rf,\n",
    "        'scaler': scaler,\n",
    "        'feature_importance': feature_importance,\n",
    "        'cv_scores': cv_scores,\n",
    "        'auc_score': auc_score,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "# Run the complete OSA classification analysis\n",
    "print(\"=== OSA BINARY CLASSIFICATION ANALYSIS ===\")\n",
    "\n",
    "# 1. Create binary classification dataset\n",
    "features_df_binary = create_osa_binary_classification(features_df)\n",
    "\n",
    "# 2. Statistical comparison\n",
    "comparison_results = statistical_comparison_osa_vs_rest(features_df_binary)\n",
    "\n",
    "# 3. Plot top discriminative features\n",
    "plot_osa_classification_features(features_df_binary, comparison_results, top_n=12)\n",
    "\n",
    "# 4. Machine learning classification\n",
    "# ml_results = machine_learning_osa_classification(features_df_binary)\n",
    "\n",
    "# 5. Save results\n",
    "print(\"\\n=== Saving Results ===\")\n",
    "features_df_binary.to_csv('osa_binary_classification_data.csv', index=False)\n",
    "if comparison_results is not None:\n",
    "    comparison_results.to_csv('osa_vs_rest_statistical_comparison.csv', index=False)\n",
    "\n",
    "print(\"\\n✅ OSA Classification Analysis Complete!\")\n",
    "print(\"Key files saved:\")\n",
    "print(\"  - osa_binary_classification_data.csv\")\n",
    "print(\"  - osa_vs_rest_statistical_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6a21a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class MultiNightOSAAnalyzer:\n",
    "    \"\"\"\n",
    "    Comprehensive multi-night OSA analysis framework.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_directory):\n",
    "        \"\"\"\n",
    "        Initialize the analyzer with a directory containing multiple nights of data.\n",
    "        \n",
    "        Parameters:\n",
    "        - data_directory: Path to directory containing night-specific data files\n",
    "        \"\"\"\n",
    "        self.data_directory = data_directory\n",
    "        self.night_results = {}\n",
    "        self.aggregated_features = None\n",
    "        self.aggregated_comparison = None\n",
    "        self.final_model = None\n",
    "        \n",
    "    def process_single_night(self, respeck_data, events_data, night_id, feature_extraction_func):\n",
    "        \"\"\"\n",
    "        Process a single night's data using your existing pipeline.\n",
    "        \n",
    "        Parameters:\n",
    "        - respeck_data: DataFrame with respiratory data for this night\n",
    "        - events_data: DataFrame with events for this night\n",
    "        - night_id: Identifier for this night (e.g., date, patient_night_1, etc.)\n",
    "        - feature_extraction_func: Your feature extraction function\n",
    "        \n",
    "        Returns:\n",
    "        - night_results: Dictionary with results for this night\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== Processing Night: {night_id} ===\")\n",
    "        \n",
    "        try:\n",
    "            # Use your existing pipeline functions\n",
    "            # Step 1: Process events\n",
    "            processed_events_df = self.process_existing_events_df(events_data)\n",
    "            \n",
    "            # Step 2: Create labeled respiratory data\n",
    "            labeled_df = self.create_labeled_respiratory_data(respeck_data, processed_events_df)\n",
    "            \n",
    "            # Step 3: Add timestamp column\n",
    "            labeled_df['timestamp'] = labeled_df.index\n",
    "            \n",
    "            # Step 4: Extract features\n",
    "            features_list = self.extract_features_by_precise_labels_fixed(\n",
    "                labeled_df, feature_extraction_func\n",
    "            )\n",
    "            \n",
    "            # Step 5: Aggregate features\n",
    "            if len(features_list) > 0:\n",
    "                features_df = self.aggregate_breath_features(features_list)\n",
    "                \n",
    "                # Add night identifier\n",
    "                features_df['night_id'] = night_id\n",
    "                \n",
    "                # Create binary OSA classification\n",
    "                features_df_binary = self.create_osa_binary_classification(features_df)\n",
    "                \n",
    "                night_result = {\n",
    "                    'night_id': night_id,\n",
    "                    'labeled_df': labeled_df,\n",
    "                    'features_df': features_df_binary,\n",
    "                    'n_samples': len(features_df_binary),\n",
    "                    'n_osa': len(features_df_binary[features_df_binary['binary_label'] == 'OSA']),\n",
    "                    'n_non_osa': len(features_df_binary[features_df_binary['binary_label'] == 'Non-OSA']),\n",
    "                    'success': True\n",
    "                }\n",
    "                \n",
    "                print(f\"✅ Night {night_id}: {len(features_df_binary)} samples \"\n",
    "                      f\"({night_result['n_osa']} OSA, {night_result['n_non_osa']} Non-OSA)\")\n",
    "                \n",
    "                return night_result\n",
    "                \n",
    "            else:\n",
    "                print(f\"❌ Night {night_id}: No features extracted\")\n",
    "                return {'night_id': night_id, 'success': False, 'error': 'No features extracted'}\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Night {night_id}: Error - {e}\")\n",
    "            return {'night_id': night_id, 'success': False, 'error': str(e)}\n",
    "    \n",
    "    def process_multiple_nights(self, night_data_list, feature_extraction_func):\n",
    "        \"\"\"\n",
    "        Process multiple nights of data.\n",
    "        \n",
    "        Parameters:\n",
    "        - night_data_list: List of dictionaries, each containing:\n",
    "          {'night_id': str, 'respeck_data': DataFrame, 'events_data': DataFrame}\n",
    "        - feature_extraction_func: Your feature extraction function\n",
    "        \n",
    "        Returns:\n",
    "        - processing_summary: Summary of processing results\n",
    "        \"\"\"\n",
    "        print(\"=== MULTI-NIGHT OSA ANALYSIS ===\")\n",
    "        print(f\"Processing {len(night_data_list)} nights of data...\")\n",
    "        \n",
    "        successful_nights = []\n",
    "        failed_nights = []\n",
    "        \n",
    "        for night_data in night_data_list:\n",
    "            result = self.process_single_night(\n",
    "                night_data['respeck_data'],\n",
    "                night_data['events_data'], \n",
    "                night_data['night_id'],\n",
    "                feature_extraction_func\n",
    "            )\n",
    "            \n",
    "            if result['success']:\n",
    "                successful_nights.append(result)\n",
    "                self.night_results[result['night_id']] = result\n",
    "            else:\n",
    "                failed_nights.append(result)\n",
    "        \n",
    "        print(f\"\\n=== Processing Summary ===\")\n",
    "        print(f\"Successful nights: {len(successful_nights)}\")\n",
    "        print(f\"Failed nights: {len(failed_nights)}\")\n",
    "        \n",
    "        if failed_nights:\n",
    "            print(f\"Failed night details:\")\n",
    "            for failed in failed_nights:\n",
    "                print(f\"  {failed['night_id']}: {failed['error']}\")\n",
    "        \n",
    "        return {\n",
    "            'successful_nights': len(successful_nights),\n",
    "            'failed_nights': len(failed_nights),\n",
    "            'successful_results': successful_nights,\n",
    "            'failed_results': failed_nights\n",
    "        }\n",
    "    \n",
    "    def aggregate_all_nights(self):\n",
    "        \"\"\"\n",
    "        Aggregate features from all successfully processed nights.\n",
    "        \"\"\"\n",
    "        print(\"\\n=== Aggregating Multi-Night Data ===\")\n",
    "        \n",
    "        if not self.night_results:\n",
    "            print(\"❌ No night results to aggregate\")\n",
    "            return None\n",
    "        \n",
    "        # Combine all features DataFrames\n",
    "        all_features = []\n",
    "        night_summary = []\n",
    "        \n",
    "        for night_id, night_result in self.night_results.items():\n",
    "            features_df = night_result['features_df']\n",
    "            all_features.append(features_df)\n",
    "            \n",
    "            night_summary.append({\n",
    "                'night_id': night_id,\n",
    "                'total_samples': night_result['n_samples'],\n",
    "                'osa_samples': night_result['n_osa'],\n",
    "                'non_osa_samples': night_result['n_non_osa'],\n",
    "                'osa_percentage': (night_result['n_osa'] / night_result['n_samples']) * 100\n",
    "            })\n",
    "        \n",
    "        # Concatenate all features\n",
    "        self.aggregated_features = pd.concat(all_features, ignore_index=True)\n",
    "        \n",
    "        print(f\"Aggregated data summary:\")\n",
    "        print(f\"  Total nights: {len(self.night_results)}\")\n",
    "        print(f\"  Total samples: {len(self.aggregated_features)}\")\n",
    "        \n",
    "        # Show class distribution\n",
    "        class_counts = self.aggregated_features['binary_label'].value_counts()\n",
    "        print(f\"  OSA samples: {class_counts.get('OSA', 0)}\")\n",
    "        print(f\"  Non-OSA samples: {class_counts.get('Non-OSA', 0)}\")\n",
    "        print(f\"  OSA percentage: {(class_counts.get('OSA', 0) / len(self.aggregated_features) * 100):.1f}%\")\n",
    "        \n",
    "        # Show per-night breakdown\n",
    "        night_summary_df = pd.DataFrame(night_summary)\n",
    "        print(f\"\\nPer-night breakdown:\")\n",
    "        print(night_summary_df.to_string(index=False))\n",
    "        \n",
    "        return self.aggregated_features\n",
    "    \n",
    "    def statistical_analysis_aggregated(self):\n",
    "        \"\"\"\n",
    "        Perform statistical analysis on aggregated multi-night data.\n",
    "        \"\"\"\n",
    "        print(\"\\n=== Multi-Night Statistical Analysis ===\")\n",
    "        \n",
    "        if self.aggregated_features is None:\n",
    "            print(\"❌ No aggregated features available. Run aggregate_all_nights() first.\")\n",
    "            return None\n",
    "        \n",
    "        # Statistical comparison\n",
    "        self.aggregated_comparison = self.statistical_comparison_osa_vs_rest(self.aggregated_features)\n",
    "        \n",
    "        return self.aggregated_comparison\n",
    "    \n",
    "    def build_final_model(self, test_size=0.2, cv_folds=10):\n",
    "        \"\"\"\n",
    "        Build final OSA classification model using all aggregated data.\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== Building Final OSA Classification Model ===\")\n",
    "        \n",
    "        if self.aggregated_features is None:\n",
    "            print(\"❌ No aggregated features available\")\n",
    "            return None\n",
    "        \n",
    "        # Prepare features and labels\n",
    "        feature_cols = self.aggregated_features.select_dtypes(include=[np.number]).columns\n",
    "        feature_cols = [col for col in feature_cols if col not in ['period_duration', 'sample_count', 'is_osa']]\n",
    "        \n",
    "        X = self.aggregated_features[feature_cols].fillna(0)\n",
    "        y = self.aggregated_features['is_osa']\n",
    "        \n",
    "        print(f\"Model training summary:\")\n",
    "        print(f\"  Features: {len(feature_cols)}\")\n",
    "        print(f\"  Total samples: {len(X)}\")\n",
    "        print(f\"  OSA samples: {y.sum()}\")\n",
    "        print(f\"  Non-OSA samples: {len(y) - y.sum()}\")\n",
    "        print(f\"  Nights included: {self.aggregated_features['night_id'].nunique()}\")\n",
    "        \n",
    "        # Stratified split to maintain class balance\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Train Random Forest with optimized parameters\n",
    "        rf = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            random_state=42,\n",
    "            class_weight='balanced',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Stratified cross-validation\n",
    "        skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "        cv_scores = cross_val_score(rf, X_train_scaled, y_train, cv=skf, scoring='roc_auc')\n",
    "        \n",
    "        print(f\"\\n{cv_folds}-Fold Cross-Validation Results:\")\n",
    "        print(f\"  AUC scores: {cv_scores}\")\n",
    "        print(f\"  Mean AUC: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "        \n",
    "        # Test performance\n",
    "        y_pred = rf.predict(X_test_scaled)\n",
    "        y_pred_proba = rf.predict_proba(X_test_scaled)[:, 1]\n",
    "        \n",
    "        test_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        \n",
    "        print(f\"\\nTest Set Performance:\")\n",
    "        print(f\"  AUC: {test_auc:.3f}\")\n",
    "        print(classification_report(y_test, y_pred, target_names=['Non-OSA', 'OSA']))\n",
    "        \n",
    "        # Feature importance\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': feature_cols,\n",
    "            'importance': rf.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\nTop 10 Most Important Features:\")\n",
    "        for _, row in feature_importance.head(10).iterrows():\n",
    "            print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
    "        \n",
    "        # Store final model\n",
    "        self.final_model = {\n",
    "            'model': rf,\n",
    "            'scaler': scaler,\n",
    "            'feature_importance': feature_importance,\n",
    "            'cv_scores': cv_scores,\n",
    "            'test_auc': test_auc,\n",
    "            'feature_cols': feature_cols,\n",
    "            'confusion_matrix': confusion_matrix(y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        return self.final_model\n",
    "    \n",
    "    def plot_multi_night_results(self):\n",
    "        \"\"\"\n",
    "        Create comprehensive visualizations for multi-night analysis.\n",
    "        \"\"\"\n",
    "        print(\"\\n=== Creating Multi-Night Visualizations ===\")\n",
    "        \n",
    "        if self.aggregated_features is None:\n",
    "            print(\"❌ No aggregated features available\")\n",
    "            return\n",
    "        \n",
    "        # 1. Per-night OSA distribution\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        \n",
    "        plt.subplot(2, 3, 1)\n",
    "        night_counts = self.aggregated_features.groupby(['night_id', 'binary_label']).size().unstack(fill_value=0)\n",
    "        night_counts.plot(kind='bar', ax=plt.gca(), color=['lightblue', 'salmon'])\n",
    "        plt.title('OSA Distribution by Night')\n",
    "        plt.xlabel('Night ID')\n",
    "        plt.ylabel('Sample Count')\n",
    "        plt.legend(['Non-OSA', 'OSA'])\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # 2. Overall class distribution\n",
    "        plt.subplot(2, 3, 2)\n",
    "        class_counts = self.aggregated_features['binary_label'].value_counts()\n",
    "        plt.pie(class_counts.values, labels=class_counts.index, autopct='%1.1f%%', colors=['lightblue', 'salmon'])\n",
    "        plt.title('Overall OSA Distribution')\n",
    "        \n",
    "        # 3. Top discriminative features (if comparison available)\n",
    "        if self.aggregated_comparison is not None:\n",
    "            plt.subplot(2, 3, 3)\n",
    "            top_features = self.aggregated_comparison[self.aggregated_comparison['significant']].nlargest(10, lambda x: abs(x)['cohens_d'])\n",
    "            plt.barh(range(len(top_features)), top_features['cohens_d'])\n",
    "            plt.yticks(range(len(top_features)), [f[:20] + '...' if len(f) > 20 else f for f in top_features['feature']])\n",
    "            plt.xlabel('Effect Size (Cohen\\'s d)')\n",
    "            plt.title('Top 10 Discriminative Features')\n",
    "        \n",
    "        # 4. Model performance (if model available)\n",
    "        if self.final_model is not None:\n",
    "            plt.subplot(2, 3, 4)\n",
    "            cv_scores = self.final_model['cv_scores']\n",
    "            plt.boxplot(cv_scores)\n",
    "            plt.ylabel('AUC Score')\n",
    "            plt.title(f'Cross-Validation Performance\\n(Mean: {cv_scores.mean():.3f})')\n",
    "            \n",
    "            # 5. Feature importance\n",
    "            plt.subplot(2, 3, 5)\n",
    "            top_importance = self.final_model['feature_importance'].head(10)\n",
    "            plt.barh(range(len(top_importance)), top_importance['importance'])\n",
    "            plt.yticks(range(len(top_importance)), [f[:20] + '...' if len(f) > 20 else f for f in top_importance['feature']])\n",
    "            plt.xlabel('Importance')\n",
    "            plt.title('Top 10 Feature Importance')\n",
    "            \n",
    "            # 6. Confusion matrix\n",
    "            plt.subplot(2, 3, 6)\n",
    "            cm = self.final_model['confusion_matrix']\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                       xticklabels=['Non-OSA', 'OSA'], yticklabels=['Non-OSA', 'OSA'])\n",
    "            plt.title('Confusion Matrix')\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('Actual')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def save_results(self, output_directory='multi_night_osa_results'):\n",
    "        \"\"\"\n",
    "        Save all results to files.\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== Saving Results to {output_directory} ===\")\n",
    "        \n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        \n",
    "        # Save aggregated features\n",
    "        if self.aggregated_features is not None:\n",
    "            self.aggregated_features.to_csv(f'{output_directory}/aggregated_features.csv', index=False)\n",
    "            print(\"✅ Saved aggregated_features.csv\")\n",
    "        \n",
    "        # Save statistical comparison\n",
    "        if self.aggregated_comparison is not None:\n",
    "            self.aggregated_comparison.to_csv(f'{output_directory}/statistical_comparison.csv', index=False)\n",
    "            print(\"✅ Saved statistical_comparison.csv\")\n",
    "        \n",
    "        # Save model and feature importance\n",
    "        if self.final_model is not None:\n",
    "            self.final_model['feature_importance'].to_csv(f'{output_directory}/feature_importance.csv', index=False)\n",
    "            \n",
    "            # Save model summary\n",
    "            model_summary = {\n",
    "                'cv_mean_auc': self.final_model['cv_scores'].mean(),\n",
    "                'cv_std_auc': self.final_model['cv_scores'].std(),\n",
    "                'test_auc': self.final_model['test_auc'],\n",
    "                'total_samples': len(self.aggregated_features),\n",
    "                'osa_samples': self.aggregated_features['is_osa'].sum(),\n",
    "                'n_nights': self.aggregated_features['night_id'].nunique(),\n",
    "                'n_features': len(self.final_model['feature_cols'])\n",
    "            }\n",
    "            \n",
    "            pd.DataFrame([model_summary]).to_csv(f'{output_directory}/model_summary.csv', index=False)\n",
    "            print(\"✅ Saved feature_importance.csv and model_summary.csv\")\n",
    "        \n",
    "        print(f\"All results saved to {output_directory}/\")\n",
    "    \n",
    "    # Include all your existing helper functions here\n",
    "    # (process_existing_events_df, create_labeled_respiratory_data, etc.)\n",
    "    # For brevity, I'll just reference them - you can copy them from your existing code\n",
    "    \n",
    "    def process_existing_events_df(self, events_df):\n",
    "        # Your existing function\n",
    "        pass\n",
    "    \n",
    "    def create_labeled_respiratory_data(self, respeck_df, events_df):\n",
    "        # Your existing function\n",
    "        pass\n",
    "    \n",
    "    def extract_features_by_precise_labels_fixed(self, labeled_df, feature_extraction_func):\n",
    "        # Your existing function\n",
    "        pass\n",
    "    \n",
    "    def aggregate_breath_features(self, features_list):\n",
    "        # Your existing function\n",
    "        pass\n",
    "    \n",
    "    def create_osa_binary_classification(self, features_df):\n",
    "        # Your existing function\n",
    "        pass\n",
    "    \n",
    "    def statistical_comparison_osa_vs_rest(self, features_df_binary):\n",
    "        # Your existing function\n",
    "        pass\n",
    "\n",
    "# Usage example:\n",
    "\"\"\"\n",
    "# Example usage for multi-night analysis\n",
    "\n",
    "# Prepare your data - list of dictionaries with night data\n",
    "night_data_list = [\n",
    "    {\n",
    "        'night_id': '2025-04-04',\n",
    "        'respeck_data': respeck_df_night1,  # Your respeck DataFrame for night 1\n",
    "        'events_data': events_df_night1     # Your events DataFrame for night 1\n",
    "    },\n",
    "    {\n",
    "        'night_id': '2025-04-05', \n",
    "        'respeck_data': respeck_df_night2,\n",
    "        'events_data': events_df_night2\n",
    "    },\n",
    "    # Add more nights...\n",
    "]\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = MultiNightOSAAnalyzer('path/to/data/directory')\n",
    "\n",
    "# Process all nights\n",
    "processing_summary = analyzer.process_multiple_nights(night_data_list, calculate_TS_breathFeatures)\n",
    "\n",
    "# Aggregate results\n",
    "aggregated_features = analyzer.aggregate_all_nights()\n",
    "\n",
    "# Statistical analysis\n",
    "comparison_results = analyzer.statistical_analysis_aggregated()\n",
    "\n",
    "# Build final model\n",
    "final_model = analyzer.build_final_model(test_size=0.2, cv_folds=10)\n",
    "\n",
    "# Create visualizations\n",
    "analyzer.plot_multi_night_results()\n",
    "\n",
    "# Save all results\n",
    "analyzer.save_results('multi_night_osa_analysis_results')\n",
    "\n",
    "print(\"Multi-night OSA analysis complete!\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7228b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "class AutomatedMultiNightLoader:\n",
    "    \"\"\"\n",
    "    Automatically load multiple nights of data from organized directory structure.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_directory='../data/bishkek_csr/03_train_ready'):\n",
    "        \"\"\"\n",
    "        Initialize with base directory containing the organized data.\n",
    "        \n",
    "        Expected structure:\n",
    "        base_directory/\n",
    "        ├── respeck/\n",
    "        │   ├── 05-04-2025_respeck.csv\n",
    "        │   ├── 06-04-2025_respeck.csv\n",
    "        │   └── ...\n",
    "        ├── nasal_files/\n",
    "        │   ├── 05-04-2025_nasal.csv\n",
    "        │   ├── 06-04-2025_nasal.csv\n",
    "        │   └── ...\n",
    "        └── event_exports/\n",
    "            ├── 05-04-2025_event_export.csv\n",
    "            ├── 06-04-2025_event_export.csv\n",
    "            └── ...\n",
    "        \"\"\"\n",
    "        self.base_directory = base_directory\n",
    "        self.respeck_dir = os.path.join(base_directory, 'respeck')\n",
    "        self.nasal_dir = os.path.join(base_directory, 'nasal_files')\n",
    "        self.events_dir = os.path.join(base_directory, 'event_exports')\n",
    "        self.timezone = pytz.timezone('Asia/Bishkek')\n",
    "        \n",
    "    def find_all_dates(self):\n",
    "        \"\"\"\n",
    "        Scan directories to find all available dates with complete data sets.\n",
    "        \n",
    "        Returns:\n",
    "        - available_dates: List of dates that have all three required files\n",
    "        \"\"\"\n",
    "        print(\"=== Scanning for Available Nights ===\")\n",
    "        \n",
    "        # Get all respeck files and extract dates\n",
    "        respeck_files = glob.glob(os.path.join(self.respeck_dir, '*_respeck.csv'))\n",
    "        nasal_files = glob.glob(os.path.join(self.nasal_dir, '*_nasal.csv'))\n",
    "        event_files = glob.glob(os.path.join(self.events_dir, '*_event_export.csv'))\n",
    "        \n",
    "        print(f\"Found files:\")\n",
    "        print(f\"  Respeck files: {len(respeck_files)}\")\n",
    "        print(f\"  Nasal files: {len(nasal_files)}\")\n",
    "        print(f\"  Event files: {len(event_files)}\")\n",
    "        \n",
    "        # Extract dates from filenames\n",
    "        def extract_date(filename):\n",
    "            # Extract date pattern like \"05-04-2025\" from filename\n",
    "            match = re.search(r'(\\d{2}-\\d{2}-\\d{4})', os.path.basename(filename))\n",
    "            return match.group(1) if match else None\n",
    "        \n",
    "        respeck_dates = set([extract_date(f) for f in respeck_files if extract_date(f)])\n",
    "        nasal_dates = set([extract_date(f) for f in nasal_files if extract_date(f)])\n",
    "        event_dates = set([extract_date(f) for f in event_files if extract_date(f)])\n",
    "        \n",
    "        # Find dates that have all three types of files\n",
    "        complete_dates = respeck_dates & nasal_dates & event_dates\n",
    "        \n",
    "        # Sort dates chronologically\n",
    "        complete_dates = sorted(list(complete_dates))\n",
    "        \n",
    "        print(f\"\\nDates with complete data sets: {len(complete_dates)}\")\n",
    "        for date in complete_dates:\n",
    "            print(f\"  {date}\")\n",
    "        \n",
    "        if len(complete_dates) == 0:\n",
    "            print(\"❌ No complete data sets found!\")\n",
    "            print(f\"Respeck dates: {sorted(respeck_dates)}\")\n",
    "            print(f\"Nasal dates: {sorted(nasal_dates)}\")\n",
    "            print(f\"Event dates: {sorted(event_dates)}\")\n",
    "        \n",
    "        return complete_dates\n",
    "    \n",
    "    def load_single_night(self, date):\n",
    "        \"\"\"\n",
    "        Load data for a single night using the same format as your existing code.\n",
    "        \n",
    "        Parameters:\n",
    "        - date: Date string in format \"DD-MM-YYYY\"\n",
    "        \n",
    "        Returns:\n",
    "        - night_data: Dictionary with loaded data or error info\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"Loading data for {date}...\")\n",
    "            \n",
    "            # Construct file paths\n",
    "            respeck_file = os.path.join(self.respeck_dir, f'{date}_respeck.csv')\n",
    "            nasal_file = os.path.join(self.nasal_dir, f'{date}_nasal.csv')\n",
    "            events_file = os.path.join(self.events_dir, f'{date}_event_export.csv')\n",
    "            \n",
    "            # Verify files exist\n",
    "            for file_path, file_type in [(respeck_file, 'respeck'), (nasal_file, 'nasal'), (events_file, 'events')]:\n",
    "                if not os.path.exists(file_path):\n",
    "                    raise FileNotFoundError(f\"{file_type} file not found: {file_path}\")\n",
    "            \n",
    "            # Load Respeck data (using your exact format)\n",
    "            respeck_df = pd.read_csv(respeck_file)\n",
    "            respeck_df['timestamp'] = pd.to_datetime(respeck_df['alignedTimestamp'], unit='ms')\n",
    "            respeck_df['timestamp'] = respeck_df['timestamp'].dt.tz_localize('UTC').dt.tz_convert(self.timezone)\n",
    "            respeck_df.set_index('timestamp', inplace=True)\n",
    "            \n",
    "            # Load PSG data (using your exact format)\n",
    "            psg_df = pd.read_csv(nasal_file)\n",
    "            psg_df['timestamp'] = pd.to_datetime(psg_df['UnixTimestamp'], unit='ms')\n",
    "            psg_df['timestamp'] = psg_df['timestamp'].dt.tz_localize('UTC').dt.tz_convert(self.timezone)\n",
    "            psg_df.set_index('timestamp', inplace=True)\n",
    "            \n",
    "            # Load Labels data (using your exact format)\n",
    "            labels_df = pd.read_csv(events_file)\n",
    "            labels_df['timestamp'] = pd.to_datetime(labels_df['UnixTimestamp'], unit='ms')\n",
    "            labels_df['timestamp'] = labels_df['timestamp'].dt.tz_localize('UTC').dt.tz_convert(self.timezone)\n",
    "            labels_df.set_index('timestamp', inplace=True)\n",
    "            \n",
    "            print(f\"✅ {date}: Loaded {len(respeck_df)} respeck, {len(psg_df)} PSG, {len(labels_df)} events\")\n",
    "            \n",
    "            return {\n",
    "                'night_id': date,\n",
    "                'respeck_data': respeck_df,\n",
    "                'psg_data': psg_df,\n",
    "                'events_data': labels_df,\n",
    "                'success': True\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ {date}: Error loading data - {e}\")\n",
    "            return {\n",
    "                'night_id': date,\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def load_all_nights(self, max_nights=None):\n",
    "        \"\"\"\n",
    "        Load data for all available nights.\n",
    "        \n",
    "        Parameters:\n",
    "        - max_nights: Maximum number of nights to load (None for all)\n",
    "        \n",
    "        Returns:\n",
    "        - night_data_list: List of night data dictionaries\n",
    "        - loading_summary: Summary of loading results\n",
    "        \"\"\"\n",
    "        print(\"=== LOADING ALL NIGHTS ===\")\n",
    "        \n",
    "        # Find available dates\n",
    "        available_dates = self.find_all_dates()\n",
    "        \n",
    "        if not available_dates:\n",
    "            return [], {'successful': 0, 'failed': 0, 'errors': []}\n",
    "        \n",
    "        # Limit number of nights if specified\n",
    "        if max_nights is not None:\n",
    "            available_dates = available_dates[:max_nights]\n",
    "            print(f\"Loading first {len(available_dates)} nights (limited by max_nights={max_nights})\")\n",
    "        \n",
    "        # Load each night\n",
    "        night_data_list = []\n",
    "        successful_loads = 0\n",
    "        failed_loads = 0\n",
    "        errors = []\n",
    "        \n",
    "        for date in available_dates:\n",
    "            night_data = self.load_single_night(date)\n",
    "            \n",
    "            if night_data['success']:\n",
    "                night_data_list.append(night_data)\n",
    "                successful_loads += 1\n",
    "            else:\n",
    "                failed_loads += 1\n",
    "                errors.append(f\"{date}: {night_data['error']}\")\n",
    "        \n",
    "        loading_summary = {\n",
    "            'total_nights': len(available_dates),\n",
    "            'successful': successful_loads,\n",
    "            'failed': failed_loads,\n",
    "            'errors': errors\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n=== Loading Summary ===\")\n",
    "        print(f\"Total nights attempted: {loading_summary['total_nights']}\")\n",
    "        print(f\"Successfully loaded: {loading_summary['successful']}\")\n",
    "        print(f\"Failed to load: {loading_summary['failed']}\")\n",
    "        \n",
    "        if errors:\n",
    "            print(f\"Errors encountered:\")\n",
    "            for error in errors:\n",
    "                print(f\"  {error}\")\n",
    "        \n",
    "        return night_data_list, loading_summary\n",
    "    \n",
    "    def preview_data_structure(self):\n",
    "        \"\"\"\n",
    "        Preview the data structure and file organization.\n",
    "        \"\"\"\n",
    "        print(\"=== Data Structure Preview ===\")\n",
    "        \n",
    "        for subdir in ['respeck', 'nasal_files', 'event_exports']:\n",
    "            full_path = os.path.join(self.base_directory, subdir)\n",
    "            if os.path.exists(full_path):\n",
    "                files = glob.glob(os.path.join(full_path, '*.csv'))\n",
    "                print(f\"{subdir}/: {len(files)} files\")\n",
    "                if files:\n",
    "                    # Show first few filenames\n",
    "                    for file in sorted(files)[:3]:\n",
    "                        print(f\"  {os.path.basename(file)}\")\n",
    "                    if len(files) > 3:\n",
    "                        print(f\"  ... and {len(files) - 3} more\")\n",
    "            else:\n",
    "                print(f\"{subdir}/: Directory not found\")\n",
    "\n",
    "# Enhanced MultiNightOSAAnalyzer that works with the automated loader\n",
    "class EnhancedMultiNightOSAAnalyzer(MultiNightOSAAnalyzer):\n",
    "    \"\"\"\n",
    "    Enhanced version that works with the automated data loader.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_directory='../data/bishkek_csr/03_train_ready'):\n",
    "        super().__init__(base_directory)\n",
    "        self.loader = AutomatedMultiNightLoader(base_directory)\n",
    "    \n",
    "    def load_and_process_all_nights(self, feature_extraction_func, max_nights=None):\n",
    "        \"\"\"\n",
    "        Automatically load and process all available nights.\n",
    "        \n",
    "        Parameters:\n",
    "        - feature_extraction_func: Your feature extraction function\n",
    "        - max_nights: Maximum number of nights to process (None for all)\n",
    "        \n",
    "        Returns:\n",
    "        - processing_summary: Complete summary of loading and processing\n",
    "        \"\"\"\n",
    "        print(\"=== AUTOMATED MULTI-NIGHT OSA ANALYSIS ===\")\n",
    "        \n",
    "        # Step 1: Load all night data\n",
    "        night_data_list, loading_summary = self.loader.load_all_nights(max_nights)\n",
    "        \n",
    "        if not night_data_list:\n",
    "            print(\"❌ No data loaded successfully\")\n",
    "            return loading_summary\n",
    "        \n",
    "        # Step 2: Process all loaded nights\n",
    "        processing_summary = self.process_multiple_nights(night_data_list, feature_extraction_func)\n",
    "        \n",
    "        # Combine loading and processing summaries\n",
    "        combined_summary = {\n",
    "            'loading': loading_summary,\n",
    "            'processing': processing_summary,\n",
    "            'overall_success': len(self.night_results),\n",
    "            'total_attempted': loading_summary['total_nights']\n",
    "        }\n",
    "        \n",
    "        return combined_summary\n",
    "\n",
    "# Complete usage example\n",
    "def run_complete_multi_night_analysis(base_directory='../data/bishkek_csr/03_train_ready', \n",
    "                                    max_nights=None):\n",
    "    \"\"\"\n",
    "    Run complete multi-night OSA analysis with automated data loading.\n",
    "    \n",
    "    Parameters:\n",
    "    - base_directory: Path to your data directory\n",
    "    - max_nights: Maximum nights to analyze (None for all)\n",
    "    \n",
    "    Returns:\n",
    "    - analyzer: Complete analyzer with all results\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🌙 COMPLETE MULTI-NIGHT OSA ANALYSIS 🌙\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: Initialize analyzer\n",
    "    analyzer = EnhancedMultiNightOSAAnalyzer(base_directory)\n",
    "    \n",
    "    # Step 2: Preview available data\n",
    "    analyzer.loader.preview_data_structure()\n",
    "    \n",
    "    # Step 3: Load and process all nights\n",
    "    processing_summary = analyzer.load_and_process_all_nights(\n",
    "        calculate_TS_breathFeatures,  # Your feature extraction function\n",
    "        max_nights=max_nights\n",
    "    )\n",
    "    \n",
    "    if processing_summary['overall_success'] == 0:\n",
    "        print(\"❌ No nights processed successfully\")\n",
    "        return analyzer\n",
    "    \n",
    "    # Step 4: Aggregate results\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    aggregated_features = analyzer.aggregate_all_nights()\n",
    "    \n",
    "    # Step 5: Statistical analysis\n",
    "    comparison_results = analyzer.statistical_analysis_aggregated()\n",
    "    \n",
    "    # Step 6: Build final model\n",
    "    final_model = analyzer.build_final_model(test_size=0.2, cv_folds=10)\n",
    "    \n",
    "    # Step 7: Create visualizations\n",
    "    analyzer.plot_multi_night_results()\n",
    "    \n",
    "    # Step 8: Save all results\n",
    "    analyzer.save_results('complete_multi_night_osa_analysis')\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"✅ COMPLETE MULTI-NIGHT ANALYSIS FINISHED!\")\n",
    "    print(f\"📊 Processed {processing_summary['overall_success']} nights successfully\")\n",
    "    print(f\"💾 Results saved to 'complete_multi_night_osa_analysis/'\")\n",
    "    \n",
    "    return analyzer\n",
    "\n",
    "# Simple usage:\n",
    "\n",
    "# Run complete analysis on all available nights\n",
    "analyzer = run_complete_multi_night_analysis()\n",
    "\n",
    "# Or limit to first 5 nights for testing\n",
    "analyzer = run_complete_multi_night_analysis(max_nights=5)\n",
    "\n",
    "# Or specify different directory\n",
    "analyzer = run_complete_multi_night_analysis(\n",
    "    base_directory='../data/bishkek_csr/03_train_ready',\n",
    "    max_nights=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3ba3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import glob\n",
    "import pytz\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class CompleteMultiNightOSAAnalyzer:\n",
    "    \"\"\"\n",
    "    Complete multi-night OSA analysis with all required functions implemented.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_directory='../data/bishkek_csr/03_train_ready'):\n",
    "        self.base_directory = base_directory\n",
    "        self.respeck_dir = os.path.join(base_directory, 'respeck')\n",
    "        self.nasal_dir = os.path.join(base_directory, 'nasal_files')\n",
    "        self.events_dir = os.path.join(base_directory, 'event_exports')\n",
    "        self.timezone = pytz.timezone('Asia/Bishkek')\n",
    "        self.night_results = {}\n",
    "        self.aggregated_features = None\n",
    "        self.aggregated_comparison = None\n",
    "        self.final_model = None\n",
    "    \n",
    "    def find_all_dates(self):\n",
    "        \"\"\"Find all available dates with complete data sets.\"\"\"\n",
    "        print(\"=== Scanning for Available Nights ===\")\n",
    "        \n",
    "        respeck_files = glob.glob(os.path.join(self.respeck_dir, '*_respeck.csv'))\n",
    "        nasal_files = glob.glob(os.path.join(self.nasal_dir, '*_nasal.csv'))\n",
    "        event_files = glob.glob(os.path.join(self.events_dir, '*_event_export.csv'))\n",
    "        \n",
    "        print(f\"Found files:\")\n",
    "        print(f\"  Respeck files: {len(respeck_files)}\")\n",
    "        print(f\"  Nasal files: {len(nasal_files)}\")\n",
    "        print(f\"  Event files: {len(event_files)}\")\n",
    "        \n",
    "        def extract_date(filename):\n",
    "            match = re.search(r'(\\d{2}-\\d{2}-\\d{4})', os.path.basename(filename))\n",
    "            return match.group(1) if match else None\n",
    "        \n",
    "        respeck_dates = set([extract_date(f) for f in respeck_files if extract_date(f)])\n",
    "        nasal_dates = set([extract_date(f) for f in nasal_files if extract_date(f)])\n",
    "        event_dates = set([extract_date(f) for f in event_files if extract_date(f)])\n",
    "        \n",
    "        complete_dates = respeck_dates & nasal_dates & event_dates\n",
    "        complete_dates = sorted(list(complete_dates))\n",
    "        \n",
    "        print(f\"\\nDates with complete data sets: {len(complete_dates)}\")\n",
    "        for date in complete_dates:\n",
    "            print(f\"  {date}\")\n",
    "        \n",
    "        return complete_dates\n",
    "    \n",
    "    def load_single_night(self, date):\n",
    "        \"\"\"Load data for a single night.\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading data for {date}...\")\n",
    "            \n",
    "            respeck_file = os.path.join(self.respeck_dir, f'{date}_respeck.csv')\n",
    "            nasal_file = os.path.join(self.nasal_dir, f'{date}_nasal.csv')\n",
    "            events_file = os.path.join(self.events_dir, f'{date}_event_export.csv')\n",
    "            \n",
    "            # Load Respeck data\n",
    "            respeck_df = pd.read_csv(respeck_file)\n",
    "            respeck_df['timestamp'] = pd.to_datetime(respeck_df['alignedTimestamp'], unit='ms')\n",
    "            respeck_df['timestamp'] = respeck_df['timestamp'].dt.tz_localize('UTC').dt.tz_convert(self.timezone)\n",
    "            respeck_df.set_index('timestamp', inplace=True)\n",
    "            \n",
    "            # Load Events data\n",
    "            labels_df = pd.read_csv(events_file)\n",
    "            labels_df['timestamp'] = pd.to_datetime(labels_df['UnixTimestamp'], unit='ms')\n",
    "            labels_df['timestamp'] = labels_df['timestamp'].dt.tz_localize('UTC').dt.tz_convert(self.timezone)\n",
    "            labels_df.set_index('timestamp', inplace=True)\n",
    "            \n",
    "            print(f\"✅ {date}: Loaded {len(respeck_df)} respeck, {len(labels_df)} events\")\n",
    "            \n",
    "            return {\n",
    "                'night_id': date,\n",
    "                'respeck_data': respeck_df,\n",
    "                'events_data': labels_df,\n",
    "                'success': True\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ {date}: Error loading data - {e}\")\n",
    "            return {\n",
    "                'night_id': date,\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def process_existing_events_df(self, events_df):\n",
    "        \"\"\"Process events DataFrame to add proper timestamps.\"\"\"\n",
    "        print(f\"Processing existing events DataFrame with {len(events_df)} events\")\n",
    "        \n",
    "        processed_df = events_df.copy()\n",
    "        processed_df['start_time'] = pd.to_datetime(processed_df['UnixTimestamp'], unit='ms').dt.tz_localize('UTC').dt.tz_convert(self.timezone)\n",
    "        processed_df['duration_seconds'] = processed_df['Duration'].str.replace(',', '.').astype(float)\n",
    "        processed_df['end_time'] = processed_df['start_time'] + pd.to_timedelta(processed_df['duration_seconds'], unit='s')\n",
    "        processed_df = processed_df.sort_values('start_time').reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Event types: {processed_df['Event'].value_counts().to_dict()}\")\n",
    "        return processed_df\n",
    "    \n",
    "    def create_labeled_respiratory_data(self, respeck_df, events_df, breathing_col='breathingSignal'):\n",
    "        \"\"\"Create labeled respiratory data with precise event timing.\"\"\"\n",
    "        print(\"Creating labeled respiratory data...\")\n",
    "        \n",
    "        if not isinstance(respeck_df.index, pd.DatetimeIndex):\n",
    "            respeck_df.index = pd.to_datetime(respeck_df.index)\n",
    "        \n",
    "        labeled_df = respeck_df.copy()\n",
    "        labeled_df['event_label'] = 'Normal'\n",
    "        labeled_df['event_duration'] = np.nan\n",
    "        labeled_df['event_id'] = np.nan\n",
    "        \n",
    "        respeck_start = labeled_df.index.min()\n",
    "        respeck_end = labeled_df.index.max()\n",
    "        events_start = events_df['start_time'].min()\n",
    "        events_end = events_df['end_time'].max()\n",
    "        \n",
    "        print(f\"Respeck data: {respeck_start} to {respeck_end}\")\n",
    "        print(f\"Events data: {events_start} to {events_end}\")\n",
    "        \n",
    "        # Handle timezone issues\n",
    "        if respeck_start.tz is not None and events_start.tz is None:\n",
    "            events_df = events_df.copy()\n",
    "            events_df['start_time'] = events_df['start_time'].dt.tz_localize(respeck_start.tz)\n",
    "            events_df['end_time'] = events_df['end_time'].dt.tz_localize(respeck_start.tz)\n",
    "            events_start = events_df['start_time'].min()\n",
    "            events_end = events_df['end_time'].max()\n",
    "        elif respeck_start.tz is None and events_start.tz is not None:\n",
    "            labeled_df.index = labeled_df.index.tz_localize(events_start.tz)\n",
    "            respeck_start = labeled_df.index.min()\n",
    "            respeck_end = labeled_df.index.max()\n",
    "        \n",
    "        # Check for time overlap\n",
    "        overlap_start = max(respeck_start, events_start)\n",
    "        overlap_end = min(respeck_end, events_end)\n",
    "        \n",
    "        if overlap_end <= overlap_start:\n",
    "            # Try adding 24 hours to events if they're from previous day\n",
    "            time_diff = events_start - respeck_start\n",
    "            if time_diff.total_seconds() < -12*3600:\n",
    "                events_df = events_df.copy()\n",
    "                events_df['start_time'] = events_df['start_time'] + pd.Timedelta(hours=24)\n",
    "                events_df['end_time'] = events_df['end_time'] + pd.Timedelta(hours=24)\n",
    "                events_start = events_df['start_time'].min()\n",
    "                events_end = events_df['end_time'].max()\n",
    "                overlap_start = max(respeck_start, events_start)\n",
    "                overlap_end = min(respeck_end, events_end)\n",
    "        \n",
    "        if overlap_end > overlap_start:\n",
    "            overlap_duration = overlap_end - overlap_start\n",
    "            print(f\"✅ Overlap found: {overlap_start} to {overlap_end} ({overlap_duration})\")\n",
    "        else:\n",
    "            print(f\"❌ No overlap found between respeck and events data\")\n",
    "            return labeled_df\n",
    "        \n",
    "        # Find overlapping events\n",
    "        overlapping_events = events_df[\n",
    "            (events_df['end_time'] >= respeck_start) & \n",
    "            (events_df['start_time'] <= respeck_end)\n",
    "        ].copy()\n",
    "        \n",
    "        print(f\"Found {len(overlapping_events)} events overlapping with respiratory data\")\n",
    "        \n",
    "        # Label each event period\n",
    "        events_applied = 0\n",
    "        total_event_samples = 0\n",
    "        \n",
    "        for idx, event in overlapping_events.iterrows():\n",
    "            event_mask = (\n",
    "                (labeled_df.index >= event['start_time']) & \n",
    "                (labeled_df.index <= event['end_time'])\n",
    "            )\n",
    "            \n",
    "            data_points_in_event = event_mask.sum()\n",
    "            \n",
    "            if data_points_in_event > 0:\n",
    "                labeled_df.loc[event_mask, 'event_label'] = event['Event']\n",
    "                labeled_df.loc[event_mask, 'event_duration'] = event['duration_seconds']\n",
    "                labeled_df.loc[event_mask, 'event_id'] = idx\n",
    "                events_applied += 1\n",
    "                total_event_samples += data_points_in_event\n",
    "                \n",
    "                if events_applied <= 5:\n",
    "                    print(f\"  Event {idx}: {event['Event']} at {event['start_time']} \"\n",
    "                          f\"({data_points_in_event} data points)\")\n",
    "        \n",
    "        print(f\"Successfully applied {events_applied} events to respiratory data\")\n",
    "        print(f\"Total event samples: {total_event_samples}\")\n",
    "        \n",
    "        # Summary statistics\n",
    "        label_counts = labeled_df['event_label'].value_counts()\n",
    "        print(f\"\\nLabel distribution:\")\n",
    "        for label, count in label_counts.items():\n",
    "            percentage = (count / len(labeled_df)) * 100\n",
    "            print(f\"  {label}: {count:,} samples ({percentage:.1f}%)\")\n",
    "        \n",
    "        return labeled_df\n",
    "    \n",
    "    def extract_features_by_precise_labels_fixed(self, labeled_df, feature_extraction_func, \n",
    "                                               segment_length_minutes=5):\n",
    "        \"\"\"Extract features using precise event labels.\"\"\"\n",
    "        print(f\"Extracting features using precise labels...\")\n",
    "        \n",
    "        all_features = []\n",
    "        \n",
    "        # Group consecutive samples with the same label\n",
    "        labeled_df['label_group'] = (labeled_df['event_label'] != labeled_df['event_label'].shift()).cumsum()\n",
    "        \n",
    "        event_periods = []\n",
    "        for group_id, group_data in labeled_df.groupby('label_group'):\n",
    "            if len(group_data) < 50:  # Skip very short segments\n",
    "                continue\n",
    "                \n",
    "            event_type = group_data['event_label'].iloc[0]\n",
    "            start_time = group_data.index.min()\n",
    "            end_time = group_data.index.max()\n",
    "            duration = (end_time - start_time).total_seconds()\n",
    "            \n",
    "            event_periods.append({\n",
    "                'event_type': event_type,\n",
    "                'start_time': start_time,\n",
    "                'end_time': end_time,\n",
    "                'duration': duration,\n",
    "                'sample_count': len(group_data)\n",
    "            })\n",
    "        \n",
    "        print(f\"Found {len(event_periods)} distinct event periods\")\n",
    "        \n",
    "        # Show breakdown by event type\n",
    "        period_counts = {}\n",
    "        for period in event_periods:\n",
    "            event_type = period['event_type']\n",
    "            period_counts[event_type] = period_counts.get(event_type, 0) + 1\n",
    "        \n",
    "        print(\"Periods by event type:\")\n",
    "        for event_type, count in sorted(period_counts.items()):\n",
    "            print(f\"  {event_type}: {count} periods\")\n",
    "        \n",
    "        successful_extractions = 0\n",
    "        for i, period in enumerate(event_periods):\n",
    "            try:\n",
    "                period_data = labeled_df[\n",
    "                    (labeled_df.index >= period['start_time']) & \n",
    "                    (labeled_df.index <= period['end_time'])\n",
    "                ]\n",
    "                \n",
    "                if 'breathingSignal' not in period_data.columns or len(period_data) < 50:\n",
    "                    continue\n",
    "                \n",
    "                # Extract features using the timestamp column\n",
    "                features = feature_extraction_func(\n",
    "                    period_data['timestamp'].to_numpy(),\n",
    "                    period_data['breathingSignal'].values\n",
    "                )\n",
    "                \n",
    "                if features is not None:\n",
    "                    features['event_type'] = period['event_type']\n",
    "                    features['event_time'] = period['start_time']\n",
    "                    features['period_duration'] = period['duration']\n",
    "                    features['sample_count'] = period['sample_count']\n",
    "                    \n",
    "                    all_features.append(features)\n",
    "                    successful_extractions += 1\n",
    "                    \n",
    "                    if i % 100 == 0:\n",
    "                        print(f\"  Processed {i+1}/{len(event_periods)} periods... ({successful_extractions} successful)\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                if i < 5:\n",
    "                    print(f\"Error processing period {i} ({period['event_type']}): {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Successfully extracted features for {successful_extractions} out of {len(event_periods)} periods\")\n",
    "        \n",
    "        # Show successful extractions by event type\n",
    "        success_counts = {}\n",
    "        for features in all_features:\n",
    "            event_type = features['event_type']\n",
    "            success_counts[event_type] = success_counts.get(event_type, 0) + 1\n",
    "        \n",
    "        print(\"Successful extractions by event type:\")\n",
    "        for event_type, count in sorted(success_counts.items()):\n",
    "            print(f\"  {event_type}: {count} periods\")\n",
    "        \n",
    "        return all_features\n",
    "    \n",
    "    def aggregate_breath_features(self, features_list):\n",
    "        \"\"\"Convert list of feature dictionaries to aggregated DataFrame.\"\"\"\n",
    "        aggregated_features = []\n",
    "        \n",
    "        for features in features_list:\n",
    "            event_features = {\n",
    "                'event_type': features['event_type'],\n",
    "                'event_time': features['event_time'],\n",
    "                'period_duration': features.get('period_duration', np.nan),\n",
    "                'sample_count': features.get('sample_count', np.nan)\n",
    "            }\n",
    "            \n",
    "            for feature_name, values in features.items():\n",
    "                if feature_name in ['event_type', 'event_time', 'period_duration', 'sample_count', 'timestamp', 'breathingSignal']:\n",
    "                    continue\n",
    "                \n",
    "                if hasattr(values, 'values'):\n",
    "                    values = values.values\n",
    "                \n",
    "                if isinstance(values, (list, np.ndarray, pd.Series)):\n",
    "                    try:\n",
    "                        values_array = np.array(values, dtype=float)\n",
    "                        values_clean = values_array[np.isfinite(values_array)]\n",
    "                        \n",
    "                        if len(values_clean) > 0:\n",
    "                            event_features[f'{feature_name}_mean'] = np.mean(values_clean)\n",
    "                            event_features[f'{feature_name}_std'] = np.std(values_clean)\n",
    "                            event_features[f'{feature_name}_median'] = np.median(values_clean)\n",
    "                            event_features[f'{feature_name}_min'] = np.min(values_clean)\n",
    "                            event_features[f'{feature_name}_max'] = np.max(values_clean)\n",
    "                            event_features[f'{feature_name}_range'] = np.max(values_clean) - np.min(values_clean)\n",
    "                            event_features[f'{feature_name}_q25'] = np.percentile(values_clean, 25)\n",
    "                            event_features[f'{feature_name}_q75'] = np.percentile(values_clean, 75)\n",
    "                            event_features[f'{feature_name}_iqr'] = np.percentile(values_clean, 75) - np.percentile(values_clean, 25)\n",
    "                            event_features[f'{feature_name}_skew'] = pd.Series(values_clean).skew()\n",
    "                            event_features[f'{feature_name}_kurtosis'] = pd.Series(values_clean).kurtosis()\n",
    "                            \n",
    "                            if np.mean(values_clean) != 0:\n",
    "                                event_features[f'{feature_name}_cv'] = np.std(values_clean) / np.abs(np.mean(values_clean))\n",
    "                            else:\n",
    "                                event_features[f'{feature_name}_cv'] = 0\n",
    "                                \n",
    "                            event_features[f'{feature_name}_count'] = len(values_clean)\n",
    "                            \n",
    "                            if feature_name in ['auc_values', 'breath_durations', 'inhalation_durations', 'exhalation_durations']:\n",
    "                                if len(values_clean) >= 2:\n",
    "                                    x = np.arange(len(values_clean))\n",
    "                                    slope = np.polyfit(x, values_clean, 1)[0] if len(values_clean) > 1 else 0\n",
    "                                    event_features[f'{feature_name}_trend'] = slope\n",
    "                                    \n",
    "                                    rate_of_change = np.mean(np.diff(values_clean)) if len(values_clean) > 1 else 0\n",
    "                                    event_features[f'{feature_name}_rate_change'] = rate_of_change\n",
    "                                    \n",
    "                    except (ValueError, TypeError) as e:\n",
    "                        continue\n",
    "                        \n",
    "                elif isinstance(values, (int, float)) and np.isfinite(values):\n",
    "                    event_features[feature_name] = values\n",
    "            \n",
    "            aggregated_features.append(event_features)\n",
    "        \n",
    "        return pd.DataFrame(aggregated_features)\n",
    "    \n",
    "    def create_osa_binary_classification(self, features_df):\n",
    "        \"\"\"Create binary OSA vs Non-OSA classification.\"\"\"\n",
    "        features_df_binary = features_df.copy()\n",
    "        features_df_binary['is_osa'] = (features_df_binary['event_type'] == 'Obstructive Apnea').astype(int)\n",
    "        features_df_binary['binary_label'] = features_df_binary['is_osa'].map({1: 'OSA', 0: 'Non-OSA'})\n",
    "        return features_df_binary\n",
    "    \n",
    "    def process_single_night(self, respeck_data, events_data, night_id, feature_extraction_func):\n",
    "        \"\"\"Process a single night's data.\"\"\"\n",
    "        print(f\"\\n=== Processing Night: {night_id} ===\")\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Process events\n",
    "            processed_events_df = self.process_existing_events_df(events_data)\n",
    "            \n",
    "            # Step 2: Create labeled respiratory data\n",
    "            labeled_df = self.create_labeled_respiratory_data(respeck_data, processed_events_df)\n",
    "            \n",
    "            # Step 3: Add timestamp column\n",
    "            labeled_df['timestamp'] = labeled_df.index\n",
    "            \n",
    "            # Step 4: Extract features\n",
    "            features_list = self.extract_features_by_precise_labels_fixed(\n",
    "                labeled_df, feature_extraction_func\n",
    "            )\n",
    "            \n",
    "            # Step 5: Aggregate features\n",
    "            if len(features_list) > 0:\n",
    "                features_df = self.aggregate_breath_features(features_list)\n",
    "                features_df['night_id'] = night_id\n",
    "                features_df_binary = self.create_osa_binary_classification(features_df)\n",
    "                \n",
    "                night_result = {\n",
    "                    'night_id': night_id,\n",
    "                    'labeled_df': labeled_df,\n",
    "                    'features_df': features_df_binary,\n",
    "                    'n_samples': len(features_df_binary),\n",
    "                    'n_osa': len(features_df_binary[features_df_binary['binary_label'] == 'OSA']),\n",
    "                    'n_non_osa': len(features_df_binary[features_df_binary['binary_label'] == 'Non-OSA']),\n",
    "                    'success': True\n",
    "                }\n",
    "                \n",
    "                print(f\"✅ Night {night_id}: {len(features_df_binary)} samples \"\n",
    "                      f\"({night_result['n_osa']} OSA, {night_result['n_non_osa']} Non-OSA)\")\n",
    "                \n",
    "                return night_result\n",
    "                \n",
    "            else:\n",
    "                print(f\"❌ Night {night_id}: No features extracted\")\n",
    "                return {'night_id': night_id, 'success': False, 'error': 'No features extracted'}\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Night {night_id}: Error - {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return {'night_id': night_id, 'success': False, 'error': str(e)}\n",
    "    \n",
    "    def load_and_process_all_nights(self, feature_extraction_func, max_nights=None):\n",
    "        \"\"\"Load and process all available nights.\"\"\"\n",
    "        print(\"=== AUTOMATED MULTI-NIGHT OSA ANALYSIS ===\")\n",
    "        \n",
    "        # Find available dates\n",
    "        available_dates = self.find_all_dates()\n",
    "        if not available_dates:\n",
    "            return {'successful_nights': 0, 'failed_nights': 0}\n",
    "        \n",
    "        if max_nights is not None:\n",
    "            available_dates = available_dates[:max_nights]\n",
    "            print(f\"Loading first {len(available_dates)} nights (limited by max_nights={max_nights})\")\n",
    "        \n",
    "        # Load and process each night\n",
    "        successful_nights = []\n",
    "        failed_nights = []\n",
    "        \n",
    "        for date in available_dates:\n",
    "            # Load night data\n",
    "            night_data = self.load_single_night(date)\n",
    "            \n",
    "            if night_data['success']:\n",
    "                # Process night data\n",
    "                result = self.process_single_night(\n",
    "                    night_data['respeck_data'],\n",
    "                    night_data['events_data'], \n",
    "                    night_data['night_id'],\n",
    "                    feature_extraction_func\n",
    "                )\n",
    "                \n",
    "                if result['success']:\n",
    "                    successful_nights.append(result)\n",
    "                    self.night_results[result['night_id']] = result\n",
    "                else:\n",
    "                    failed_nights.append(result)\n",
    "            else:\n",
    "                failed_nights.append(night_data)\n",
    "        \n",
    "        print(f\"\\n=== Processing Summary ===\")\n",
    "        print(f\"Successful nights: {len(successful_nights)}\")\n",
    "        print(f\"Failed nights: {len(failed_nights)}\")\n",
    "        \n",
    "        if failed_nights:\n",
    "            print(f\"Failed night details:\")\n",
    "            for failed in failed_nights:\n",
    "                print(f\"  {failed['night_id']}: {failed['error']}\")\n",
    "        \n",
    "        return {\n",
    "            'successful_nights': len(successful_nights),\n",
    "            'failed_nights': len(failed_nights),\n",
    "            'successful_results': successful_nights,\n",
    "            'failed_results': failed_nights\n",
    "        }\n",
    "    \n",
    "    def aggregate_all_nights(self):\n",
    "        \"\"\"Aggregate features from all successfully processed nights.\"\"\"\n",
    "        print(\"\\n=== Aggregating Multi-Night Data ===\")\n",
    "        \n",
    "        if not self.night_results:\n",
    "            print(\"❌ No night results to aggregate\")\n",
    "            return None\n",
    "        \n",
    "        all_features = []\n",
    "        night_summary = []\n",
    "        \n",
    "        for night_id, night_result in self.night_results.items():\n",
    "            features_df = night_result['features_df']\n",
    "            all_features.append(features_df)\n",
    "            \n",
    "            night_summary.append({\n",
    "                'night_id': night_id,\n",
    "                'total_samples': night_result['n_samples'],\n",
    "                'osa_samples': night_result['n_osa'],\n",
    "                'non_osa_samples': night_result['n_non_osa'],\n",
    "                'osa_percentage': (night_result['n_osa'] / night_result['n_samples']) * 100\n",
    "            })\n",
    "        \n",
    "        self.aggregated_features = pd.concat(all_features, ignore_index=True)\n",
    "        \n",
    "        print(f\"Aggregated data summary:\")\n",
    "        print(f\"  Total nights: {len(self.night_results)}\")\n",
    "        print(f\"  Total samples: {len(self.aggregated_features)}\")\n",
    "        \n",
    "        class_counts = self.aggregated_features['binary_label'].value_counts()\n",
    "        print(f\"  OSA samples: {class_counts.get('OSA', 0)}\")\n",
    "        print(f\"  Non-OSA samples: {class_counts.get('Non-OSA', 0)}\")\n",
    "        print(f\"  OSA percentage: {(class_counts.get('OSA', 0) / len(self.aggregated_features) * 100):.1f}%\")\n",
    "        \n",
    "        night_summary_df = pd.DataFrame(night_summary)\n",
    "        print(f\"\\nPer-night breakdown:\")\n",
    "        print(night_summary_df.to_string(index=False))\n",
    "        \n",
    "        return self.aggregated_features\n",
    "\n",
    "# Simple usage function\n",
    "def run_complete_multi_night_analysis(base_directory='../data/bishkek_csr/03_train_ready', \n",
    "                                    max_nights=None):\n",
    "    \"\"\"Run complete multi-night OSA analysis.\"\"\"\n",
    "    \n",
    "    print(\"🌙 COMPLETE MULTI-NIGHT OSA ANALYSIS 🌙\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    analyzer = CompleteMultiNightOSAAnalyzer(base_directory)\n",
    "    \n",
    "    # Load and process all nights\n",
    "    processing_summary = analyzer.load_and_process_all_nights(\n",
    "        calculate_TS_breathFeatures,\n",
    "        max_nights=max_nights\n",
    "    )\n",
    "    \n",
    "    if processing_summary['successful_nights'] == 0:\n",
    "        print(\"❌ No nights processed successfully\")\n",
    "        return analyzer\n",
    "    \n",
    "    # Aggregate results\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    aggregated_features = analyzer.aggregate_all_nights()\n",
    "    \n",
    "    if aggregated_features is not None:\n",
    "        print(\"✅ MULTI-NIGHT ANALYSIS COMPLETED!\")\n",
    "        print(f\"📊 Processed {processing_summary['successful_nights']} nights successfully\")\n",
    "        \n",
    "        # Show class distribution\n",
    "        class_counts = aggregated_features['binary_label'].value_counts()\n",
    "        print(f\"📈 OSA samples: {class_counts.get('OSA', 0)}\")\n",
    "        print(f\"📈 Non-OSA samples: {class_counts.get('Non-OSA', 0)}\")\n",
    "    \n",
    "    return analyzer\n",
    "\n",
    "# Usage:\n",
    "\n",
    "# analyzer = run_complete_multi_night_analysis(max_nights=3)\n",
    "\n",
    "# Or run all nights\n",
    "analyzer = run_complete_multi_night_analysis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206fec29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
