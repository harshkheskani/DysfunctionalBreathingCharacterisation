{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8282ff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import pytz\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.subplots as sp\n",
    "import pytz as tz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.signal import butter, filtfilt\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35e382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPECK_FILE = '../data/bishkek_csr/03_train_ready/respeck/05-04-2025_respeck.csv'\n",
    "PSG_FILE = '../data/bishkek_csr/03_train_ready/nasal_files/05-04-2025_nasal.csv'\n",
    "LABELS_FILE = '../data/bishkek_csr/03_train_ready/event_exports/05-04-2025_event_export.csv'\n",
    "OUTPUT_FILE = './08-05-2025_respeck_features.csv'\n",
    "\n",
    "# --- Load Data ---\n",
    "print(\"Loading data...\")\n",
    "\n",
    "respeck_df = pd.read_csv(RESPECK_FILE)\n",
    "respeck_df['timestamp'] = pd.to_datetime(respeck_df['alignedTimestamp'], unit='ms')\n",
    "tz = pytz.timezone('Asia/Bishkek')\n",
    "respeck_df['timestamp'] = respeck_df['timestamp'].dt.tz_localize('UTC').dt.tz_convert(tz)\n",
    "\n",
    "psg_df = pd.read_csv(PSG_FILE)\n",
    "psg_df['timestamp'] = pd.to_datetime(psg_df['UnixTimestamp'], unit='ms')\n",
    "tz = pytz.timezone('Asia/Bishkek')\n",
    "psg_df['timestamp'] = psg_df['timestamp'].dt.tz_localize('UTC').dt.tz_convert(tz)\n",
    "\n",
    "labels_df = pd.read_csv(LABELS_FILE)\n",
    "labels_df['timestamp'] = pd.to_datetime(labels_df['UnixTimestamp'], unit='ms')\n",
    "tz = pytz.timezone('Asia/Bishkek')\n",
    "labels_df['timestamp'] = labels_df['timestamp'].dt.tz_localize('UTC').dt.tz_convert(tz)\n",
    "\n",
    "# forward and back fill respeck data before extraction\n",
    "\n",
    "start_time_respeck = respeck_df['timestamp'].min()\n",
    "end_time_respeck = respeck_df['timestamp'].max()\n",
    "\n",
    "start_time_psg = psg_df['timestamp'].min()\n",
    "end_time_psg = psg_df['timestamp'].max()\n",
    "\n",
    "overlap_start = max(start_time_respeck, start_time_psg)\n",
    "overlap_end = min(end_time_respeck, end_time_psg)\n",
    "\n",
    "\n",
    "print(overlap_start)\n",
    "print(overlap_end)\n",
    "\n",
    "respeck_df = respeck_df[(respeck_df['timestamp'] >= overlap_start) & (respeck_df['timestamp'] <= overlap_end)]\n",
    "psg_df = psg_df[(psg_df['timestamp'] >= overlap_start) & (psg_df['timestamp'] <= overlap_end)]\n",
    "\n",
    "Dynamically calculate the sampling rate from the timestamps\n",
    "time_diffs_ms = respeck_df['alignedTimestamp'].diff().median()\n",
    "if pd.isna(time_diffs_ms) or time_diffs_ms == 0:\n",
    "\n",
    "    fs = 1000.0 / time_diffs_ms  # Sampling frequency in Hz\n",
    "    print(f\"    - Calculated sampling rate: {fs:.2f} Hz\")\n",
    "\n",
    "    # Define filter parameters\n",
    "    lowcut = 0.1   # Lower cutoff frequency in Hz\n",
    "    highcut = 1.5  # Upper cutoff frequency in Hz\n",
    "    order = 2      # Filter order (2 is a good choice to avoid distortion)\n",
    "\n",
    "    try:\n",
    "        # Design the Butterworth bandpass filter\n",
    "        nyquist = 0.5 * fs\n",
    "        low = lowcut / nyquist\n",
    "        high = highcut / nyquist\n",
    "        b, a = butter(order, [low, high], btype='band')\n",
    "        \n",
    "        respeck_df['original_breathingSignal'] = respeck_df['breathingSignal']\n",
    "\n",
    "    # 2. Apply the filter and OVERWRITE the 'breathingSignal' column with the clean data\n",
    "        respeck_df['breathingSignal'] = filtfilt(b, a, respeck_df['breathingSignal'])\n",
    "\n",
    "        # # Apply the filter and store it in a NEW column\n",
    "        # # We keep the original 'breathingSignal' for reference\n",
    "        # respeck_df['filteredBreathingSignal'] = filtfilt(b, a, respeck_df['breathingSignal'])\n",
    "    except ValueError as e:\n",
    "        print(f\"  - WARNING: Skipping session. Filter could not be applied. Error: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3a1985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jack's Util file\n",
    "\n",
    "def nans(dims):\n",
    "    a = np.empty(dims)\n",
    "    a[:] = np.nan\n",
    "    return a\n",
    "\n",
    "''' Find the RMS value of an input signal in array form. '''\n",
    "def rms(signal):\n",
    "    return np.sqrt(np.mean(signal**2))\n",
    "\n",
    "def rmsHamming(signal):\n",
    "    squares = signal**2\n",
    "    weights = np.hamming(len(signal))\n",
    "    weightedSum = 0.0\n",
    "    weightsSum = 0.0\n",
    "\n",
    "    for i in range(len(signal)):\n",
    "        weightedSum += squares[i] * weights[i]\n",
    "        weightsSum += weights[i]\n",
    "\n",
    "    return np.sqrt(weightedSum / weightsSum)\n",
    "\n",
    "''' Find islands of defined values in a signal that may contain NaNs. '''\n",
    "def findIslandLimits(signal, minIslandLength=0, minIslandGap=0):\n",
    "\n",
    "    islands = []\n",
    "\n",
    "    start = None\n",
    "    end = None\n",
    "    foundIsland = False\n",
    "\n",
    "    for i in range(len(signal)):\n",
    "        if not signal[i]:\n",
    "            if start == None:\n",
    "                start = i\n",
    "            else:\n",
    "                end = i + 1\n",
    "                if i == len(signal) - 1:\n",
    "                    foundIsland = True\n",
    "        else:\n",
    "            if start != None:\n",
    "                if end != None:\n",
    "                    foundIsland = True\n",
    "                else:\n",
    "                    start = None\n",
    "\n",
    "        if foundIsland:\n",
    "            if (minIslandGap > 0) and (len(islands) > 0):\n",
    "                prevIslandStart = islands[-1][0]\n",
    "                prevIslandEnd = islands[-1][1]\n",
    "                islandGap = start - prevIslandEnd - 1\n",
    "                if islandGap < minIslandGap:\n",
    "                    # merge the new island with the previous one\n",
    "                    islands[-1] = ((prevIslandStart, end))\n",
    "                else:\n",
    "                    islands.append((start, end))\n",
    "            else:    \n",
    "                islands.append((start, end))\n",
    "\n",
    "            start = None\n",
    "            end = None\n",
    "            foundIsland = False\n",
    "            \n",
    "    # now return only the islands that are long enough\n",
    "    longIslands = []\n",
    "    for island in islands:\n",
    "        if (island[1] - island[0]) >= minIslandLength:\n",
    "            longIslands.append(island)\n",
    "\n",
    "    return longIslands\n",
    "\n",
    "def calculateThresholdLevels(signal, rmsBackwardLength, rmsForwardLength, rmsMultiplier, symmetrical):\n",
    "    result = nans((len(signal), 2))\n",
    "    \n",
    "    if not symmetrical:\n",
    "        \n",
    "        #fill sum of squares buffers\n",
    "        posValues = []\n",
    "        negValues = []\n",
    "        windowLength = rmsBackwardLength + rmsForwardLength\n",
    "        if len(signal) < windowLength:\n",
    "            return result\n",
    "        \n",
    "        lastBananaIndex = np.nan\n",
    "            \n",
    "        for i in range(windowLength - 1):\n",
    "            if signal[i] >= 0:\n",
    "                posValues.append(signal[i])\n",
    "            elif signal[i] < 0:\n",
    "                negValues.append(signal[i])\n",
    "            else: # if nan\n",
    "                lastBananaIndex = i\n",
    "                \n",
    "        posArray = np.array(posValues)\n",
    "        negArray = np.array(negValues)\n",
    "        \n",
    "        sumOfSquaresPos = np.sum(posArray**2)\n",
    "        posCount = len(posArray)\n",
    "        sumOfSquaresNeg = np.sum(negArray**2)\n",
    "        negCount = len(negArray)\n",
    "        \n",
    "        for i in range(0, len(signal)):\n",
    "            if i < rmsBackwardLength or i >= len(signal) - rmsForwardLength:\n",
    "                posResult = np.nan\n",
    "                negResult = np.nan\n",
    "            else:\n",
    "                newValue = signal[i+rmsForwardLength-1]\n",
    "                if np.isnan(newValue):\n",
    "                    lastBananaIndex = i+rmsForwardLength-1\n",
    "                else:\n",
    "                    if newValue >= 0:\n",
    "                        sumOfSquaresPos += newValue**2\n",
    "                        posCount += 1\n",
    "                    elif newValue < 0:\n",
    "                        sumOfSquaresNeg += newValue**2\n",
    "                        negCount += 1\n",
    "                \n",
    "                if not np.isnan(lastBananaIndex) and i - lastBananaIndex <= rmsBackwardLength:\n",
    "                    posResult = np.nan\n",
    "                    negResult = np.nan\n",
    "                else:\n",
    "                    posResult = np.sqrt(sumOfSquaresPos / posCount) * rmsMultiplier\n",
    "                    negResult = -np.sqrt(sumOfSquaresNeg / negCount) * rmsMultiplier\n",
    "                \n",
    "                oldValue = signal[i-rmsBackwardLength]\n",
    "                \n",
    "                if oldValue >= 0:\n",
    "                    sumOfSquaresPos -= oldValue**2\n",
    "                    posCount -= 1\n",
    "                elif oldValue < 0:\n",
    "                    sumOfSquaresNeg -= oldValue**2\n",
    "                    negCount -=1\n",
    "            result[i,0] = posResult\n",
    "            result[i,1] = negResult\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    else:\n",
    "        #fill sum of squares buffers\n",
    "        allValues = []\n",
    "        windowLength = rmsBackwardLength + rmsForwardLength\n",
    "        if len(signal) < windowLength:\n",
    "            return result\n",
    "        \n",
    "        lastBananaIndex = np.nan\n",
    "        \n",
    "        for i in range(windowLength - 1):\n",
    "            if not np.isnan(signal[i]):\n",
    "                allValues.append(signal[i])\n",
    "            else:\n",
    "                lastBananaIndex = i\n",
    "        allArray = np.array(allValues)\n",
    "        \n",
    "        sumOfSquaresAll = np.sum(allArray**2)\n",
    "        allCount = len(allArray)\n",
    "        \n",
    "        for i in range(0, len(signal)):\n",
    "            if i < rmsBackwardLength or i >= len(signal) - rmsForwardLength:\n",
    "                allResult = np.nan\n",
    "            else:\n",
    "                newValue = signal[i+rmsForwardLength-1]\n",
    "                if np.isnan(newValue):\n",
    "                    lastBananaIndex = i+rmsForwardLength-1\n",
    "                else:\n",
    "                    sumOfSquaresAll += newValue**2\n",
    "                    allCount += 1\n",
    "                \n",
    "                if not np.isnan(lastBananaIndex) and i - lastBananaIndex <= rmsBackwardLength:\n",
    "                    allResult = np.nan\n",
    "                else:\n",
    "                    allResult = np.sqrt(sumOfSquaresAll / allCount) * rmsMultiplier\n",
    "                \n",
    "                oldValue = signal[i-rmsBackwardLength]\n",
    "                if not np.isnan(oldValue):\n",
    "                    sumOfSquaresAll -= oldValue**2\n",
    "                    allCount -= 1\n",
    "                    \n",
    "            result[i,0] = allResult\n",
    "            result[i,1] = -allResult\n",
    "        #figure()\n",
    "        #plot(signal)\n",
    "        #plot(result)\n",
    "        #show()\n",
    "        return result\n",
    "\n",
    "def calculateBreathTimes(signal, posThresholds, negThresholds, minThreshold, zeroCrossingBreathStart):\n",
    "    \n",
    "    def breathTimes(startIndex, endIndex):\n",
    "\n",
    "        def setInitialState(startValue, posThreshold, negThreshold):\n",
    "            if startValue < negThreshold:\n",
    "                state = LOW\n",
    "            elif startValue > posThreshold:\n",
    "                state = HIGH\n",
    "            else:\n",
    "                state = MID_UNKNOWN\n",
    "            return state\n",
    "    \n",
    "        state = setInitialState(signal[startIndex], posThresholds[startIndex], negThresholds[startIndex])\n",
    "        times = []\n",
    "    \n",
    "        for i in range(startIndex + 1, endIndex + 1):\n",
    "            posThreshold = posThresholds[i]\n",
    "            negThreshold = negThresholds[i]\n",
    "            if state == LOW and signal[i] > negThreshold:\n",
    "                state = MID_RISING\n",
    "            elif state == HIGH and signal[i] < posThreshold:\n",
    "                state = MID_FALLING\n",
    "            elif (state == MID_RISING or state == MID_UNKNOWN) and signal[i] > posThreshold:\n",
    "                state = HIGH\n",
    "            elif (state == MID_FALLING or state == MID_UNKNOWN) and signal[i] < negThreshold:\n",
    "                state = LOW\n",
    "                times.append(i)\n",
    "\n",
    "        if zeroCrossingBreathStart:\n",
    "            zeroCrossingBreathTimes = []\n",
    "            for t in times:\n",
    "                for i in range(t,-1,-1):\n",
    "                    if signal[i] >= 0:\n",
    "                        zeroCrossingBreathTimes.append(i)\n",
    "                        break\n",
    "            return zeroCrossingBreathTimes\n",
    "        else:\n",
    "            return times\n",
    "\n",
    "    LOW, MID_FALLING, MID_UNKNOWN, MID_RISING, HIGH = range(5)\n",
    "\n",
    "    \n",
    "    invalidated = np.ones(np.shape(signal), dtype=bool)\n",
    "    for i in range(len(invalidated)):\n",
    "        if posThresholds[i] > minThreshold or negThresholds[i] < -minThreshold:\n",
    "            invalidated[i] = False\n",
    "    \n",
    "\n",
    "    minIslandLength = 0\n",
    "    islandLimits = findIslandLimits(invalidated, minIslandLength)\n",
    "    \n",
    "    times = []\n",
    "    for (start, end) in islandLimits:\n",
    "        bt = breathTimes(start, end)\n",
    "        if len(bt) > 0:\n",
    "            times.append(bt)\n",
    "\n",
    "    return times\n",
    "\n",
    "\n",
    "# Code from Jack Taylor\n",
    "\n",
    "def countLocalMaximas(values):\n",
    "    count = 0\n",
    "    if len(values) < 3:\n",
    "        return 1\n",
    "    if len(values) > 1 and values[0] > values[1]:\n",
    "        count += 1\n",
    "    if len(values) > 1 and values[-1] > values[-2]:\n",
    "        count += 1\n",
    "    for i in range(1, len(values) - 1):\n",
    "        if values[i] > values[i - 1] and values[i] > values[i + 1]:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def countLocalMinimas(values):\n",
    "    count = 0\n",
    "    if len(values) < 3:\n",
    "        return 1\n",
    "    if len(values) > 1 and values[0] < values[1]:\n",
    "        count += 1\n",
    "    if len(values) > 1 and values[-1] < values[-2]:\n",
    "        count += 1\n",
    "    for i in range(1, len(values) - 1):\n",
    "        if values[i] < values[i - 1] and values[i] < values[i + 1]:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def generate_RRV(sliced):\n",
    "    sliced = sliced.dropna()\n",
    "    if sliced.size == 0:\n",
    "        return np.nan\n",
    "    breathingSignal = sliced.values\n",
    "    N = breathingSignal.shape[-1]\n",
    "    y = breathingSignal\n",
    "    yf = np.fft.fft(y)\n",
    "    yff = 2.0/N * np.abs(yf[:N//2])\n",
    "    temp_DCnotremov = yff\n",
    "    if len(temp_DCnotremov) == 0 or len(temp_DCnotremov) == 1: \n",
    "        return 0.0\n",
    "    else:\n",
    "        DC = np.amax(temp_DCnotremov)\n",
    "        maxi = np.argmax(temp_DCnotremov)\n",
    "        temp_DCremov = np.delete(temp_DCnotremov, maxi)\n",
    "        H1 = np.amax(temp_DCremov)\n",
    "        return 100-(H1/DC)*100\n",
    "\n",
    "def getBreaths(df):\n",
    "    minThreshold = 0.001\n",
    "    mult = 0.0125\n",
    "    \n",
    "    signal = list(df.breathingSignal)\n",
    "    \n",
    "    time_diff = df['timestamp'].diff()\n",
    "    time_diff.map(lambda x: x.total_seconds()).mean()\n",
    "    \n",
    "    window_size = int((20 / time_diff.dropna().apply(lambda x: x.total_seconds()).mean()) // 2)\n",
    "    threshs = calculateThresholdLevels(list(signal), window_size, window_size, mult, False)\n",
    "    posThresh = threshs[:, 0]\n",
    "    negThresh = threshs[:, 1]\n",
    "\n",
    "    times = calculateBreathTimes(list(signal), posThresh, negThresh, minThreshold, False)\n",
    "\n",
    "    total = set()\n",
    "    minBreathLength = float(\"inf\")\n",
    "    maxBreathLength = float(\"-inf\")\n",
    "    for i in range(0, len(times)):\n",
    "        vals = times[i]\n",
    "        for j in range(0, len(vals)-1):\n",
    "            start, end = vals[j], vals[j+1]\n",
    "            minBreathLength = min(minBreathLength, end-start+1)\n",
    "            maxBreathLength = max(maxBreathLength, end-start+1)\n",
    "            for k in range(start, end+1):\n",
    "                total.add(k)\n",
    "\n",
    "    f = list(df.breathingSignal.dropna())\n",
    "    a = f\"Uses Breath From {len(total)}/{len(f)} = {round((len(total)/len(f)) * 100, 2)}% Signal\"\n",
    "    b = f\"Max Breath Length: {maxBreathLength} points. Min Breath Length: {minBreathLength} points\"\n",
    "    print(a)\n",
    "    print(b)\n",
    "        \n",
    "    return times\n",
    "\n",
    "\n",
    "def mode(l):\n",
    "    if len(l) == 0:\n",
    "        return np.NaN, {}, []\n",
    "    \n",
    "    sortedRoundedArray = np.sort(np.around(l))\n",
    "    dict = {}\n",
    "    dist = np.zeros(sortedRoundedArray[-1] + 1)\n",
    "    maxCount = 0\n",
    "    for e in sortedRoundedArray:\n",
    "        dist[e] += 1\n",
    "        if e in dict:\n",
    "            newCount = dict[e] + 1\n",
    "            dict[e] = newCount\n",
    "        else:\n",
    "            newCount = 1\n",
    "            dict[e] = newCount\n",
    "            \n",
    "        if newCount > maxCount:\n",
    "                maxCount = newCount\n",
    "    \n",
    "    if maxCount > 0:\n",
    "        l = []\n",
    "        for e in dict:\n",
    "            if dict[e] == maxCount:\n",
    "                l.append(e)\n",
    "        sorted = np.sort(l)\n",
    "        return sorted[len(sorted) // 2], dict, dist\n",
    "                \n",
    "    else:\n",
    "        return np.NaN, dict, dist\n",
    "    \n",
    "\n",
    "def extractFeatures(df):\n",
    "    times = getBreaths(df)\n",
    "\n",
    "    areas = []\n",
    "    extremas = []\n",
    "    peakRespiratoryFlows = []\n",
    "    types = []\n",
    "    durations = []\n",
    "    activityLevels = []\n",
    "    activityTypes = []\n",
    "    starts = []\n",
    "    ends = []\n",
    "    \n",
    "    activityLevel = np.array(df.activityLevel)\n",
    "    activityType = np.array(df.activityType)\n",
    "    signal = np.array(df.breathingSignal)\n",
    "    timestamps = list(df.timestamp)\n",
    "\n",
    "    for i in range(0, len(times)):\n",
    "        if i % 25 == 0:\n",
    "            print(f\"{i}/{len(times)}... \", end=\" \")\n",
    "        vals = times[i]\n",
    "        \n",
    "        for j in range(0, len(vals)-1):\n",
    "            start, end = vals[j], vals[j+1]\n",
    "            flag = False\n",
    "            breath = signal[start:end+1]\n",
    "            breakPoint = start\n",
    "            for k, val in enumerate(breath):\n",
    "                if val >= 0.005: # arbitrary but to remove noise...\n",
    "                    breakPoint = start + k\n",
    "                    break\n",
    "\n",
    "            # compute inhalation\n",
    "            inhalation, inhalation_times = signal[start:breakPoint], timestamps[start:breakPoint]\n",
    "            exhalation, exhalation_times = signal[breakPoint:end+1], timestamps[breakPoint:end+1]\n",
    "                    \n",
    "            level = activityLevel[start:end+1].mean()\n",
    "            modeType = mode(activityType[start:end+1])[0]\n",
    "            \n",
    "            # compute inhalation\n",
    "            if len(inhalation) > 1:\n",
    "                peak = max(abs(np.array(inhalation)))\n",
    "                extrema = countLocalMaximas(inhalation)\n",
    "                dx = (inhalation_times[-1]-inhalation_times[0]).total_seconds() / len(inhalation)\n",
    "                area = abs(np.trapezoid(y=inhalation,dx=dx))\n",
    "                duration = (inhalation_times[-1]-inhalation_times[0]).total_seconds()\n",
    "                \n",
    "                areas.append(area)\n",
    "                extremas.append(extrema)\n",
    "                peakRespiratoryFlows.append(peak)\n",
    "                types.append(\"Inhalation\")\n",
    "                durations.append(duration)\n",
    "                activityLevels.append(level)\n",
    "                activityTypes.append(modeType)\n",
    "                starts.append(inhalation_times[0])\n",
    "                ends.append(inhalation_times[-1])\n",
    "\n",
    "            if len(exhalation) > 1:\n",
    "                peak = max(abs(np.array(exhalation)))\n",
    "                extrema = countLocalMinimas(exhalation)    \n",
    "                dx = (exhalation_times[-1]-exhalation_times[0]).total_seconds() / len(exhalation)\n",
    "                area = abs(np.trapezoid(y=exhalation,dx=dx))  \n",
    "                duration = (exhalation_times[-1]-exhalation_times[0]).total_seconds()\n",
    "                \n",
    "                areas.append(area)\n",
    "                extremas.append(extrema)\n",
    "                peakRespiratoryFlows.append(peak)\n",
    "                types.append(\"Exhalation\")\n",
    "                durations.append(duration)\n",
    "                activityLevels.append(level)\n",
    "                activityTypes.append(modeType)\n",
    "                starts.append(exhalation_times[0])\n",
    "                ends.append(exhalation_times[-1])\n",
    "\n",
    "    return pd.DataFrame(data={\"type\": types, \"area\": areas, \"peakRespiratoryFlow\": peakRespiratoryFlows, \"extremas\": extremas, \"duration\": durations, \"meanActivityLevel\": activityLevels, \"modeActivityType\": activityTypes, \"startTimestamp\": starts, \"endTimestamp\": ends})\n",
    "\n",
    "\n",
    "def getRegularity(df):\n",
    "    # get distance to 1st PC for area, PRF only --> makes rapid shallow in feature level\n",
    "    # make it temporal by adding distance to PC from all 3 as a seperaate PCA raansform\n",
    "    scaler = MinMaxScaler()\n",
    "    columns = ['area', 'peakRespiratoryFlow']\n",
    "    df_normalized = scaler.fit_transform(df[columns])\n",
    "    pca = PCA(n_components=1)  \n",
    "    pca.fit(df_normalized)\n",
    "    df_pca = pca.transform(df_normalized)\n",
    "    first_principal_component = pca.components_[0]\n",
    "    te = np.linalg.norm(df_normalized - np.outer(df_normalized.dot(first_principal_component), first_principal_component), axis=1)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    # Okay so we are looking at the resapmled breathing rate\n",
    "    columns = ['area', 'peakRespiratoryFlow', 'BR_mean']\n",
    "    df_normalized = scaler.fit_transform(df[columns])\n",
    "    pca = PCA(n_components=3)  \n",
    "    pca.fit(df_normalized)\n",
    "    df_pca = pca.transform(df_normalized)\n",
    "    \n",
    "    first_principal_component = pca.components_[0]\n",
    "    second_principal_component = pca.components_[1]\n",
    "    third_principal_component = pca.components_[2]\n",
    "    \n",
    "    distances_to_first_component = np.linalg.norm(df_normalized - np.outer(df_normalized.dot(first_principal_component), first_principal_component), axis=1)\n",
    "    distances_to_second_component = np.linalg.norm(df_normalized - np.outer(df_normalized.dot(second_principal_component), second_principal_component), axis=1)\n",
    "    distances_to_third_component = np.linalg.norm(df_normalized - np.outer(df_normalized.dot(third_principal_component), third_principal_component), axis=1)\n",
    "\n",
    "    # Linear combination of these distances\n",
    "    distances_difference = te + (distances_to_first_component - distances_to_second_component + distances_to_third_component)\n",
    "\n",
    "    distances_difference = (distances_difference - distances_difference.min()) / (distances_difference.max() - distances_difference.min())\n",
    "    \n",
    "    return 1 - distances_difference\n",
    "\n",
    "def combineDfs(respeck_df, original_respeck_df):\n",
    "    breath_averages = []\n",
    "    \n",
    "    original_respeck_df.set_index('timestamp', inplace=True)\n",
    "    original_respeck_df['BR_md'] = original_respeck_df[['breathingRate']].resample('30s').median().reindex(original_respeck_df.index, method='nearest')\n",
    "    original_respeck_df['BR_mean'] = original_respeck_df[['breathingRate']].resample('30s').mean().reindex(original_respeck_df.index, method='nearest')\n",
    "    original_respeck_df['BR_std'] = original_respeck_df[['breathingRate']].resample('30s').std().reindex(original_respeck_df.index, method='nearest')\n",
    "\n",
    "    original_respeck_df['AL_md'] = original_respeck_df[['activityLevel']].resample('30s').median().reindex(original_respeck_df.index, method='nearest')\n",
    "    original_respeck_df['AL_mean'] = original_respeck_df[['activityLevel']].resample('30s').mean().reindex(original_respeck_df.index, method='nearest')\n",
    "    original_respeck_df['AL_std'] = original_respeck_df[['activityLevel']].resample('30s').std().reindex(original_respeck_df.index, method='nearest')\n",
    "\n",
    "\n",
    "    RRV = original_respeck_df[[\"breathingSignal\"]].resample('30s').apply(generate_RRV)\n",
    "    RRV = RRV.replace(0, np.nan).ffill().bfill()\n",
    "    original_respeck_df['RRV'] = RRV.reindex(original_respeck_df.index, method='nearest')\n",
    "\n",
    "    # average of 3 Neighbours\n",
    "    RRV3MA = RRV.rolling(window=3, center = True).mean() * 0.65\n",
    "    original_respeck_df['RRV3MA'] = RRV3MA.reindex(original_respeck_df.index, method='nearest')\n",
    "    \n",
    "    original_respeck_df = original_respeck_df.reset_index()\n",
    "    \n",
    "    for index, row in respeck_df.iterrows():\n",
    "        start_timestamp_str = row['startTimestamp']\n",
    "        end_timestamp_str = row['endTimestamp']\n",
    "\n",
    "        start_timestamp = pd.to_datetime(start_timestamp_str)\n",
    "        end_timestamp = pd.to_datetime(end_timestamp_str)\n",
    "\n",
    "        \n",
    "        filtered_df = original_respeck_df[\n",
    "            (original_respeck_df['timestamp'] >= start_timestamp) &\n",
    "            (original_respeck_df['timestamp'] <= end_timestamp)\n",
    "        ]\n",
    "        \"\"\"\n",
    "        get sleeping features\n",
    "        \"\"\"\n",
    "        breath_averages.append({\n",
    "            'type': row['type'],\n",
    "            'startTimestamp': start_timestamp,\n",
    "            'endTimestamp': end_timestamp,\n",
    "            'area': row['area'],\n",
    "            'extremas': row['extremas'],\n",
    "            'meanActivityLevel': row['meanActivityLevel'],\n",
    "            'modeActivityType': row['modeActivityType'],\n",
    "            'peakRespiratoryFlow': row['peakRespiratoryFlow'],\n",
    "            'duration': row['duration'],\n",
    "            'BR_md': filtered_df.BR_md.mean(),\n",
    "            'BR_mean': filtered_df.BR_mean.mean(),\n",
    "            'BR_std': filtered_df.BR_std.mean(),\n",
    "            'AL_md': filtered_df.AL_md.mean(),\n",
    "            'AL_mean': filtered_df.AL_mean.mean(),\n",
    "            'AL_std': filtered_df.AL_std.mean(),\n",
    "            'RRV': filtered_df.RRV.mean(),\n",
    "            'RRV3MA': filtered_df.RRV3MA.mean(),\n",
    "        })\n",
    "    breath_averages_df = pd.DataFrame(breath_averages)\n",
    "    return breath_averages_df\n",
    "\n",
    "\n",
    "def calculate_breathing_rate_from_breaths(df, breath_times, window_minutes=1):\n",
    "    \"\"\"\n",
    "    Calculate breathing rate from detected breath times.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame with timestamp and breathingSignal columns\n",
    "    - breath_times: Output from getBreaths function\n",
    "    - window_minutes: Time window for rate calculation in minutes\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with timestamp and calculated breathing rate\n",
    "    \"\"\"\n",
    "    # Convert timestamps to datetime\n",
    "\n",
    "    df['datetime'] = df['timestamp']\n",
    "    \n",
    "    # Flatten all breath indices\n",
    "    all_breath_indices = []\n",
    "    for breath_group in breath_times:\n",
    "        all_breath_indices.extend(breath_group)\n",
    "    \n",
    "    # Sort breath indices\n",
    "    all_breath_indices.sort()\n",
    "    \n",
    "    # Create breathing rate time series\n",
    "    breathing_rates = []\n",
    "    timestamps = []\n",
    "    \n",
    "    # Calculate rate using sliding window\n",
    "    window_seconds = window_minutes * 60\n",
    "    \n",
    "    for i, breath_idx in enumerate(all_breath_indices):\n",
    "        if breath_idx >= len(df):\n",
    "            continue\n",
    "            \n",
    "        current_time = df.iloc[breath_idx]['datetime']\n",
    "        timestamps.append(df.iloc[breath_idx]['timestamp'])\n",
    "        \n",
    "        # Count breaths in the past window\n",
    "        breath_count = 0\n",
    "        for j in range(i, -1, -1):  # Look backwards\n",
    "            if all_breath_indices[j] >= len(df):\n",
    "                continue\n",
    "            breath_time = df.iloc[all_breath_indices[j]]['datetime']\n",
    "            time_diff = (current_time - breath_time).total_seconds()\n",
    "            \n",
    "            if time_diff <= window_seconds:\n",
    "                breath_count += 1\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        # Convert to breaths per minute\n",
    "        rate = (breath_count / window_seconds) * 60\n",
    "        breathing_rates.append(rate)\n",
    "    \n",
    "    # Create result DataFrame\n",
    "    result_df = pd.DataFrame({\n",
    "        'timestamp': timestamps,\n",
    "        'calculated_breathing_rate': breathing_rates\n",
    "    })\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "old_breaths = getBreaths(respeck_df)\n",
    "# Convert 'startTimestamp' to datetime\n",
    "# breath_features['startTimestamp'] = pd.to_datetime(breath_features['startTimestamp'])\n",
    "\n",
    "# # Count the number of breaths (inhalations + exhalations)\n",
    "# breath_features['breath_count'] = 1  # Each row corresponds to a breath\n",
    "\n",
    "# # Total number of breaths\n",
    "# total_breaths = breath_features['breath_count'].sum()\n",
    "\n",
    "# # Get the total duration of the DataFrame in minutes\n",
    "# start_time = breath_features['startTimestamp'].min()\n",
    "# end_time = breath_features['startTimestamp'].max()\n",
    "# total_duration_minutes = (end_time - start_time).total_seconds() / 60  # Convert to minutes\n",
    "\n",
    "# # Calculate average breaths per minute\n",
    "# if total_duration_minutes > 0:\n",
    "#     avg_breaths_per_minute = total_breaths / total_duration_minutes\n",
    "# else:\n",
    "#     avg_breaths_per_minute = 0\n",
    "\n",
    "# print(f'Total Breaths: {total_breaths}')\n",
    "# print(f'Total Duration (minutes): {total_duration_minutes:.2f}')\n",
    "# print(f'Average Breaths per Minute: {avg_breaths_per_minute:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2174cefd",
   "metadata": {},
   "source": [
    "## Accelerometer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdad159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- START OF FILE jack-breaths.py ---\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Jack's Util file\n",
    "\n",
    "def nans(dims):\n",
    "    a = np.empty(dims)\n",
    "    a[:] = np.nan\n",
    "    return a\n",
    "\n",
    "''' Find the RMS value of an input signal in array form. '''\n",
    "def rms(signal):\n",
    "    return np.sqrt(np.mean(signal**2))\n",
    "\n",
    "def rmsHamming(signal):\n",
    "    squares = signal**2\n",
    "    weights = np.hamming(len(signal))\n",
    "    weightedSum = 0.0\n",
    "    weightsSum = 0.0\n",
    "\n",
    "    for i in range(len(signal)):\n",
    "        weightedSum += squares[i] * weights[i]\n",
    "        weightsSum += weights[i]\n",
    "\n",
    "    return np.sqrt(weightedSum / weightsSum)\n",
    "\n",
    "''' Find islands of defined values in a signal that may contain NaNs. '''\n",
    "def findIslandLimits(signal, minIslandLength=0, minIslandGap=0):\n",
    "\n",
    "    islands = []\n",
    "\n",
    "    start = None\n",
    "    end = None\n",
    "    foundIsland = False\n",
    "\n",
    "    for i in range(len(signal)):\n",
    "        if not signal[i]:\n",
    "            if start == None:\n",
    "                start = i\n",
    "            else:\n",
    "                end = i + 1\n",
    "                if i == len(signal) - 1:\n",
    "                    foundIsland = True\n",
    "        else:\n",
    "            if start != None:\n",
    "                if end != None:\n",
    "                    foundIsland = True\n",
    "                else:\n",
    "                    start = None\n",
    "\n",
    "        if foundIsland:\n",
    "            if (minIslandGap > 0) and (len(islands) > 0):\n",
    "                prevIslandStart = islands[-1][0]\n",
    "                prevIslandEnd = islands[-1][1]\n",
    "                islandGap = start - prevIslandEnd - 1\n",
    "                if islandGap < minIslandGap:\n",
    "                    # merge the new island with the previous one\n",
    "                    islands[-1] = ((prevIslandStart, end))\n",
    "                else:\n",
    "                    islands.append((start, end))\n",
    "            else:\n",
    "                islands.append((start, end))\n",
    "\n",
    "            start = None\n",
    "            end = None\n",
    "            foundIsland = False\n",
    "            \n",
    "    # now return only the islands that are long enough\n",
    "    longIslands = []\n",
    "    for island in islands:\n",
    "        if (island[1] - island[0]) >= minIslandLength:\n",
    "            longIslands.append(island)\n",
    "\n",
    "    return longIslands\n",
    "\n",
    "def calculateThresholdLevels(signal, rmsBackwardLength, rmsForwardLength, rmsMultiplier, symmetrical):\n",
    "    result = nans((len(signal), 2))\n",
    "    \n",
    "    if not symmetrical:\n",
    "        \n",
    "        #fill sum of squares buffers\n",
    "        posValues = []\n",
    "        negValues = []\n",
    "        windowLength = rmsBackwardLength + rmsForwardLength\n",
    "        if len(signal) < windowLength:\n",
    "            return result\n",
    "        \n",
    "        lastBananaIndex = np.nan\n",
    "            \n",
    "        for i in range(windowLength - 1):\n",
    "            if signal[i] >= 0:\n",
    "                posValues.append(signal[i])\n",
    "            elif signal[i] < 0:\n",
    "                negValues.append(signal[i])\n",
    "            else: # if nan\n",
    "                lastBananaIndex = i\n",
    "                \n",
    "        posArray = np.array(posValues)\n",
    "        negArray = np.array(negValues)\n",
    "        \n",
    "        sumOfSquaresPos = np.sum(posArray**2)\n",
    "        posCount = len(posArray)\n",
    "        sumOfSquaresNeg = np.sum(negArray**2)\n",
    "        negCount = len(negArray)\n",
    "        \n",
    "        for i in range(0, len(signal)):\n",
    "            if i < rmsBackwardLength or i >= len(signal) - rmsForwardLength:\n",
    "                posResult = np.nan\n",
    "                negResult = np.nan\n",
    "            else:\n",
    "                newValue = signal[i+rmsForwardLength-1]\n",
    "                if np.isnan(newValue):\n",
    "                    lastBananaIndex = i+rmsForwardLength-1\n",
    "                else:\n",
    "                    if newValue >= 0:\n",
    "                        sumOfSquaresPos += newValue**2\n",
    "                        posCount += 1\n",
    "                    elif newValue < 0:\n",
    "                        sumOfSquaresNeg += newValue**2\n",
    "                        negCount += 1\n",
    "                \n",
    "                if not np.isnan(lastBananaIndex) and i - lastBananaIndex <= rmsBackwardLength:\n",
    "                    posResult = np.nan\n",
    "                    negResult = np.nan\n",
    "                else:\n",
    "                    posResult = np.sqrt(sumOfSquaresPos / posCount) * rmsMultiplier if posCount > 0 else np.nan\n",
    "                    negResult = -np.sqrt(sumOfSquaresNeg / negCount) * rmsMultiplier if negCount > 0 else np.nan\n",
    "\n",
    "                oldValue = signal[i-rmsBackwardLength]\n",
    "                \n",
    "                if oldValue >= 0:\n",
    "                    sumOfSquaresPos -= oldValue**2\n",
    "                    posCount -= 1\n",
    "                elif oldValue < 0:\n",
    "                    sumOfSquaresNeg -= oldValue**2\n",
    "                    negCount -=1\n",
    "            result[i,0] = posResult\n",
    "            result[i,1] = negResult\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    else:\n",
    "        #fill sum of squares buffers\n",
    "        allValues = []\n",
    "        windowLength = rmsBackwardLength + rmsForwardLength\n",
    "        if len(signal) < windowLength:\n",
    "            return result\n",
    "        \n",
    "        lastBananaIndex = np.nan\n",
    "        \n",
    "        for i in range(windowLength - 1):\n",
    "            if not np.isnan(signal[i]):\n",
    "                allValues.append(signal[i])\n",
    "            else:\n",
    "                lastBananaIndex = i\n",
    "        allArray = np.array(allValues)\n",
    "        \n",
    "        sumOfSquaresAll = np.sum(allArray**2)\n",
    "        allCount = len(allArray)\n",
    "        \n",
    "        for i in range(0, len(signal)):\n",
    "            if i < rmsBackwardLength or i >= len(signal) - rmsForwardLength:\n",
    "                allResult = np.nan\n",
    "            else:\n",
    "                newValue = signal[i+rmsForwardLength-1]\n",
    "                if np.isnan(newValue):\n",
    "                    lastBananaIndex = i+rmsForwardLength-1\n",
    "                else:\n",
    "                    sumOfSquaresAll += newValue**2\n",
    "                    allCount += 1\n",
    "                \n",
    "                if not np.isnan(lastBananaIndex) and i - lastBananaIndex <= rmsBackwardLength:\n",
    "                    allResult = np.nan\n",
    "                else:\n",
    "                    allResult = np.sqrt(sumOfSquaresAll / allCount) * rmsMultiplier if allCount > 0 else np.nan\n",
    "\n",
    "                oldValue = signal[i-rmsBackwardLength]\n",
    "                if not np.isnan(oldValue):\n",
    "                    sumOfSquaresAll -= oldValue**2\n",
    "                    allCount -= 1\n",
    "                    \n",
    "            result[i,0] = allResult\n",
    "            result[i,1] = -allResult\n",
    "        return result\n",
    "\n",
    "def calculateBreathTimes(signal, posThresholds, negThresholds, minThreshold, zeroCrossingBreathStart):\n",
    "    \n",
    "    def breathTimes(startIndex, endIndex):\n",
    "\n",
    "        def setInitialState(startValue, posThreshold, negThreshold):\n",
    "            if startValue < negThreshold:\n",
    "                state = LOW\n",
    "            elif startValue > posThreshold:\n",
    "                state = HIGH\n",
    "            else:\n",
    "                state = MID_UNKNOWN\n",
    "            return state\n",
    "    \n",
    "        state = setInitialState(signal[startIndex], posThresholds[startIndex], negThresholds[startIndex])\n",
    "        times = []\n",
    "    \n",
    "        for i in range(startIndex + 1, endIndex + 1):\n",
    "            posThreshold = posThresholds[i]\n",
    "            negThreshold = negThresholds[i]\n",
    "            if state == LOW and signal[i] > negThreshold:\n",
    "                state = MID_RISING\n",
    "            elif state == HIGH and signal[i] < posThreshold:\n",
    "                state = MID_FALLING\n",
    "            elif (state == MID_RISING or state == MID_UNKNOWN) and signal[i] > posThreshold:\n",
    "                state = HIGH\n",
    "            elif (state == MID_FALLING or state == MID_UNKNOWN) and signal[i] < negThreshold:\n",
    "                state = LOW\n",
    "                times.append(i)\n",
    "\n",
    "        if zeroCrossingBreathStart:\n",
    "            zeroCrossingBreathTimes = []\n",
    "            for t in times:\n",
    "                for i in range(t,-1,-1):\n",
    "                    if signal[i] >= 0:\n",
    "                        zeroCrossingBreathTimes.append(i)\n",
    "                        break\n",
    "            return zeroCrossingBreathTimes\n",
    "        else:\n",
    "            return times\n",
    "\n",
    "    LOW, MID_FALLING, MID_UNKNOWN, MID_RISING, HIGH = range(5)\n",
    "\n",
    "    invalidated = np.ones(np.shape(signal), dtype=bool)\n",
    "    for i in range(len(invalidated)):\n",
    "        if posThresholds[i] > minThreshold or negThresholds[i] < -minThreshold:\n",
    "            invalidated[i] = False\n",
    "    \n",
    "    minIslandLength = 0\n",
    "    islandLimits = findIslandLimits(invalidated, minIslandLength)\n",
    "    \n",
    "    times = []\n",
    "    for (start, end) in islandLimits:\n",
    "        bt = breathTimes(start, end - 1) # Corrected end index\n",
    "        if len(bt) > 0:\n",
    "            times.append(bt)\n",
    "\n",
    "    return times\n",
    "\n",
    "\n",
    "# Code from Jack Taylor\n",
    "\n",
    "def countLocalMaximas(values):\n",
    "    count = 0\n",
    "    if len(values) < 3:\n",
    "        return 1\n",
    "    if len(values) > 1 and values[0] > values[1]:\n",
    "        count += 1\n",
    "    if len(values) > 1 and values[-1] > values[-2]:\n",
    "        count += 1\n",
    "    for i in range(1, len(values) - 1):\n",
    "        if values[i] > values[i - 1] and values[i] > values[i + 1]:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def countLocalMinimas(values):\n",
    "    count = 0\n",
    "    if len(values) < 3:\n",
    "        return 1\n",
    "    if len(values) > 1 and values[0] < values[1]:\n",
    "        count += 1\n",
    "    if len(values) > 1 and values[-1] < values[-2]:\n",
    "        count += 1\n",
    "    for i in range(1, len(values) - 1):\n",
    "        if values[i] < values[i - 1] and values[i] < values[i + 1]:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def generate_RRV(sliced):\n",
    "    sliced = sliced.dropna()\n",
    "    if sliced.size == 0:\n",
    "        return np.nan\n",
    "    breathingSignal = sliced.values\n",
    "    N = breathingSignal.shape[-1]\n",
    "    y = breathingSignal\n",
    "    yf = np.fft.fft(y)\n",
    "    yff = 2.0/N * np.abs(yf[:N//2])\n",
    "    temp_DCnotremov = yff\n",
    "    if len(temp_DCnotremov) == 0 or len(temp_DCnotremov) == 1:\n",
    "        return 0.0\n",
    "    else:\n",
    "        DC = np.amax(temp_DCnotremov)\n",
    "        maxi = np.argmax(temp_DCnotremov)\n",
    "        temp_DCremov = np.delete(temp_DCnotremov, maxi)\n",
    "        H1 = np.amax(temp_DCremov)\n",
    "        return 100-(H1/DC)*100\n",
    "\n",
    "def getBreathsConservative(df, return_dataframe=True):\n",
    "    \"\"\"\n",
    "    This function wraps the original breath detection logic and formats the output\n",
    "    to be compatible with the `compare_breathing_rates_over_time_corrected` testing script.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe with 'timestamp' and 'breathingSignal'.\n",
    "        return_dataframe (bool): If True, returns (DataFrame, stats). Otherwise,\n",
    "                                 returns the raw 'times' list and stats.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - breath_df (pd.DataFrame): A DataFrame with 'timestamp' and 'type' for each detected breath event.\n",
    "            - stats (dict): A dictionary with statistics ('inhalations', 'exhalations', 'breaths_per_minute').\n",
    "    \"\"\"\n",
    "    # Use the core `getBreaths` logic but without the verbose printing\n",
    "    times = getBreaths(df)\n",
    "\n",
    "    signal_array = np.array(df.breathingSignal)\n",
    "    timestamps = list(df.timestamp)\n",
    "    breath_events = []\n",
    "    inhalation_count = 0\n",
    "    exhalation_count = 0\n",
    "\n",
    "    for island in times:\n",
    "        for j in range(len(island) - 1):\n",
    "            start_idx = island[j]\n",
    "            end_idx = island[j+1]\n",
    "\n",
    "            if start_idx >= end_idx or end_idx >= len(signal_array):\n",
    "                continue\n",
    "\n",
    "            breath_segment = signal_array[start_idx:end_idx+1]\n",
    "            peak_idx_relative = np.argmax(breath_segment)\n",
    "            peak_idx = start_idx + peak_idx_relative\n",
    "\n",
    "            if peak_idx > start_idx:\n",
    "                inhalation_start_time = timestamps[start_idx]\n",
    "                breath_events.append({'timestamp': inhalation_start_time, 'type': 'Inhalation'})\n",
    "                inhalation_count += 1\n",
    "\n",
    "            if end_idx > peak_idx:\n",
    "                exhalation_start_time = timestamps[peak_idx]\n",
    "                breath_events.append({'timestamp': exhalation_start_time, 'type': 'Exhalation'})\n",
    "                exhalation_count += 1\n",
    "\n",
    "    if not breath_events:\n",
    "        breath_df = pd.DataFrame(columns=['timestamp', 'type'])\n",
    "    else:\n",
    "        breath_df = pd.DataFrame(breath_events)\n",
    "        breath_df['timestamp'] = pd.to_datetime(breath_df['timestamp'])\n",
    "\n",
    "        # ==================== FIX IS HERE ====================\n",
    "        # The test script expects to localize from UTC. To ensure this works,\n",
    "        # we strip any existing timezone info, returning a \"naive\" datetime.\n",
    "        # The test script will then correctly localize this naive time to UTC.\n",
    "        if breath_df['timestamp'].dt.tz is not None:\n",
    "            breath_df['timestamp'] = breath_df['timestamp'].dt.tz_convert('UTC').dt.tz_localize(None)\n",
    "        # =====================================================\n",
    "\n",
    "    total_breaths = min(inhalation_count, exhalation_count)\n",
    "    \n",
    "    if not df.empty and not df['timestamp'].empty:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        duration_seconds = (df['timestamp'].max() - df['timestamp'].min()).total_seconds()\n",
    "        duration_minutes = duration_seconds / 60 if duration_seconds > 0 else 0\n",
    "    else:\n",
    "        duration_minutes = 0\n",
    "\n",
    "    breaths_per_minute = total_breaths / duration_minutes if duration_minutes > 0 else 0\n",
    "\n",
    "    stats = {\n",
    "        'inhalations': inhalation_count,\n",
    "        'exhalations': exhalation_count,\n",
    "        'breaths_per_minute': breaths_per_minute\n",
    "    }\n",
    "\n",
    "    if return_dataframe:\n",
    "        return breath_df, stats\n",
    "    else:\n",
    "        return times, stats\n",
    "# ==============================================================================\n",
    "# END OF NEW FUNCTION\n",
    "# ==============================================================================\n",
    "\n",
    "def mode(l):\n",
    "    if len(l) == 0:\n",
    "        return np.NaN, {}, []\n",
    "    \n",
    "    # This function expects integer inputs, handle potential floats\n",
    "    l = [int(x) for x in np.nan_to_num(l)]\n",
    "    if not l: return np.NaN, {}, []\n",
    "\n",
    "    sortedRoundedArray = np.sort(l)\n",
    "    dict = {}\n",
    "    \n",
    "    # Handle potentially large integer values gracefully\n",
    "    dist = {} # Use dict instead of pre-allocating large array\n",
    "    maxCount = 0\n",
    "    for e in sortedRoundedArray:\n",
    "        dist[e] = dist.get(e, 0) + 1\n",
    "        dict[e] = dict.get(e, 0) + 1\n",
    "        newCount = dict[e]\n",
    "\n",
    "        if newCount > maxCount:\n",
    "                maxCount = newCount\n",
    "    \n",
    "    if maxCount > 0:\n",
    "        l_modes = []\n",
    "        for e in dict:\n",
    "            if dict[e] == maxCount:\n",
    "                l_modes.append(e)\n",
    "        sorted_modes = np.sort(l_modes)\n",
    "        # Return the median of the modes\n",
    "        return sorted_modes[len(sorted_modes) // 2], dict, dist\n",
    "                \n",
    "    else:\n",
    "        return np.NaN, dict, dist\n",
    "\n",
    "def extractFeatures(df):\n",
    "    times = getBreaths(df)\n",
    "\n",
    "    areas = []\n",
    "    extremas = []\n",
    "    peakRespiratoryFlows = []\n",
    "    types = []\n",
    "    durations = []\n",
    "    activityLevels = []\n",
    "    activityTypes = []\n",
    "    starts = []\n",
    "    ends = []\n",
    "    \n",
    "    activityLevel = np.array(df.activityLevel)\n",
    "    activityType = np.array(df.activityType)\n",
    "    signal = np.array(df.breathingSignal)\n",
    "    timestamps = list(df.timestamp)\n",
    "\n",
    "    for i in range(0, len(times)):\n",
    "        if i % 25 == 0 and len(times) > 0:\n",
    "            print(f\"Processing island {i}/{len(times)}... \", end=\" \")\n",
    "        vals = times[i]\n",
    "        \n",
    "        for j in range(0, len(vals)-1):\n",
    "            start, end = vals[j], vals[j+1]\n",
    "            flag = False\n",
    "            breath = signal[start:end+1]\n",
    "            breakPoint = start\n",
    "            for k, val in enumerate(breath):\n",
    "                if val >= 0.005: # arbitrary but to remove noise...\n",
    "                    breakPoint = start + k\n",
    "                    break\n",
    "\n",
    "            # compute inhalation\n",
    "            inhalation, inhalation_times = signal[start:breakPoint], timestamps[start:breakPoint]\n",
    "            exhalation, exhalation_times = signal[breakPoint:end+1], timestamps[breakPoint:end+1]\n",
    "                    \n",
    "            level = activityLevel[start:end+1].mean()\n",
    "            modeType = mode(activityType[start:end+1])[0]\n",
    "            \n",
    "            # compute inhalation\n",
    "            if len(inhalation) > 1:\n",
    "                peak = max(abs(np.array(inhalation)))\n",
    "                extrema = countLocalMaximas(inhalation)\n",
    "                dx = (inhalation_times[-1]-inhalation_times[0]).total_seconds() / len(inhalation)\n",
    "                area = abs(np.trapezoid(y=inhalation,dx=dx))\n",
    "                duration = (inhalation_times[-1]-inhalation_times[0]).total_seconds()\n",
    "                \n",
    "                areas.append(area)\n",
    "                extremas.append(extrema)\n",
    "                peakRespiratoryFlows.append(peak)\n",
    "                types.append(\"Inhalation\")\n",
    "                durations.append(duration)\n",
    "                activityLevels.append(level)\n",
    "                activityTypes.append(modeType)\n",
    "                starts.append(inhalation_times[0])\n",
    "                ends.append(inhalation_times[-1])\n",
    "\n",
    "            if len(exhalation) > 1:\n",
    "                peak = max(abs(np.array(exhalation)))\n",
    "                extrema = countLocalMinimas(exhalation)    \n",
    "                dx = (exhalation_times[-1]-exhalation_times[0]).total_seconds() / len(exhalation)\n",
    "                area = abs(np.trapezoid(y=exhalation,dx=dx))  \n",
    "                duration = (exhalation_times[-1]-exhalation_times[0]).total_seconds()\n",
    "                \n",
    "                areas.append(area)\n",
    "                extremas.append(extrema)\n",
    "                peakRespiratoryFlows.append(peak)\n",
    "                types.append(\"Exhalation\")\n",
    "                durations.append(duration)\n",
    "                activityLevels.append(level)\n",
    "                activityTypes.append(modeType)\n",
    "                starts.append(exhalation_times[0])\n",
    "                ends.append(exhalation_times[-1])\n",
    "\n",
    "    return pd.DataFrame(data={\"type\": types, \"area\": areas, \"peakRespiratoryFlow\": peakRespiratoryFlows, \"extremas\": extremas, \"duration\": durations, \"meanActivityLevel\": activityLevels, \"modeActivityType\": activityTypes, \"startTimestamp\": starts, \"endTimestamp\": ends})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7072ba",
   "metadata": {},
   "source": [
    "## OLD vs NEW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fed4bcd",
   "metadata": {},
   "source": [
    "## PSG breaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd46fa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPECK_FILE = '../data/bishkek_csr/03_train_ready/respeck/11-05-2025_respeck.csv'\n",
    "PSG_FILE = '../data/bishkek_csr/03_train_ready/nasal_files/11-05-2025_nasal.csv'\n",
    "LABELS_FILE = '../data/bishkek_csr/03_train_ready/event_exports/11-05-2025_event_export.csv'\n",
    "OUTPUT_FILE = './08-05-2025_respeck_features.csv'\n",
    "\n",
    "# --- Load Data ---\n",
    "print(\"Loading data...\")\n",
    "\n",
    "respeck_df = pd.read_csv(RESPECK_FILE)\n",
    "respeck_df['timestamp'] = pd.to_datetime(respeck_df['alignedTimestamp'], unit='ms')\n",
    "tz = pytz.timezone('Asia/Bishkek')\n",
    "respeck_df['timestamp'] = respeck_df['timestamp'].dt.tz_localize('UTC').dt.tz_convert(tz)\n",
    "\n",
    "psg_df = pd.read_csv(PSG_FILE)\n",
    "psg_df['timestamp'] = pd.to_datetime(psg_df['UnixTimestamp'], unit='ms')\n",
    "tz = pytz.timezone('Asia/Bishkek')\n",
    "psg_df['timestamp'] = psg_df['timestamp'].dt.tz_localize('UTC').dt.tz_convert(tz)\n",
    "\n",
    "labels_df = pd.read_csv(LABELS_FILE)\n",
    "labels_df['timestamp'] = pd.to_datetime(labels_df['UnixTimestamp'], unit='ms')\n",
    "tz = pytz.timezone('Asia/Bishkek')\n",
    "labels_df['timestamp'] = labels_df['timestamp'].dt.tz_localize('UTC').dt.tz_convert(tz)\n",
    "\n",
    "# forward and back fill respeck data before extraction\n",
    "\n",
    "start_time_respeck = respeck_df['timestamp'].min()\n",
    "end_time_respeck = respeck_df['timestamp'].max()\n",
    "\n",
    "start_time_psg = psg_df['timestamp'].min()\n",
    "end_time_psg = psg_df['timestamp'].max()\n",
    "\n",
    "overlap_start = max(start_time_respeck, start_time_psg)\n",
    "overlap_end = min(end_time_respeck, end_time_psg)\n",
    "\n",
    "\n",
    "print(overlap_start)\n",
    "print(overlap_end)\n",
    "\n",
    "respeck_df = respeck_df[(respeck_df['timestamp'] >= overlap_start) & (respeck_df['timestamp'] <= overlap_end)]\n",
    "psg_df = psg_df[(psg_df['timestamp'] >= overlap_start) & (psg_df['timestamp'] <= overlap_end)]\n",
    "\n",
    "# Dynamically calculate the sampling rate from the timestamps\n",
    "time_diffs_ms = respeck_df['alignedTimestamp'].diff().median()\n",
    "if pd.isna(time_diffs_ms) or time_diffs_ms == 0:\n",
    "\n",
    "    fs = 1000.0 / time_diffs_ms  # Sampling frequency in Hz\n",
    "    print(f\"    - Calculated sampling rate: {fs:.2f} Hz\")\n",
    "\n",
    "    # Define filter parameters\n",
    "    lowcut = 0.1   # Lower cutoff frequency in Hz\n",
    "    highcut = 1.5  # Upper cutoff frequency in Hz\n",
    "    order = 2      # Filter order (2 is a good choice to avoid distortion)\n",
    "\n",
    "    try:\n",
    "        # Design the Butterworth bandpass filter\n",
    "        nyquist = 0.5 * fs\n",
    "        low = lowcut / nyquist\n",
    "        high = highcut / nyquist\n",
    "        b, a = butter(order, [low, high], btype='band')\n",
    "        \n",
    "        respeck_df['original_breathingSignal'] = respeck_df['breathingSignal']\n",
    "\n",
    "    # 2. Apply the filter and OVERWRITE the 'breathingSignal' column with the clean data\n",
    "        respeck_df['breathingSignal'] = filtfilt(b, a, respeck_df['breathingSignal'])\n",
    "\n",
    "        # # Apply the filter and store it in a NEW column\n",
    "        # # We keep the original 'breathingSignal' for reference\n",
    "        # respeck_df['filteredBreathingSignal'] = filtfilt(b, a, respeck_df['breathingSignal'])\n",
    "    except ValueError as e:\n",
    "        print(f\"  - WARNING: Skipping session. Filter could not be applied. Error: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a18e7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import find_peaks, detrend, butter, filtfilt\n",
    "from skimage.filters import threshold_otsu\n",
    "\n",
    "\n",
    "def calculate_robust_min_distance(detrended_signal, sampling_rate, height_threshold):\n",
    "    \"\"\"\n",
    "    More robust dynamic distance calculation that prevents spikes\n",
    "    \"\"\"\n",
    "\n",
    "    rough_height = height_threshold * 0.8  # More restrictive than 0.5\n",
    "    rough_distance = int(0.8 * sampling_rate)  # More restrictive than 0.4\n",
    "    \n",
    "    rough_peaks, _ = find_peaks(np.abs(detrended_signal), \n",
    "                               height=rough_height, \n",
    "                               distance=rough_distance)\n",
    "    \n",
    "    if len(rough_peaks) >= 3:  # Need at least 3 peaks for reliable intervals\n",
    "        intervals = np.diff(rough_peaks)\n",
    "        \n",
    "        # 2. Use more robust statistics - filter outliers first\n",
    "        q75, q25 = np.percentile(intervals, [75, 25])\n",
    "        iqr = q75 - q25\n",
    "        \n",
    "        # Remove outliers (intervals too short or too long)\n",
    "        valid_intervals = intervals[\n",
    "            (intervals >= q25 - 1.5 * iqr) & \n",
    "            (intervals <= q75 + 1.5 * iqr)\n",
    "        ]\n",
    "        \n",
    "        if len(valid_intervals) >= 2:\n",
    "            median_interval = np.median(valid_intervals)\n",
    "            \n",
    "            # 3. More conservative multiplier and stricter bounds\n",
    "            proposed_min_distance = median_interval * 0.8  # More conservative than 0.7\n",
    "            \n",
    "            # 4. Stricter bounds - never allow very short distances\n",
    "            min_distance = int(np.clip(proposed_min_distance,\n",
    "                                     a_min=int(sampling_rate * 0.8),    # 0.8s instead of 0.5s\n",
    "                                     a_max=int(sampling_rate * 4.0)))\n",
    "            \n",
    "            # 5. Sanity check - if calculated distance suggests >40 BPM, cap it\n",
    "            max_reasonable_rate = 40  # breaths per minute\n",
    "            min_reasonable_distance = int(60 * sampling_rate / max_reasonable_rate)\n",
    "            min_distance = max(min_distance, min_reasonable_distance)\n",
    "            \n",
    "            return min_distance\n",
    "    \n",
    "    # Fallback for unclear signals\n",
    "    return int(sampling_rate * 1.2)  # Conservative 1.2s instead of 1.5s\n",
    "\n",
    "def _consolidate_event_group(event_group):\n",
    "    \"\"\"\n",
    "    Helper function to consolidate a group of consecutive events of the same type.\n",
    "    \"\"\"\n",
    "    if len(event_group) == 1:\n",
    "        return event_group[0]\n",
    "    \n",
    "    event_type = event_group[0]['type']\n",
    "    start_timestamp = event_group[0]['timestamp']\n",
    "    end_timestamp = event_group[-1]['timestamp']\n",
    "    \n",
    "    duration_delta = end_timestamp - start_timestamp\n",
    "    if hasattr(duration_delta, 'total_seconds'):\n",
    "        duration = duration_delta.total_seconds()\n",
    "    else:\n",
    "        duration = duration_delta / np.timedelta64(1, 's')\n",
    "    \n",
    "    amplitudes = [event['amplitude'] for event in event_group]\n",
    "    raw_amplitudes = [event['raw_amplitude'] for event in event_group]\n",
    "    \n",
    "    max_amplitude_idx = np.argmax([abs(amp) for amp in amplitudes])\n",
    "    consolidated_amplitude = amplitudes[max_amplitude_idx]\n",
    "    consolidated_raw_amplitude = raw_amplitudes[max_amplitude_idx]\n",
    "    consolidated_index = event_group[max_amplitude_idx]['index']\n",
    "    \n",
    "    consolidated_event = {\n",
    "        'type': event_type, 'index': consolidated_index, 'timestamp': start_timestamp,\n",
    "        'end_timestamp': end_timestamp, 'duration_seconds': duration, 'amplitude': consolidated_amplitude,\n",
    "        'raw_amplitude': consolidated_raw_amplitude, 'event_type': event_group[0]['event_type'],\n",
    "        'orientation_type': event_group[0]['orientation_type'], 'gravity_influence': event_group[0]['gravity_influence'],\n",
    "        'events_merged': len(event_group), 'is_consolidated': True\n",
    "    }\n",
    "    return consolidated_event\n",
    "\n",
    "\n",
    "def _calibrate_orientation_thresholds_with_accelerometer(signal, sampling_rate, accel_x, accel_y, accel_z, fallback_low=0.15, fallback_high=0.5):\n",
    "    try:\n",
    "        if len(accel_x) > 0:\n",
    "            avg_z = np.mean(accel_z)\n",
    "            if abs(avg_z) > 0.7:\n",
    "                gravity_influence = \"high\"\n",
    "                preprocessing_needed = \"high_pass_filter\"\n",
    "            else:\n",
    "                gravity_influence = \"medium\"\n",
    "                preprocessing_needed = \"detrend_only\"\n",
    "        else:\n",
    "            gravity_influence = \"medium\"\n",
    "            preprocessing_needed = \"detrend_only\"\n",
    "        \n",
    "        return (0.1, 0.5), {'orientation_type': 'unknown', 'gravity_influence': gravity_influence, 'preprocessing_needed': preprocessing_needed}\n",
    "    except:\n",
    "         return (0.1, 0.5), {'orientation_type': 'unknown', 'gravity_influence': 'medium', 'preprocessing_needed': 'detrend_only'}\n",
    "\n",
    "def _detect_high_breathing_rate_periods(signal, sampling_rate, window_minutes=2):\n",
    "    window_samples = int(window_minutes * 60 * sampling_rate)\n",
    "    high_rate_mask = np.zeros(len(signal), dtype=bool)\n",
    "    step_size = window_samples // 4\n",
    "    for i in range(0, len(signal) - window_samples, step_size):\n",
    "        window_signal = signal[i:i + window_samples]\n",
    "        detrended = detrend(window_signal, type='constant')\n",
    "        rough_peaks, _ = find_peaks(np.abs(detrended), distance=int(0.4 * sampling_rate))\n",
    "        estimated_rate = len(rough_peaks) * (60 / window_minutes)\n",
    "        if estimated_rate > 20:\n",
    "            high_rate_mask[i:i + window_samples] = True\n",
    "    return high_rate_mask\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# THE MAIN FUNCTION: ORIGINAL LOGIC + DYNAMIC DISTANCE FIX\n",
    "# =============================================================================\n",
    "def adaptive_breath_detection_original_fixed(df, adaptation_window_minutes=10, \n",
    "                                           sensitivity='medium', method='peaks',\n",
    "                                           pad_duration_minutes=20):\n",
    "    \"\"\"\n",
    "    Your original, successful function with a single, targeted fix\n",
    "    to make the peak detection distance dynamically adaptive.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\" ORIGINAL ALGORITHM - WITH DYNAMIC DISTANCE FIX\")\n",
    "    print(\"=\" * 75)\n",
    "    \n",
    "    # --- 1. Input Validation and Data Preparation (Your Original Code) ---\n",
    "    required_columns = ['breathingSignal', 'timestamp']\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        raise ValueError(f\"DataFrame must contain columns: {required_columns}\")\n",
    "    \n",
    "    signal_series = df['breathingSignal'].copy().replace([np.inf, -np.inf], np.nan)\n",
    "    valid_signal = signal_series.dropna()\n",
    "    if len(valid_signal) < 200:\n",
    "        raise ValueError(f\"Insufficient valid samples: {len(valid_signal)}\")\n",
    "    \n",
    "    original_signal = valid_signal.values\n",
    "    valid_indices = valid_signal.index\n",
    "    original_timestamps = df.loc[valid_indices, 'timestamp'].values\n",
    "    original_accel_x = df.loc[valid_indices, 'x'].values\n",
    "    original_accel_y = df.loc[valid_indices, 'y'].values\n",
    "    original_accel_z = df.loc[valid_indices, 'z'].values\n",
    "    activity_level = df.loc[valid_indices, 'activityLevel'].values if 'activityLevel' in df.columns else None\n",
    "    \n",
    "    time_diffs = pd.Series(pd.to_datetime(original_timestamps)).diff().dt.total_seconds().dropna()\n",
    "    avg_sample_period = time_diffs.median()\n",
    "    if pd.isna(avg_sample_period) or avg_sample_period <= 0:\n",
    "        avg_sample_period = 0.02\n",
    "    sampling_rate = 1 / avg_sample_period\n",
    "    \n",
    "    pad_samples = int(pad_duration_minutes * 60 * sampling_rate)\n",
    "    signal_padded = np.pad(original_signal, pad_samples, mode='reflect')\n",
    "    accel_x_padded = np.pad(original_accel_x, pad_samples, mode='reflect')\n",
    "    accel_y_padded = np.pad(original_accel_y, pad_samples, mode='reflect')\n",
    "    accel_z_padded = np.pad(original_accel_z, pad_samples, mode='reflect')\n",
    "    activity_padded = np.pad(activity_level, pad_samples, mode='edge') if activity_level is not None else None\n",
    "\n",
    "    high_rate_mask = _detect_high_breathing_rate_periods(signal_padded, sampling_rate)\n",
    "    \n",
    "    # --- 2. Your Original Processing Loop ---\n",
    "    adaptation_window_samples = int(adaptation_window_minutes * 60 * sampling_rate)\n",
    "    \n",
    "    sensitivity_params = {\n",
    "        'low': {'base_height': 0.6, 'base_prominence': 0.5},\n",
    "        'medium': {'base_height': 0.5, 'base_prominence': 0.4},\n",
    "        'high': {'base_height': 0.4, 'base_prominence': 0.3}\n",
    "    }\n",
    "    params = sensitivity_params.get(sensitivity, sensitivity_params['medium'])\n",
    "    all_breath_events = []\n",
    "    step_size = adaptation_window_samples // 4\n",
    "    window_start = 0\n",
    "    \n",
    "    while window_start + adaptation_window_samples <= len(signal_padded):\n",
    "        window_end = window_start + adaptation_window_samples\n",
    "        \n",
    "        window_signal = signal_padded[window_start:window_end]\n",
    "        window_accel_x = accel_x_padded[window_start:window_end]\n",
    "        window_accel_y = accel_y_padded[window_start:window_end]\n",
    "        window_accel_z = accel_z_padded[window_start:window_end]\n",
    "        \n",
    "        is_high_rate_window = np.mean(high_rate_mask[window_start:window_end]) > 0.15        \n",
    "        try:\n",
    "            (_, _), window_orientation_info = _calibrate_orientation_thresholds_with_accelerometer(\n",
    "                window_signal, sampling_rate, window_accel_x, window_accel_y, window_accel_z\n",
    "            )\n",
    "        except:\n",
    "            window_orientation_info = {'orientation_type': 'unknown', 'gravity_influence': 'medium', 'preprocessing_needed': 'detrend_only'}\n",
    "        \n",
    "        # --- Your original sophisticated parameter calculation ---\n",
    "        detrended_signal = detrend(window_signal, type='constant')\n",
    "        signal_std = np.std(detrended_signal)\n",
    "        # (All your logic for MAD, gravity, activity, quality, etc. is preserved here)\n",
    "        gravity_influence = window_orientation_info['gravity_influence']\n",
    "        if gravity_influence == 'high': min_amplitude, base_height_factor = 0.015, 0.25\n",
    "        elif gravity_influence == 'medium': min_amplitude, base_height_factor = 0.008, 0.15\n",
    "        else: min_amplitude, base_height_factor = 0.005, 0.1\n",
    "            \n",
    "        height_threshold = max(signal_std * params['base_height'] * base_height_factor, min_amplitude * 0.3)\n",
    "        prominence_threshold = height_threshold * 0.8\n",
    "        \n",
    "        # =========================================================================\n",
    "        # THE ONLY CHANGE: DYNAMIC DISTANCE FIX\n",
    "        # This replaces your old rigid `if is_high_rate_window:` block for distance.\n",
    "        # =========================================================================\n",
    "        # 1. Perform a lenient first pass to estimate the local rhythm of the current window.\n",
    "        rough_peaks, _ = find_peaks(np.abs(detrended_signal),distance=int(0.4 * sampling_rate))\n",
    "        \n",
    "        # 2. If enough peaks were found, calculate the median interval between them.\n",
    "        if len(rough_peaks) > 2:\n",
    "            median_interval_samples = np.median(np.diff(rough_peaks))\n",
    "            # 3. Set the definitive min_distance to a fraction of that median interval.\n",
    "            # We clip it to prevent it from being too short (noise) or too long (missed breaths).\n",
    "            min_distance = int(np.clip(median_interval_samples * 0.7,      # 70% of median interval\n",
    "                                   a_min=int(sampling_rate * 0.8),     # Never shorter than 0.5s (120 BPM)\n",
    "                                   a_max=int(sampling_rate * 4.0)))    # Never longer than 4s (15 BPM)\n",
    "        else:\n",
    "            # 4. If not enough peaks were found, fall back to a safe, normal-rate default.\n",
    "            min_distance = int(sampling_rate * 1.5) \n",
    "        # =========================================================================\n",
    "\n",
    "        # --- Your original peak detection and event creation logic ---\n",
    "        processed_signal = detrended_signal # Use your filtered signal if applicable\n",
    "        if window_orientation_info.get('preprocessing_needed') == 'high_pass_filter':\n",
    "            try:\n",
    "                b, a = butter(2, 0.1 / (sampling_rate / 2), btype='high')\n",
    "                processed_signal = filtfilt(b, a, processed_signal)\n",
    "            except Exception: pass\n",
    "        \n",
    "        try:\n",
    "            peaks, _ = find_peaks(processed_signal, height=height_threshold, distance=min_distance, prominence=prominence_threshold)\n",
    "            troughs, _ = find_peaks(-processed_signal, height=height_threshold, distance=min_distance, prominence=prominence_threshold)\n",
    "            \n",
    "            # --- Your original event creation logic ---\n",
    "            for peak_idx in peaks:\n",
    "                global_padded_idx = window_start + peak_idx\n",
    "                original_signal_idx = global_padded_idx - pad_samples\n",
    "                if 0 <= original_signal_idx < len(original_timestamps):\n",
    "                    all_breath_events.append({\n",
    "                        'type': 'Inhalation', 'index': valid_indices[original_signal_idx],\n",
    "                        'timestamp': original_timestamps[original_signal_idx], 'amplitude': processed_signal[peak_idx], \n",
    "                        'raw_amplitude': original_signal[original_signal_idx], 'event_type': 'peak', \n",
    "                        'orientation_type': window_orientation_info['orientation_type'],\n",
    "                        'gravity_influence': gravity_influence, 'high_rate_period': is_high_rate_window\n",
    "                    })\n",
    "            \n",
    "            for trough_idx in troughs:\n",
    "                global_padded_idx = window_start + trough_idx\n",
    "                original_signal_idx = global_padded_idx - pad_samples\n",
    "                if 0 <= original_signal_idx < len(original_timestamps):\n",
    "                    all_breath_events.append({\n",
    "                        'type': 'Exhalation', 'index': valid_indices[original_signal_idx],\n",
    "                        'timestamp': original_timestamps[original_signal_idx], 'amplitude': processed_signal[trough_idx], \n",
    "                        'raw_amplitude': original_signal[original_signal_idx], 'event_type': 'trough', \n",
    "                        'orientation_type': window_orientation_info['orientation_type'],\n",
    "                        'gravity_influence': gravity_influence, 'high_rate_period': is_high_rate_window\n",
    "                    })\n",
    "                    \n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        window_start += step_size\n",
    "    \n",
    "    # --- 3. Finalization and Stats (Your Original Code) ---\n",
    "    if not all_breath_events:\n",
    "        return pd.DataFrame(), {'error': 'No events detected'}\n",
    "    \n",
    "    all_breath_events.sort(key=lambda x: x['timestamp'])\n",
    "    \n",
    "    filtered_events = []\n",
    "    last_timestamp = None\n",
    "    min_event_spacing = pd.Timedelta(seconds=0.1) # Slightly shorter to allow faster rates\n",
    "    for event in all_breath_events:\n",
    "        if last_timestamp is None or (pd.Timestamp(event['timestamp']) - last_timestamp) > min_event_spacing:\n",
    "            filtered_events.append(event)\n",
    "            last_timestamp = pd.Timestamp(event['timestamp'])\n",
    "    \n",
    "    consolidated_events = []\n",
    "    current_group = []\n",
    "    for event in filtered_events:\n",
    "        if not current_group or current_group[-1]['type'] == event['type']:\n",
    "            current_group.append(event)\n",
    "        else:\n",
    "            consolidated_events.append(_consolidate_event_group(current_group))\n",
    "            current_group = [event]\n",
    "    if current_group:\n",
    "        consolidated_events.append(_consolidate_event_group(current_group))\n",
    "    \n",
    "    breath_df = pd.DataFrame(consolidated_events).sort_values('timestamp').reset_index(drop=True)\n",
    "    \n",
    "    inhalations = len(breath_df[breath_df['type'] == 'Inhalation'])\n",
    "    exhalations = len(breath_df[breath_df['type'] == 'Exhalation'])\n",
    "    breathing_cycles = min(inhalations, exhalations)\n",
    "    duration_minutes = (pd.to_datetime(original_timestamps[-1]) - pd.to_datetime(original_timestamps[0])).total_seconds() / 60\n",
    "    breaths_per_minute = breathing_cycles / duration_minutes if duration_minutes > 0 else 0\n",
    "    \n",
    "    stats = {\n",
    "        'breaths_per_minute': breaths_per_minute, 'breathing_cycles': breathing_cycles,\n",
    "        'inhalations': inhalations, 'exhalations': exhalations, 'duration_minutes': duration_minutes\n",
    "    }\n",
    "    \n",
    "    return breath_df, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f00790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "import seaborn as sns\n",
    "\n",
    "# Import NeuroKit - make sure it's installed: pip install neurokit2\n",
    "try:\n",
    "    import neurokit2 as nk\n",
    "    NEUROKIT_AVAILABLE = True\n",
    "    print(\" NeuroKit2 successfully imported\")\n",
    "except ImportError:\n",
    "    NEUROKIT_AVAILABLE = False\n",
    "    print(\" NeuroKit2 not available. Install with: pip install neurokit2\")\n",
    "\n",
    "def cal_timeseries_instantaneous_rr(signal, sampling_rate=12, window=10):\n",
    "    \"\"\"\n",
    "    Calculate the instantaneous respiratory rate (breaths per minute) from a given respiratory signal.\n",
    "    Modified to accept dynamic sampling rate.\n",
    "\n",
    "    Parameters:\n",
    "    - signal (array-like): The respiratory signal data.\n",
    "    - sampling_rate (float): Sampling rate of the signal in Hz\n",
    "    - window (int): Window size for rate calculation\n",
    "\n",
    "    Returns:\n",
    "    - rsp_rate (array-like): The computed respiratory rate over time.\n",
    "    \"\"\"\n",
    "    if not NEUROKIT_AVAILABLE:\n",
    "        raise ImportError(\"NeuroKit2 is required but not installed\")\n",
    "    \n",
    "    try:\n",
    "        rsp_rate = nk.rsp_rate(signal, troughs=None, sampling_rate=sampling_rate, window=window,\n",
    "                               hop_size=1, method='trough', peak_method='khodadad2018',\n",
    "                               interpolation_method='monotone_cubic')\n",
    "        return rsp_rate\n",
    "    except Exception as e:\n",
    "        print(f\"NeuroKit rsp_rate failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def compare_algorithms_vs_respeck_builtin_with_neurokit(respeck_df, window_minutes=5):\n",
    "    \"\"\"\n",
    "    Compare three methods against RESpeck's built-in breathing rate measurements:\n",
    "    1. New adaptive algorithm\n",
    "    2. Old algorithm  \n",
    "    3. NeuroKit2 respiratory rate\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\" ALGORITHMS + NEUROKIT vs RESpeck BUILT-IN COMPARISON\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\" Using {window_minutes}-minute non-overlapping windows\")\n",
    "    \n",
    "    # --- 1. Check RESpeck built-in breathing rate data ---\n",
    "    if 'breathingRate' not in respeck_df.columns:\n",
    "        print(\" No 'breathingRate' column found in RESpeck data\")\n",
    "        return None\n",
    "    \n",
    "    respeck_sensor_data = respeck_df[['timestamp', 'breathingRate']].copy()\n",
    "    respeck_sensor_data['breathingRate'] = pd.to_numeric(respeck_sensor_data['breathingRate'], errors='coerce')\n",
    "    \n",
    "    # Only keep valid breathing rate measurements\n",
    "    valid_respeck = respeck_sensor_data.dropna(subset=['breathingRate'])\n",
    "    \n",
    "    if valid_respeck.empty:\n",
    "        print(\" No valid RESpeck breathing rate data available\")\n",
    "        return None\n",
    "    \n",
    "    print(f\" Found {len(valid_respeck):,} valid RESpeck measurements\")\n",
    "    print(f\"   Time range: {valid_respeck['timestamp'].min()} to {valid_respeck['timestamp'].max()}\")\n",
    "    print(f\"   Rate range: {valid_respeck['breathingRate'].min():.1f} - {valid_respeck['breathingRate'].max():.1f} breaths/min\")\n",
    "    \n",
    "    # --- 2. Calculate sampling rate ---\n",
    "    time_diffs = respeck_df['timestamp'].diff().dropna()\n",
    "    avg_sample_period = time_diffs.apply(lambda x: x.total_seconds()).median()\n",
    "    if pd.isna(avg_sample_period) or avg_sample_period <= 0:\n",
    "        avg_sample_period = 0.02\n",
    "    sampling_rate = 1 / avg_sample_period\n",
    "    print(f\" Detected sampling rate: {sampling_rate:.1f} Hz\")\n",
    "    \n",
    "    # --- 3. Determine time windows ---\n",
    "    data_start = respeck_df['timestamp'].min()\n",
    "    data_end = respeck_df['timestamp'].max()\n",
    "    \n",
    "    total_duration = (data_end - data_start).total_seconds() / 60  # minutes\n",
    "    num_windows = int(total_duration // window_minutes)\n",
    "    \n",
    "    print(f\"\\n Dataset Overview:\")\n",
    "    print(f\"   Data range: {data_start} to {data_end}\")\n",
    "    print(f\"   Total duration: {total_duration:.1f} minutes\")\n",
    "    print(f\"   Number of {window_minutes}-min windows: {num_windows}\")\n",
    "    \n",
    "    if num_windows < 1:\n",
    "        print(f\" Insufficient data for {window_minutes}-minute windows\")\n",
    "        return None\n",
    "    \n",
    "    # --- 4. Process each window ---\n",
    "    results = []\n",
    "    \n",
    "    for i in range(num_windows):\n",
    "        window_start = data_start + pd.Timedelta(minutes=i * window_minutes)\n",
    "        window_end = window_start + pd.Timedelta(minutes=window_minutes)\n",
    "        \n",
    "        print(f\"\\n--- Window {i+1}/{num_windows}: {window_start.strftime('%H:%M')} to {window_end.strftime('%H:%M')} ---\")\n",
    "        \n",
    "        # Extract data for this window\n",
    "        respeck_window = respeck_df[(respeck_df['timestamp'] >= window_start) & \n",
    "                                   (respeck_df['timestamp'] < window_end)].copy()\n",
    "        \n",
    "        if len(respeck_window) < 50:\n",
    "            print(f\"     Insufficient data in window {i+1}\")\n",
    "            continue\n",
    "        \n",
    "        window_result = {\n",
    "            'window_id': i + 1,\n",
    "            'start_time': window_start,\n",
    "            'end_time': window_end,\n",
    "            'respeck_samples': len(respeck_window)\n",
    "        }\n",
    "        \n",
    "        # Get RESpeck built-in breathing rate for this window\n",
    "        respeck_builtin_rates = respeck_window['breathingRate'].dropna()\n",
    "        if len(respeck_builtin_rates) > 0:\n",
    "            window_result['respeck_builtin_bpm'] = respeck_builtin_rates.mean()\n",
    "            window_result['respeck_builtin_std'] = respeck_builtin_rates.std()\n",
    "            window_result['respeck_builtin_count'] = len(respeck_builtin_rates)\n",
    "        else:\n",
    "            print(f\"     No valid RESpeck breathing rates in window {i+1}\")\n",
    "            continue\n",
    "        \n",
    "        # --- Method 1: New Algorithm ---\n",
    "        try:\n",
    "            print(\"    Running new algorithm...\")\n",
    "            breath_df_new, stats_new = adaptive_breath_detection_original_fixed(\n",
    "                respeck_window, \n",
    "                adaptation_window_minutes=0.5,        \n",
    "                pad_duration_minutes=1,             \n",
    "                # sensitivity='medium'                \n",
    "            )\n",
    "            \n",
    "            window_result['new_algo_events'] = len(breath_df_new)\n",
    "            window_result['new_algo_cycles'] = stats_new.get('breathing_cycles', 0)\n",
    "            window_result['new_algo_bpm'] = stats_new.get('breaths_per_minute', 0)\n",
    "            window_result['new_algo_success'] = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    New algorithm failed: {e}\")\n",
    "            window_result.update({\n",
    "                'new_algo_events': 0, 'new_algo_cycles': 0, 'new_algo_bpm': 0, 'new_algo_success': False\n",
    "            })\n",
    "        \n",
    "        # --- Method 2: Old Algorithm ---\n",
    "        try:\n",
    "            print(\"    Running old algorithm...\")\n",
    "            breath_df_old, stats_old = getBreathsConservative(respeck_window)\n",
    "            \n",
    "            window_result['old_algo_events'] = len(breath_df_old)\n",
    "            window_result['old_algo_cycles'] = min(stats_old.get('inhalations', 0), stats_old.get('exhalations', 0))\n",
    "            window_result['old_algo_bpm'] = stats_old.get('breaths_per_minute', 0)\n",
    "            window_result['old_algo_success'] = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Old algorithm failed: {e}\")\n",
    "            window_result.update({\n",
    "                'old_algo_events': 0, 'old_algo_cycles': 0, 'old_algo_bpm': 0, 'old_algo_success': False\n",
    "            })\n",
    "        \n",
    "        # --- Method 3: NeuroKit Algorithm ---\n",
    "        try:\n",
    "            print(\"    Running NeuroKit algorithm...\")\n",
    "            \n",
    "            if not NEUROKIT_AVAILABLE:\n",
    "                raise ImportError(\"NeuroKit2 not available\")\n",
    "            \n",
    "            # Extract breathing signal and clean it\n",
    "            breathing_signal = respeck_window['breathingSignal'].dropna().values\n",
    "            \n",
    "            if len(breathing_signal) < 30:  # Need minimum samples\n",
    "                raise ValueError(\"Insufficient signal length for NeuroKit\")\n",
    "            \n",
    "            # Calculate instantaneous respiratory rate\n",
    "            window_size_seconds = min(20, len(breathing_signal) / sampling_rate / 2)  # Adaptive window\n",
    "            instantaneous_rr = cal_timeseries_instantaneous_rr(\n",
    "                breathing_signal, \n",
    "                sampling_rate=sampling_rate, \n",
    "                window=int(window_size_seconds)\n",
    "            )\n",
    "            \n",
    "            if instantaneous_rr is not None and len(instantaneous_rr) > 0:\n",
    "                # Remove outliers and calculate average\n",
    "                valid_rates = instantaneous_rr[~np.isnan(instantaneous_rr)]\n",
    "                \n",
    "                if len(valid_rates) > 0:\n",
    "                    # Remove extreme outliers (outside 5-50 bpm range)\n",
    "                    valid_rates = valid_rates[(valid_rates >= 5) & (valid_rates <= 50)]\n",
    "                    \n",
    "                    if len(valid_rates) > 0:\n",
    "                        # Use median for robustness\n",
    "                        avg_neurokit_bpm = np.median(valid_rates)\n",
    "                        std_neurokit_bpm = np.std(valid_rates)\n",
    "                        \n",
    "                        window_result['neurokit_bpm'] = avg_neurokit_bpm\n",
    "                        window_result['neurokit_std'] = std_neurokit_bpm\n",
    "                        window_result['neurokit_valid_samples'] = len(valid_rates)\n",
    "                        window_result['neurokit_success'] = True\n",
    "                        \n",
    "                        print(f\"    NeuroKit: {avg_neurokit_bpm:.1f}  {std_neurokit_bpm:.1f} bpm ({len(valid_rates)} valid samples)\")\n",
    "                    else:\n",
    "                        raise ValueError(\"No valid rates after outlier removal\")\n",
    "                else:\n",
    "                    raise ValueError(\"No valid instantaneous rates calculated\")\n",
    "            else:\n",
    "                raise ValueError(\"NeuroKit returned no valid data\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    NeuroKit algorithm failed: {e}\")\n",
    "            window_result.update({\n",
    "                'neurokit_bpm': 0, 'neurokit_std': 0, 'neurokit_valid_samples': 0, 'neurokit_success': False\n",
    "            })\n",
    "        \n",
    "        results.append(window_result)\n",
    "        \n",
    "        # Print window summary\n",
    "        print(f\"    Window {i+1} Results:\")\n",
    "        print(f\"      RESpeck Built-in: {window_result.get('respeck_builtin_bpm', 0):.1f} bpm\")\n",
    "        print(f\"      New Algorithm: {window_result.get('new_algo_cycles', 0)} cycles ({window_result.get('new_algo_bpm', 0):.1f} bpm)\")\n",
    "        print(f\"      Old Algorithm: {window_result.get('old_algo_cycles', 0)} cycles ({window_result.get('old_algo_bpm', 0):.1f} bpm)\")\n",
    "        print(f\"      NeuroKit: {window_result.get('neurokit_bpm', 0):.1f} bpm\")\n",
    "    \n",
    "    # --- 5. Analyze Results ---\n",
    "    if not results:\n",
    "        print(\" No valid windows processed\")\n",
    "        return None\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    print(f\"\\n OVERALL ANALYSIS ({len(results_df)} windows)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Filter successful detections\n",
    "    valid_results = results_df[\n",
    "        (results_df['new_algo_success'] == True) & \n",
    "        (results_df['old_algo_success'] == True) &\n",
    "        (results_df['neurokit_success'] == True)\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\" Valid windows (all algorithms succeeded): {len(valid_results)}/{len(results_df)}\")\n",
    "    \n",
    "    if len(valid_results) == 0:\n",
    "        print(\" No windows where all algorithms succeeded\")\n",
    "        # Try with just successful RESpeck + NeuroKit\n",
    "        valid_results = results_df[\n",
    "            (results_df['neurokit_success'] == True)\n",
    "        ].copy()\n",
    "        print(f\" Fallback: Windows with NeuroKit success: {len(valid_results)}\")\n",
    "        \n",
    "        if len(valid_results) == 0:\n",
    "            return results_df\n",
    "    \n",
    "    # --- 6. Statistical Comparisons ---\n",
    "    print(f\"\\n STATISTICAL SUMMARY:\")\n",
    "    \n",
    "    methods = ['respeck_builtin', 'new_algo', 'old_algo', 'neurokit']\n",
    "    method_names = ['RESpeck Built-in', 'New Algorithm', 'Old Algorithm', 'NeuroKit']\n",
    "    \n",
    "    summary_stats = {}\n",
    "    \n",
    "    for method, name in zip(methods, method_names):\n",
    "        bpm_col = f'{method}_bpm'\n",
    "        \n",
    "        if bpm_col in valid_results.columns and valid_results[bpm_col].notna().any():\n",
    "            bpm_values = valid_results[bpm_col].dropna().values\n",
    "            \n",
    "            if len(bpm_values) > 0:\n",
    "                mean_bpm = np.mean(bpm_values)\n",
    "                std_bpm = np.std(bpm_values)\n",
    "                median_bpm = np.median(bpm_values)\n",
    "                \n",
    "                summary_stats[method] = {\n",
    "                    'name': name,\n",
    "                    'mean_bpm': mean_bpm,\n",
    "                    'std_bpm': std_bpm,\n",
    "                    'median_bpm': median_bpm,\n",
    "                    'values': bpm_values\n",
    "                }\n",
    "                \n",
    "                print(f\"{name}:\")\n",
    "                print(f\"   Mean: {mean_bpm:.1f}  {std_bpm:.1f} bpm\")\n",
    "                print(f\"   Median: {median_bpm:.1f} bpm\")\n",
    "                print(f\"   Range: {np.min(bpm_values):.1f} - {np.max(bpm_values):.1f} bpm\")\n",
    "                print()\n",
    "    \n",
    "    # --- 7. Correlation Analysis ---\n",
    "    print(f\" CORRELATION ANALYSIS (vs RESpeck Built-in):\")\n",
    "    \n",
    "    correlations = {}\n",
    "    \n",
    "    if len(valid_results) > 2:\n",
    "        try:\n",
    "            # Calculate correlations for all methods that have data\n",
    "            if 'new_algo_bpm' in valid_results.columns:\n",
    "                corr_new_respeck, p_new_respeck = pearsonr(valid_results['new_algo_bpm'], valid_results['respeck_builtin_bpm'])\n",
    "                correlations['new_vs_respeck'] = (corr_new_respeck, p_new_respeck)\n",
    "                print(f\"New Algorithm vs RESpeck Built-in: r = {corr_new_respeck:.3f} (p = {p_new_respeck:.3f})\")\n",
    "            \n",
    "            if 'old_algo_bpm' in valid_results.columns:\n",
    "                corr_old_respeck, p_old_respeck = pearsonr(valid_results['old_algo_bpm'], valid_results['respeck_builtin_bpm'])\n",
    "                correlations['old_vs_respeck'] = (corr_old_respeck, p_old_respeck)\n",
    "                print(f\"Old Algorithm vs RESpeck Built-in: r = {corr_old_respeck:.3f} (p = {p_old_respeck:.3f})\")\n",
    "            \n",
    "            if 'neurokit_bpm' in valid_results.columns:\n",
    "                corr_neurokit_respeck, p_neurokit_respeck = pearsonr(valid_results['neurokit_bpm'], valid_results['respeck_builtin_bpm'])\n",
    "                correlations['neurokit_vs_respeck'] = (corr_neurokit_respeck, p_neurokit_respeck)\n",
    "                print(f\"NeuroKit vs RESpeck Built-in: r = {corr_neurokit_respeck:.3f} (p = {p_neurokit_respeck:.3f})\")\n",
    "            \n",
    "            # Cross-comparisons\n",
    "            if 'new_algo_bpm' in valid_results.columns and 'old_algo_bpm' in valid_results.columns:\n",
    "                corr_new_old, p_new_old = pearsonr(valid_results['new_algo_bpm'], valid_results['old_algo_bpm'])\n",
    "                correlations['new_vs_old'] = (corr_new_old, p_new_old)\n",
    "                print(f\"New vs Old Algorithm: r = {corr_new_old:.3f} (p = {p_new_old:.3f})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" Correlation analysis failed: {e}\")\n",
    "    \n",
    "    # --- 8. Agreement Analysis ---\n",
    "    print(f\"\\n AGREEMENT ANALYSIS (vs RESpeck Built-in as Reference):\")\n",
    "    \n",
    "    algorithm_methods = [('new_algo', 'New Algorithm'), ('old_algo', 'Old Algorithm'), ('neurokit', 'NeuroKit')]\n",
    "    \n",
    "    for method, name in algorithm_methods:\n",
    "        bpm_col = f'{method}_bpm'\n",
    "        \n",
    "        if bpm_col in valid_results.columns and valid_results[bpm_col].notna().any():\n",
    "            valid_comparison = valid_results.dropna(subset=[bpm_col, 'respeck_builtin_bpm'])\n",
    "            \n",
    "            if len(valid_comparison) > 0:\n",
    "                differences = valid_comparison[bpm_col] - valid_comparison['respeck_builtin_bpm']\n",
    "                \n",
    "                mean_diff = np.mean(differences)\n",
    "                std_diff = np.std(differences)\n",
    "                mae = np.mean(np.abs(differences))\n",
    "                \n",
    "                within_2 = np.sum(np.abs(differences) <= 2) / len(differences) * 100\n",
    "                within_3 = np.sum(np.abs(differences) <= 3) / len(differences) * 100\n",
    "                \n",
    "                print(f\"{name} (n={len(valid_comparison)}):\")\n",
    "                print(f\"   Mean difference: {mean_diff:+.1f}  {std_diff:.1f} bpm\")\n",
    "                print(f\"   Mean Absolute Error: {mae:.1f} bpm\")\n",
    "                print(f\"   Within 2 bpm: {within_2:.1f}%\")\n",
    "                print(f\"   Within 3 bpm: {within_3:.1f}%\")\n",
    "                print()\n",
    "    \n",
    "    # --- 9. Create Visualizations ---\n",
    "    create_enhanced_comparison_plots(valid_results, summary_stats, correlations, window_minutes)\n",
    "    \n",
    "    # --- 10. Return Results ---\n",
    "    final_results = {\n",
    "        'all_windows': results_df,\n",
    "        'valid_windows': valid_results,\n",
    "        'summary_stats': summary_stats,\n",
    "        'correlations': correlations,\n",
    "        'num_valid_windows': len(valid_results),\n",
    "        'total_windows': len(results_df),\n",
    "        'window_duration_minutes': window_minutes,\n",
    "        'neurokit_available': NEUROKIT_AVAILABLE\n",
    "    }\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "def create_enhanced_comparison_plots(valid_results, summary_stats, correlations, window_minutes):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualization plots for the enhanced comparison including NeuroKit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Determine which methods have data\n",
    "    available_methods = []\n",
    "    method_colors = []\n",
    "    method_markers = []\n",
    "    \n",
    "    base_methods = [\n",
    "        ('respeck_builtin_bpm', 'RESpeck Built-in', 'black', 'o'),\n",
    "        ('new_algo_bpm', 'New Algorithm', 'red', 'o'),\n",
    "        ('old_algo_bpm', 'Old Algorithm', 'blue', 's'),\n",
    "        ('neurokit_bpm', 'NeuroKit', 'green', '^')\n",
    "    ]\n",
    "    \n",
    "    for col, name, color, marker in base_methods:\n",
    "        if col in valid_results.columns and valid_results[col].notna().any():\n",
    "            available_methods.append((col, name, color, marker))\n",
    "            method_colors.append(color)\n",
    "            method_markers.append(marker)\n",
    "    \n",
    "    if len(available_methods) < 2:\n",
    "        print(\" Insufficient methods with data for plotting\")\n",
    "        return\n",
    "    \n",
    "    # Create dynamic subplot layout\n",
    "    n_methods = len(available_methods)\n",
    "    if n_methods == 4:\n",
    "        fig = plt.figure(figsize=(20, 15))\n",
    "        gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(18, 12))\n",
    "        gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 1. Time series comparison (spans top row)\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    \n",
    "    for col, name, color, marker in available_methods:\n",
    "        valid_data = valid_results[col].dropna()\n",
    "        indices = valid_data.index\n",
    "        ax1.plot(indices, valid_data.values, \n",
    "                marker=marker, linestyle='-', label=name, color=color, \n",
    "                alpha=0.8, markersize=6, linewidth=2 if name == 'RESpeck Built-in' else 1.5)\n",
    "    \n",
    "    ax1.set_title(f'Breathing Rate Comparison Across {window_minutes}-Minute Windows')\n",
    "    ax1.set_xlabel('Window Number')\n",
    "    ax1.set_ylabel('Breathing Rate (breaths/min)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Box plot comparison\n",
    "    ax2 = fig.add_subplot(gs[1, 0])\n",
    "    \n",
    "    box_data = []\n",
    "    box_labels = []\n",
    "    box_colors = []\n",
    "    \n",
    "    for col, name, color, marker in available_methods:\n",
    "        valid_data = valid_results[col].dropna()\n",
    "        if len(valid_data) > 0:\n",
    "            box_data.append(valid_data.values)\n",
    "            box_labels.append(name.replace(' ', '\\n'))\n",
    "            box_colors.append(color)\n",
    "    \n",
    "    if box_data:\n",
    "        bp = ax2.boxplot(box_data, labels=box_labels, patch_artist=True)\n",
    "        for patch, color in zip(bp['boxes'], box_colors):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.6)\n",
    "    \n",
    "    ax2.set_title('Distribution Comparison')\n",
    "    ax2.set_ylabel('Breathing Rate (breaths/min)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Correlation plots - create subplots for each algorithm vs RESpeck\n",
    "    correlation_plots = []\n",
    "    algorithm_methods = [(col, name, color, marker) for col, name, color, marker in available_methods \n",
    "                        if name != 'RESpeck Built-in']\n",
    "    \n",
    "    # Plot correlations\n",
    "    plot_idx = 0\n",
    "    for col, name, color, marker in algorithm_methods:\n",
    "        if plot_idx < 2:  # Limit to available subplot positions\n",
    "            if n_methods == 4:\n",
    "                ax = fig.add_subplot(gs[1, plot_idx + 1])\n",
    "            else:\n",
    "                ax = fig.add_subplot(gs[1, 1] if plot_idx == 0 else gs[2, 0])\n",
    "            \n",
    "            # Create correlation plot\n",
    "            respeck_data = valid_results['respeck_builtin_bpm'].dropna()\n",
    "            method_data = valid_results[col].dropna()\n",
    "            \n",
    "            # Find common indices\n",
    "            common_idx = respeck_data.index.intersection(method_data.index)\n",
    "            \n",
    "            if len(common_idx) > 1:\n",
    "                x_vals = respeck_data.loc[common_idx].values\n",
    "                y_vals = method_data.loc[common_idx].values\n",
    "                \n",
    "                ax.scatter(x_vals, y_vals, alpha=0.7, color=color, s=50)\n",
    "                \n",
    "                # Perfect agreement line\n",
    "                min_val = min(np.min(x_vals), np.min(y_vals))\n",
    "                max_val = max(np.max(x_vals), np.max(y_vals))\n",
    "                ax.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.5, label='Perfect Agreement')\n",
    "                \n",
    "                # Regression line\n",
    "                corr_key = f\"{col.replace('_bpm', '')}_vs_respeck\"\n",
    "                if corr_key in correlations:\n",
    "                    z = np.polyfit(x_vals, y_vals, 1)\n",
    "                    p = np.poly1d(z)\n",
    "                    ax.plot(x_vals, p(x_vals), color=color, alpha=0.8, \n",
    "                           label=f'r = {correlations[corr_key][0]:.3f}')\n",
    "                \n",
    "                ax.set_xlabel('RESpeck Built-in (breaths/min)')\n",
    "                ax.set_ylabel(f'{name} (breaths/min)')\n",
    "                ax.set_title(f'{name} vs RESpeck Built-in')\n",
    "                ax.legend()\n",
    "                ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plot_idx += 1\n",
    "    \n",
    "    # 4. Bland-Altman plot comparing all algorithms vs RESpeck Built-in\n",
    "    if n_methods == 4:\n",
    "        ax_ba = fig.add_subplot(gs[2, :])\n",
    "    else:\n",
    "        ax_ba = fig.add_subplot(gs[2, 1])\n",
    "    \n",
    "    # Perfect agreement line\n",
    "    ax_ba.axhline(y=0, color='black', linestyle='-', alpha=0.5, label='Perfect Agreement')\n",
    "    \n",
    "    for col, name, color, marker in algorithm_methods:\n",
    "        respeck_data = valid_results['respeck_builtin_bpm'].dropna()\n",
    "        method_data = valid_results[col].dropna()\n",
    "        common_idx = respeck_data.index.intersection(method_data.index)\n",
    "        \n",
    "        if len(common_idx) > 0:\n",
    "            differences = method_data.loc[common_idx] - respeck_data.loc[common_idx]\n",
    "            mean_diff = differences.mean()\n",
    "            \n",
    "            # Mean line for this method\n",
    "            ax_ba.axhline(y=mean_diff, color=color, linestyle='-', linewidth=2,\n",
    "                         label=f'{name} (Mean: {mean_diff:.1f})')\n",
    "    \n",
    "    ax_ba.set_xlabel('Breathing Rate Range (breaths/min)')\n",
    "    ax_ba.set_ylabel('Algorithm - RESpeck Built-in (breaths/min)')\n",
    "    ax_ba.set_title('Bland-Altman: All Algorithms vs RESpeck Built-in')\n",
    "    ax_ba.legend()\n",
    "    ax_ba.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'Enhanced Algorithms vs RESpeck Built-in Comparison\\n({len(valid_results)} valid {window_minutes}-minute windows)', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def run_enhanced_respeck_comparison(respeck_df, window_minutes=5):\n",
    "    \"\"\"\n",
    "    Run the enhanced comparison including NeuroKit and provide a comprehensive summary.\n",
    "    \"\"\"\n",
    "    print(\" STARTING ENHANCED ALGORITHMS vs RESpeck COMPARISON\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Check NeuroKit availability\n",
    "    if not NEUROKIT_AVAILABLE:\n",
    "        print(\"  NeuroKit2 not available - install with: pip install neurokit2\")\n",
    "        print(\"   Continuing with available algorithms only...\")\n",
    "    \n",
    "    # Run the comparison\n",
    "    results = compare_algorithms_vs_respeck_builtin_with_neurokit(respeck_df, window_minutes=window_minutes)\n",
    "    \n",
    "    if results is None:\n",
    "        print(\" Comparison failed - check your data\")\n",
    "        return None\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" ENHANCED FINAL SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    valid_windows = results['num_valid_windows']\n",
    "    total_windows = results['total_windows']\n",
    "    \n",
    "    print(f\" Windows analyzed: {valid_windows}/{total_windows} successful\")\n",
    "    print(f\"  Window duration: {window_minutes} minutes each\")\n",
    "    print(f\" NeuroKit available: {results['neurokit_available']}\")\n",
    "    \n",
    "    if valid_windows > 0:\n",
    "        correlations = results['correlations']\n",
    "        \n",
    "        print(f\"\\n PERFORMANCE RANKINGS (vs RESpeck Built-in Reference):\")\n",
    "        \n",
    "        # Rank methods by correlation with RESpeck built-in\n",
    "        if correlations:\n",
    "            method_correlations = []\n",
    "            \n",
    "            for key, (corr, p_val) in correlations.items():\n",
    "                if '_vs_respeck' in key and key != 'new_vs_old':\n",
    "                    method_name = key.replace('_vs_respeck', '').replace('_', ' ').title()\n",
    "                    if 'neurokit' in key.lower():\n",
    "                        method_name = 'NeuroKit'\n",
    "                    elif 'new' in key.lower():\n",
    "                        method_name = 'New Algorithm'\n",
    "                    elif 'old' in key.lower():\n",
    "                        method_name = 'Old Algorithm'\n",
    "                    \n",
    "                    method_correlations.append((method_name, corr, p_val))\n",
    "            \n",
    "            # Sort by correlation strength\n",
    "            method_correlations.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "            \n",
    "            for i, (method, corr, p_val) in enumerate(method_correlations, 1):\n",
    "                significance = \"***\" if p_val < 0.001 else \"**\" if p_val < 0.01 else \"*\" if p_val < 0.05 else \"\"\n",
    "                print(f\"   {i}. {method}: r = {corr:.3f}{significance}\")\n",
    "        \n",
    "        # Calculate mean absolute errors for recommendation\n",
    "        valid_data = results['valid_windows']\n",
    "        if len(valid_data) > 0:\n",
    "            print(f\"\\n ACCURACY COMPARISON (vs RESpeck Built-in):\")\n",
    "            \n",
    "            algorithm_methods = [\n",
    "                ('new_algo_bpm', 'New Algorithm'),\n",
    "                ('old_algo_bpm', 'Old Algorithm'),\n",
    "                ('neurokit_bpm', 'NeuroKit')\n",
    "            ]\n",
    "            \n",
    "            best_method = None\n",
    "            best_mae = float('inf')\n",
    "            \n",
    "            for col, name in algorithm_methods:\n",
    "                if col in valid_data.columns and valid_data[col].notna().any():\n",
    "                    valid_comparison = valid_data.dropna(subset=[col, 'respeck_builtin_bpm'])\n",
    "                    \n",
    "                    if len(valid_comparison) > 0:\n",
    "                        mae = np.mean(np.abs(valid_comparison[col] - valid_comparison['respeck_builtin_bpm']))\n",
    "                        print(f\"   {name} MAE: {mae:.1f} breaths/min (n={len(valid_comparison)})\")\n",
    "                        \n",
    "                        if mae < best_mae:\n",
    "                            best_mae = mae\n",
    "                            best_method = name\n",
    "            \n",
    "            print(f\"\\n RECOMMENDATION:\")\n",
    "            if best_method:\n",
    "                print(f\" Best performing method: {best_method} (MAE: {best_mae:.1f} bpm)\")\n",
    "            else:\n",
    "                print(f\" Unable to determine best method - insufficient data\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Usage example:\n",
    "results = run_enhanced_respeck_comparison(respeck_df, window_minutes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d83586",
   "metadata": {},
   "source": [
    "## PSG ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7390fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import find_peaks, detrend\n",
    "\n",
    "def simple_5min_breath_comparison_nasal(respeck_df, nasal_df, respeck_algorithm_func, \n",
    "                                        nasal_flow_column='flow_rate'):\n",
    "    \"\"\"\n",
    "    Simple 5-minute window comparison: count breaths detected by respeck vs nasal cannula\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\" SIMPLE 5-MINUTE WINDOW COMPARISON\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Get Nasal Cannula breaths\n",
    "    print(\"Processing Nasal Cannula...\")\n",
    "    nasal_signal, nasal_sampling_rate = process_nasal_signal(nasal_df, nasal_flow_column)\n",
    "    nasal_breath_indices, _ = detect_nasal_breaths(nasal_signal, nasal_sampling_rate)\n",
    "    nasal_breath_times = pd.to_datetime(nasal_df.loc[nasal_breath_indices, 'timestamp'])\n",
    "    if nasal_breath_times.dt.tz is not None:\n",
    "        nasal_breath_times = nasal_breath_times.dt.tz_convert('UTC').dt.tz_localize(None)\n",
    "    \n",
    "    # 2. Get RESPeck breath cycles\n",
    "    print(\"Processing RESPeck...\")\n",
    "    respeck_results, _ = respeck_algorithm_func(respeck_df, adaptation_window_minutes=10, pad_duration_minutes=10)\n",
    "    respeck_cycles_df, _ = convert_respeck_events_to_cycles(respeck_results)\n",
    "    respeck_cycle_times = pd.to_datetime(respeck_cycles_df['cycle_time'])\n",
    "    if respeck_cycle_times.dt.tz is not None:\n",
    "        respeck_cycle_times = respeck_cycle_times.dt.tz_convert('UTC').dt.tz_localize(None)\n",
    "    \n",
    "    # 3. Find overlap period\n",
    "    overlap_start = max(nasal_breath_times.iloc[0], respeck_cycle_times.iloc[0])\n",
    "    overlap_end = min(nasal_breath_times.iloc[-1], respeck_cycle_times.iloc[-1])\n",
    "    \n",
    "    print(f\"Overlap: {overlap_start.strftime('%H:%M:%S')} to {overlap_end.strftime('%H:%M:%S')}\")\n",
    "    \n",
    "    # 4. Create 5-minute windows\n",
    "    window_starts = pd.date_range(start=overlap_start, end=overlap_end-pd.Timedelta(minutes=5), freq='5min')\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, window_start in enumerate(window_starts):\n",
    "        window_end = window_start + pd.Timedelta(minutes=5)\n",
    "        \n",
    "        # Count breaths in this 5-minute window\n",
    "        nasal_count = len(nasal_breath_times[(nasal_breath_times >= window_start) & (nasal_breath_times < window_end)])\n",
    "        respeck_count = len(respeck_cycle_times[(respeck_cycle_times >= window_start) & (respeck_cycle_times < window_end)])\n",
    "        \n",
    "        # ADDED: Filter out windows outside 58-110 breath range\n",
    "        # if 58 <= nasal_count <= 110 and 58 <= respeck_count <= 110:\n",
    "        results.append({\n",
    "            'Window': i+1,\n",
    "            'Start_Time': window_start.strftime('%H:%M:%S'),\n",
    "            'Nasal_Breaths': nasal_count,\n",
    "            'RESPeck_Breaths': respeck_count,\n",
    "            'Difference': respeck_count - nasal_count\n",
    "        })\n",
    "    \n",
    "    # 5. Create results DataFrame\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    # 6. Display results\n",
    "    print(f\"\\n BREATH COUNTS PER 5-MINUTE WINDOW (58-110 range only):\")\n",
    "    print(df_results.to_string(index=False))\n",
    "    \n",
    "    # 7. Summary stats\n",
    "    print(f\"\\n SUMMARY:\")\n",
    "    print(f\"Valid windows: {len(df_results)}\")\n",
    "    if not df_results.empty:\n",
    "        print(f\"Nasal average: {df_results['Nasal_Breaths'].mean():.1f} breaths per 5 minutes\")\n",
    "        print(f\"RESPeck average: {df_results['RESPeck_Breaths'].mean():.1f} breaths per 5 minutes\")\n",
    "        print(f\"Average difference: {df_results['Difference'].mean():.1f} breaths per 5 minutes\")\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "def plot_breath_comparison(df_results):\n",
    "    \"\"\"\n",
    "    Plot the valid windows comparing respeck and nasal breaths\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    if df_results.empty:\n",
    "        print(\"No valid windows to plot\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Create x-axis values (window numbers)\n",
    "    windows = df_results['Window'].values\n",
    "    nasal_breaths = df_results['Nasal_Breaths'].values\n",
    "    respeck_breaths = df_results['RESPeck_Breaths'].values\n",
    "    \n",
    "    # Plot both series\n",
    "    plt.plot(windows, nasal_breaths, 'o-', label='Nasal Cannula', color='blue', linewidth=2, markersize=6)\n",
    "    plt.plot(windows, respeck_breaths, 's-', label='RESPeck', color='red', linewidth=2, markersize=6)\n",
    "    \n",
    "    # Add reference lines for the valid range\n",
    "    plt.axhline(y=58, color='gray', linestyle='--', alpha=0.5, label='Valid range (58-110)')\n",
    "    plt.axhline(y=110, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Formatting\n",
    "    plt.xlabel('Window Number', fontsize=12)\n",
    "    plt.ylabel('Breaths per 5 minutes', fontsize=12)\n",
    "    plt.title('Breath Count Comparison: RESPeck vs Nasal Cannula\\n(Valid Windows Only: 58-110 breaths)', fontsize=14)\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Set y-axis limits with some padding\n",
    "    y_min = min(min(nasal_breaths), min(respeck_breaths)) - 5\n",
    "    y_max = max(max(nasal_breaths), max(respeck_breaths)) + 5\n",
    "    plt.ylim(y_min, y_max)\n",
    "    \n",
    "    # Add window start times as x-axis labels if not too many windows\n",
    "    if len(df_results) <= 20:\n",
    "        plt.xticks(windows, df_results['Start_Time'].values, rotation=45)\n",
    "    else:\n",
    "        plt.xticks(windows[::max(1, len(windows)//10)])  # Show every 10th window if too many\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Second plot: Difference plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    differences = df_results['Difference'].values\n",
    "    \n",
    "    colors = ['green' if d >= 0 else 'orange' for d in differences]\n",
    "    plt.bar(windows, differences, color=colors, alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    plt.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "    plt.xlabel('Window Number', fontsize=12)\n",
    "    plt.ylabel('Difference (RESPeck - Nasal)', fontsize=12)\n",
    "    plt.title('Breath Count Differences by Window\\n(Positive = RESPeck higher, Negative = Nasal higher)', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add window start times as x-axis labels if not too many windows\n",
    "    if len(df_results) <= 20:\n",
    "        plt.xticks(windows, df_results['Start_Time'].values, rotation=45)\n",
    "    else:\n",
    "        plt.xticks(windows[::max(1, len(windows)//10)])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print some stats\n",
    "    print(f\"\\n PLOT STATISTICS:\")\n",
    "    print(f\"Mean absolute difference: {abs(differences).mean():.1f} breaths\")\n",
    "    print(f\"RESPeck higher in {sum(d > 0 for d in differences)} windows\")\n",
    "    print(f\"Nasal higher in {sum(d < 0 for d in differences)} windows\")\n",
    "    print(f\"Exact match in {sum(d == 0 for d in differences)} windows\")\n",
    "\n",
    "def process_nasal_signal(nasal_df, flow_column='flow_rate'):\n",
    "    \"\"\"\n",
    "    Process nasal cannula signal - just center it around zero\n",
    "    \"\"\"\n",
    "    nasal_signal = nasal_df[flow_column].copy()\n",
    "    timestamps = pd.to_datetime(nasal_df['timestamp'])\n",
    "    time_diffs = timestamps.diff().dt.total_seconds().dropna()\n",
    "    sampling_rate = 1 / time_diffs.median()\n",
    "    \n",
    "    # Simple centering - subtract rolling mean\n",
    "    window_samples = int(2 * 60 * sampling_rate)  # 2 minutes\n",
    "    rolling_mean = nasal_signal.rolling(window=window_samples, center=True).mean()\n",
    "    centered_signal = nasal_signal - rolling_mean\n",
    "    \n",
    "    return centered_signal, sampling_rate\n",
    "\n",
    "def detect_nasal_breaths(centered_signal, sampling_rate):\n",
    "    \"\"\"\n",
    "    Detect breaths in nasal cannula data\n",
    "    \"\"\"\n",
    "    valid_signal = centered_signal.dropna()\n",
    "    \n",
    "    # Test both polarities to find which direction represents inhalation\n",
    "    pos_peaks, _ = find_peaks(valid_signal, distance=int(sampling_rate * 1.0))\n",
    "    neg_peaks, _ = find_peaks(-valid_signal, distance=int(sampling_rate * 1.0))\n",
    "    \n",
    "    pos_strength = np.mean(valid_signal.iloc[pos_peaks]) if len(pos_peaks) > 0 else 0\n",
    "    neg_strength = np.mean(-valid_signal.iloc[neg_peaks]) if len(neg_peaks) > 0 else 0\n",
    "    \n",
    "    # Choose stronger polarity\n",
    "    if pos_strength > neg_strength:\n",
    "        signal_for_detection = valid_signal\n",
    "    else:\n",
    "        signal_for_detection = -valid_signal\n",
    "    \n",
    "    # Detect breaths using MAD-based thresholds\n",
    "    signal_mad = np.median(np.abs(signal_for_detection - np.median(signal_for_detection)))\n",
    "    height_threshold = 1.5 * signal_mad\n",
    "    prominence_threshold = 1.0 * signal_mad\n",
    "    \n",
    "    breath_peaks, _ = find_peaks(\n",
    "        signal_for_detection,\n",
    "        height=height_threshold,\n",
    "        prominence=prominence_threshold,\n",
    "        distance=int(sampling_rate * 1.0)  # Minimum 1 second between breaths\n",
    "    )\n",
    "    \n",
    "    return valid_signal.index[breath_peaks], 'breath'\n",
    "\n",
    "def convert_respeck_events_to_cycles(respeck_df):\n",
    "    \"\"\"\n",
    "    Convert respeck inhalation/exhalation events to breath cycles\n",
    "    \"\"\"\n",
    "    if respeck_df.empty:\n",
    "        return pd.DataFrame(), 0\n",
    "    \n",
    "    inhalations = respeck_df[respeck_df['type'] == 'Inhalation'].copy()\n",
    "    exhalations = respeck_df[respeck_df['type'] == 'Exhalation'].copy()\n",
    "    \n",
    "    inhalations['timestamp'] = pd.to_datetime(inhalations['timestamp'])\n",
    "    exhalations['timestamp'] = pd.to_datetime(exhalations['timestamp'])\n",
    "    \n",
    "    breath_cycles = []\n",
    "    used_exhalations = set()\n",
    "    \n",
    "    for _, inhalation in inhalations.iterrows():\n",
    "        time_diffs = np.abs((exhalations['timestamp'] - inhalation['timestamp']).dt.total_seconds())\n",
    "        \n",
    "        for idx in time_diffs.argsort():\n",
    "            if idx not in used_exhalations and time_diffs.iloc[idx] <= 5:\n",
    "                exhalation = exhalations.iloc[idx]\n",
    "                cycle_time = min(inhalation['timestamp'], exhalation['timestamp'])\n",
    "                breath_cycles.append({'cycle_time': cycle_time})\n",
    "                used_exhalations.add(idx)\n",
    "                break\n",
    "    \n",
    "    return pd.DataFrame(breath_cycles), len(breath_cycles)\n",
    "\n",
    "# USAGE:\n",
    "def run_comparison(respeck_df, nasal_df, nasal_flow_column='flow_rate'):\n",
    "    \"\"\"\n",
    "    Run the comparison between respeck and nasal cannula\n",
    "    \"\"\"\n",
    "    results = simple_5min_breath_comparison_nasal(\n",
    "        respeck_df=respeck_df,\n",
    "        nasal_df=nasal_df,\n",
    "        respeck_algorithm_func=adaptive_breath_detection_original_fixed,\n",
    "        nasal_flow_column=nasal_flow_column\n",
    "    )\n",
    "    \n",
    "    # Plot the results\n",
    "    plot_breath_comparison(results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "results = run_comparison(respeck_df, psg_df, 'Resp nasal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f960cb",
   "metadata": {},
   "source": [
    "## Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50be50f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from scipy.stats import spearmanr, skew, kurtosis\n",
    "from scipy import signal as scipy_signal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_pytorch_model(model_path):\n",
    "    \"\"\"\n",
    "    Load PyTorch model - handles both full model and state_dict cases\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try loading as full model first\n",
    "        checkpoint = torch.load(model_path, map_location='cpu')\n",
    "        \n",
    "        if hasattr(checkpoint, 'eval'):\n",
    "            print(\"Loaded full model\")\n",
    "            return checkpoint, 'full_model'\n",
    "        elif isinstance(checkpoint, dict):\n",
    "            if 'model_state_dict' in checkpoint:\n",
    "                print(\"\" \\\n",
    "                \" checkpoint with state_dict\")\n",
    "                return checkpoint['model_state_dict'], 'state_dict'\n",
    "            else:\n",
    "                print(\"Loaded raw state_dict\")\n",
    "                return checkpoint, 'state_dict'\n",
    "        else:\n",
    "            print(\"Unknown model format\")\n",
    "            return checkpoint, 'unknown'\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def robust_breath_features(signal_segment, timestamps_segment):\n",
    "    \"\"\"\n",
    "    Robust feature extraction that handles edge cases\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        # Basic statistical features (always computable)\n",
    "        features['signal_mean'] = np.mean(signal_segment)\n",
    "        features['signal_std'] = np.std(signal_segment)\n",
    "        features['signal_var'] = np.var(signal_segment)\n",
    "        features['signal_min'] = np.min(signal_segment)\n",
    "        features['signal_max'] = np.max(signal_segment)\n",
    "        features['signal_range'] = features['signal_max'] - features['signal_min']\n",
    "        features['signal_skewness'] = skew(signal_segment) if len(signal_segment) > 2 else 0\n",
    "        features['signal_kurtosis'] = kurtosis(signal_segment) if len(signal_segment) > 3 else 0\n",
    "        \n",
    "        # Zero crossing rate\n",
    "        zero_crossings = np.sum(np.diff(np.sign(signal_segment - np.mean(signal_segment))) != 0)\n",
    "        features['zero_crossing_rate'] = zero_crossings / len(signal_segment)\n",
    "        \n",
    "        # RMS (Root Mean Square) - energy measure\n",
    "        features['rms'] = np.sqrt(np.mean(signal_segment**2))\n",
    "        \n",
    "        # Frequency domain features\n",
    "        try:\n",
    "            # Power spectral density\n",
    "            freqs, psd = scipy_signal.welch(signal_segment, fs=12.5, nperseg=min(64, len(signal_segment)//4))\n",
    "            \n",
    "            # Define frequency bands for breathing analysis\n",
    "            very_low_freq = (freqs >= 0.008) & (freqs < 0.04)   # Apnea cycling\n",
    "            low_freq = (freqs >= 0.04) & (freqs < 0.15)         # Abnormal patterns  \n",
    "            normal_breathing = (freqs >= 0.15) & (freqs < 0.5)   # Normal breathing\n",
    "            high_freq = (freqs >= 0.5) & (freqs < 2.0)          # Effort/artifacts\n",
    "            \n",
    "            # Calculate power in each band\n",
    "            total_power = np.sum(psd) + 1e-10\n",
    "            features['vlf_power_ratio'] = np.sum(psd[very_low_freq]) / total_power if np.any(very_low_freq) else 0\n",
    "            features['lf_power_ratio'] = np.sum(psd[low_freq]) / total_power if np.any(low_freq) else 0\n",
    "            features['normal_power_ratio'] = np.sum(psd[normal_breathing]) / total_power if np.any(normal_breathing) else 0\n",
    "            features['hf_power_ratio'] = np.sum(psd[high_freq]) / total_power if np.any(high_freq) else 0\n",
    "            \n",
    "            # Dominant frequency\n",
    "            if len(psd) > 0:\n",
    "                dominant_freq_idx = np.argmax(psd)\n",
    "                features['dominant_frequency'] = freqs[dominant_freq_idx]\n",
    "                features['dominant_power'] = psd[dominant_freq_idx]\n",
    "            else:\n",
    "                features['dominant_frequency'] = 0\n",
    "                features['dominant_power'] = 0\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Frequency analysis failed: {e}\")\n",
    "            features.update({\n",
    "                'vlf_power_ratio': 0, 'lf_power_ratio': 0, \n",
    "                'normal_power_ratio': 0, 'hf_power_ratio': 0,\n",
    "                'dominant_frequency': 0, 'dominant_power': 0\n",
    "            })\n",
    "        \n",
    "        # Try to get breathing-specific features from your original function\n",
    "        try:\n",
    "            from calculateContinuousBreathFeatures import calculate_TS_breathFeatures\n",
    "            breath_features = calculate_TS_breathFeatures(timestamps_segment, signal_segment)\n",
    "            \n",
    "            if breath_features:\n",
    "                # Extract key features if they exist and are not empty\n",
    "                if 'amplitude' in breath_features and len(breath_features['amplitude']) > 0:\n",
    "                    amplitudes = np.array(breath_features['amplitude'])\n",
    "                    features['amplitude_mean'] = np.mean(amplitudes)\n",
    "                    features['amplitude_std'] = np.std(amplitudes)\n",
    "                    features['amplitude_cv'] = features['amplitude_std'] / (features['amplitude_mean'] + 1e-10)\n",
    "                    \n",
    "                    # Amplitude reduction analysis\n",
    "                    features['amplitude_p10'] = np.percentile(amplitudes, 10)\n",
    "                    features['amplitude_p50'] = np.percentile(amplitudes, 50)  \n",
    "                    features['amplitude_p90'] = np.percentile(amplitudes, 90)\n",
    "                    features['amplitude_reduction_ratio'] = 1 - (features['amplitude_p10'] / (features['amplitude_p90'] + 1e-10))\n",
    "                \n",
    "                if 'breath_durations' in breath_features and len(breath_features['breath_durations']) > 0:\n",
    "                    durations = np.array(breath_features['breath_durations'])\n",
    "                    features['breath_duration_mean'] = np.mean(durations)\n",
    "                    features['breath_duration_std'] = np.std(durations)\n",
    "                    features['breath_duration_cv'] = features['breath_duration_std'] / (features['breath_duration_mean'] + 1e-10)\n",
    "                    features['long_breath_ratio'] = np.sum(durations > 20) / len(durations)  # >20 sec breaths\n",
    "                \n",
    "                if 'rr' in breath_features and len(breath_features['rr']) > 0:\n",
    "                    rr = np.array(breath_features['rr'])\n",
    "                    rr = rr[~np.isnan(rr)]  # Remove NaN values\n",
    "                    if len(rr) > 0:\n",
    "                        features['respiratory_rate_mean'] = np.mean(rr)\n",
    "                        features['respiratory_rate_std'] = np.std(rr)\n",
    "                        features['respiratory_rate_cv'] = features['respiratory_rate_std'] / (features['respiratory_rate_mean'] + 1e-10)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Breath feature extraction failed: {e}\")\n",
    "            # Set default values for breath features\n",
    "            breath_feature_defaults = {\n",
    "                'amplitude_mean': 0, 'amplitude_std': 0, 'amplitude_cv': 0,\n",
    "                'amplitude_p10': 0, 'amplitude_p50': 0, 'amplitude_p90': 0,\n",
    "                'amplitude_reduction_ratio': 0, 'breath_duration_mean': 0,\n",
    "                'breath_duration_std': 0, 'breath_duration_cv': 0,\n",
    "                'long_breath_ratio': 0, 'respiratory_rate_mean': 0,\n",
    "                'respiratory_rate_std': 0, 'respiratory_rate_cv': 0\n",
    "            }\n",
    "            features.update(breath_feature_defaults)\n",
    "        \n",
    "        # Additional interpretable features for OSA\n",
    "        \n",
    "        # Activity level (how much movement/breathing effort)\n",
    "        features['activity_level'] = np.mean(np.abs(np.diff(signal_segment)))\n",
    "        \n",
    "        # Signal variability in different time scales\n",
    "        if len(signal_segment) >= 20:\n",
    "            # Short-term variability (every 5 samples  0.4 seconds)\n",
    "            short_segments = signal_segment[::5]\n",
    "            features['short_term_variability'] = np.std(short_segments)\n",
    "            \n",
    "            # Long-term trend\n",
    "            if len(signal_segment) >= 60:\n",
    "                long_segments = signal_segment[::12]  # Every ~1 second\n",
    "                features['long_term_trend'] = np.abs(np.polyfit(range(len(long_segments)), long_segments, 1)[0])\n",
    "            else:\n",
    "                features['long_term_trend'] = 0\n",
    "        else:\n",
    "            features['short_term_variability'] = 0\n",
    "            features['long_term_trend'] = 0\n",
    "            \n",
    "        # Breathing irregularity proxy\n",
    "        signal_envelope = np.abs(scipy_signal.hilbert(signal_segment - np.mean(signal_segment)))\n",
    "        features['envelope_std'] = np.std(signal_envelope)\n",
    "        features['envelope_cv'] = features['envelope_std'] / (np.mean(signal_envelope) + 1e-10)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Feature extraction failed completely: {e}\")\n",
    "        # Return basic features at minimum\n",
    "        features = {\n",
    "            'signal_mean': np.mean(signal_segment),\n",
    "            'signal_std': np.std(signal_segment),\n",
    "            'signal_range': np.max(signal_segment) - np.min(signal_segment)\n",
    "        }\n",
    "    \n",
    "    return features\n",
    "\n",
    "def analyze_with_model(model_path, respiratory_signal, timestamps, is_pickle=False, window_size=30):\n",
    "    \"\"\"\n",
    "    Analyze respiratory signal with either PyTorch or Pickle model\n",
    "    \"\"\"\n",
    "    print(\"Starting robust model interpretation...\")\n",
    "    \n",
    "\n",
    "    model, model_type = load_pytorch_model(model_path)\n",
    "    \n",
    "    if model is None:\n",
    "        print(\"Model loading failed. Analyzing features only...\")\n",
    "        return analyze_features_only(respiratory_signal, timestamps, window_size)\n",
    "    \n",
    "    sampling_rate = 12.5\n",
    "    window_samples = int(window_size * sampling_rate)\n",
    "    \n",
    "    all_features = []\n",
    "    all_predictions = []\n",
    "    segment_info = []\n",
    "    \n",
    "    successful_segments = 0\n",
    "    \n",
    "    print(f\"Processing {len(respiratory_signal)} samples into {window_size}s windows...\")\n",
    "    \n",
    "    # For PyTorch state_dict, try to load architecture\n",
    "    pytorch_model = None\n",
    "    if model_type == 'state_dict':\n",
    "        print(\"Analyzing features only...\")\n",
    "        return analyze_features_only(respiratory_signal, timestamps, window_size)\n",
    "    elif model_type == 'full_model':\n",
    "        pytorch_model = model\n",
    "    \n",
    "    # Process in windows\n",
    "    for i in range(0, len(respiratory_signal) - window_samples, window_samples // 2):\n",
    "        signal_segment = respiratory_signal[i:i + window_samples]\n",
    "        time_segment = timestamps[i:i + window_samples]\n",
    "        \n",
    "        try:\n",
    "            # Extract robust features\n",
    "            features = robust_breath_features(signal_segment, time_segment)\n",
    "            all_features.append(features)\n",
    "            \n",
    "            # Try to get model prediction\n",
    "            prediction = None\n",
    "            if is_pickle:\n",
    "                # For pickle models (sklearn, etc.)\n",
    "                try:\n",
    "                    feature_array = np.array([list(features.values())]).reshape(1, -1)\n",
    "                    \n",
    "                    if hasattr(model, 'predict_proba'):\n",
    "                        pred_proba = model.predict_proba(feature_array)[0]\n",
    "                        prediction = {\n",
    "                            'class': np.argmax(pred_proba),\n",
    "                            'probabilities': pred_proba,\n",
    "                            'confidence': np.max(pred_proba)\n",
    "                        }\n",
    "                    elif hasattr(model, 'predict'):\n",
    "                        pred_class = model.predict(feature_array)[0]\n",
    "                        prediction = {\n",
    "                            'class': pred_class,\n",
    "                            'confidence': 0.5  # Unknown confidence for simple predict\n",
    "                        }\n",
    "                except Exception as e:\n",
    "                    print(f\"Pickle model prediction failed for segment {successful_segments}: {e}\")\n",
    "                    prediction = {'class': 0, 'confidence': 0.0}\n",
    "            else:\n",
    "                # For PyTorch models\n",
    "                try:\n",
    "                    if pytorch_model is not None:\n",
    "                        pytorch_model.eval()\n",
    "                        \n",
    "                        # Prepare input based on architecture\n",
    "                        model_input = torch.FloatTensor(signal_segment)\n",
    "                        \n",
    "                        # Add appropriate dimensions\n",
    "                        if 'cnn' in str(type(pytorch_model)).lower():\n",
    "                            model_input = model_input.unsqueeze(0).unsqueeze(0)  # [batch, channel, length]\n",
    "                        elif 'lstm' in str(type(pytorch_model)).lower():\n",
    "                            model_input = model_input.unsqueeze(0).unsqueeze(-1)  # [batch, length, features]\n",
    "                        else:\n",
    "                            model_input = model_input.unsqueeze(0)  # [batch, length]\n",
    "                        \n",
    "                        with torch.no_grad():\n",
    "                            output = pytorch_model(model_input)\n",
    "                            if output.shape[-1] > 1:\n",
    "                                probs = torch.softmax(output, dim=-1).numpy()[0]\n",
    "                                prediction = {\n",
    "                                    'class': np.argmax(probs),\n",
    "                                    'probabilities': probs,\n",
    "                                    'confidence': np.max(probs)\n",
    "                                }\n",
    "                            else:\n",
    "                                prob = torch.sigmoid(output).numpy()[0][0]\n",
    "                                prediction = {\n",
    "                                    'class': int(prob > 0.5),\n",
    "                                    'probability': prob,\n",
    "                                    'confidence': abs(prob - 0.5) * 2\n",
    "                                }\n",
    "                    else:\n",
    "                        prediction = {'class': 0, 'confidence': 0.0}\n",
    "                except Exception as e:\n",
    "                    print(f\"PyTorch model prediction failed for segment {successful_segments}: {e}\")\n",
    "                    prediction = {'class': 0, 'confidence': 0.0}\n",
    "            \n",
    "            all_predictions.append(prediction)\n",
    "            \n",
    "            segment_info.append({\n",
    "                'start_time': time_segment[0],\n",
    "                'end_time': time_segment[-1],\n",
    "                'segment_index': successful_segments\n",
    "            })\n",
    "            \n",
    "            successful_segments += 1\n",
    "            \n",
    "            if successful_segments % 50 == 0:\n",
    "                print(f\"Successfully processed {successful_segments} segments...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing segment {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Successfully processed {successful_segments} segments\")\n",
    "    \n",
    "    # Analyze results\n",
    "    if all_predictions and any(pred.get('confidence', 0) > 0 for pred in all_predictions):\n",
    "        correlation_analysis = analyze_feature_correlations(all_predictions, all_features)\n",
    "        \n",
    "        # Print prediction distribution\n",
    "        classes = [pred['class'] for pred in all_predictions]\n",
    "        unique, counts = np.unique(classes, return_counts=True)\n",
    "        print(f\"\\nModel predictions distribution:\")\n",
    "        class_names = {0: 'Normal', 1: 'Hypopnea', 2: 'Apnea'}\n",
    "        for cls, count in zip(unique, counts):\n",
    "            print(f\"  {class_names.get(cls, f'Class {cls}')}: {count} segments ({count/len(classes)*100:.1f}%)\")\n",
    "    else:\n",
    "        correlation_analysis = None\n",
    "        print(\"No valid predictions obtained - analyzing features only\")\n",
    "    \n",
    "    # Feature distribution analysis\n",
    "    analyze_feature_distributions(all_features)\n",
    "    \n",
    "    return {\n",
    "        'predictions': all_predictions,\n",
    "        'features': all_features,\n",
    "        'segment_info': segment_info,\n",
    "        'correlation_analysis': correlation_analysis\n",
    "    }\n",
    "\n",
    "def analyze_features_only(respiratory_signal, timestamps, window_size=30):\n",
    "    \"\"\"\n",
    "    Analyze features without model predictions\n",
    "    \"\"\"\n",
    "    print(\"Analyzing features only...\")\n",
    "    \n",
    "    sampling_rate = 12.5\n",
    "    window_samples = int(window_size * sampling_rate)\n",
    "    \n",
    "    all_features = []\n",
    "    segment_info = []\n",
    "    successful_segments = 0\n",
    "    \n",
    "    for i in range(0, len(respiratory_signal) - window_samples, window_samples // 2):\n",
    "        signal_segment = respiratory_signal[i:i + window_samples]\n",
    "        time_segment = timestamps[i:i + window_samples]\n",
    "        \n",
    "        try:\n",
    "            features = robust_breath_features(signal_segment, time_segment)\n",
    "            all_features.append(features)\n",
    "            \n",
    "            segment_info.append({\n",
    "                'start_time': time_segment[0],\n",
    "                'end_time': time_segment[-1],\n",
    "                'segment_index': successful_segments\n",
    "            })\n",
    "            \n",
    "            successful_segments += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing segment {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Successfully processed {successful_segments} segments\")\n",
    "    analyze_feature_distributions(all_features)\n",
    "    \n",
    "    return {\n",
    "        'features': all_features,\n",
    "        'segment_info': segment_info\n",
    "    }\n",
    "\n",
    "def analyze_feature_correlations(predictions, features):\n",
    "    \"\"\"\n",
    "    Analyze correlations between features and model predictions\n",
    "    \"\"\"\n",
    "    if not features or not predictions:\n",
    "        return None\n",
    "    \n",
    "    # Convert to arrays\n",
    "    classes = [pred.get('class', 0) for pred in predictions]\n",
    "    confidences = [pred.get('confidence', 0) for pred in predictions]\n",
    "    \n",
    "    # Create feature matrix\n",
    "    feature_names = list(features[0].keys())\n",
    "    feature_matrix = np.array([[f.get(name, 0) for name in feature_names] for f in features])\n",
    "    \n",
    "    correlations = {}\n",
    "    \n",
    "    for i, feature_name in enumerate(feature_names):\n",
    "        feature_values = feature_matrix[:, i]\n",
    "        \n",
    "        # Skip if no variation\n",
    "        if np.std(feature_values) < 1e-10:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Correlation with class\n",
    "            corr_class, p_class = spearmanr(feature_values, classes)\n",
    "            # Correlation with confidence  \n",
    "            corr_conf, p_conf = spearmanr(feature_values, confidences)\n",
    "            \n",
    "            correlations[feature_name] = {\n",
    "                'class_correlation': corr_class if not np.isnan(corr_class) else 0,\n",
    "                'class_p_value': p_class if not np.isnan(p_class) else 1,\n",
    "                'confidence_correlation': corr_conf if not np.isnan(corr_conf) else 0,\n",
    "                'confidence_p_value': p_conf if not np.isnan(p_conf) else 1\n",
    "            }\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Sort by absolute correlation with class\n",
    "    sorted_correlations = sorted(correlations.items(),\n",
    "                                key=lambda x: abs(x[1]['class_correlation']),\n",
    "                                reverse=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODEL INTERPRETATION RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Top features correlated with model predictions:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, (feature, corr_data) in enumerate(sorted_correlations[:15]):\n",
    "        print(f\"{i+1:2d}. {feature:<30} | Class corr: {corr_data['class_correlation']:6.3f} \"\n",
    "              f\"(p={corr_data['class_p_value']:.4f}) | Conf corr: {corr_data['confidence_correlation']:6.3f}\")\n",
    "    \n",
    "    return sorted_correlations\n",
    "\n",
    "def analyze_feature_distributions(features):\n",
    "    \"\"\"\n",
    "    Analyze feature distributions\n",
    "    \"\"\"\n",
    "    if not features:\n",
    "        return\n",
    "        \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FEATURE DISTRIBUTION ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get all feature names\n",
    "    all_feature_names = set()\n",
    "    for f in features:\n",
    "        all_feature_names.update(f.keys())\n",
    "    \n",
    "    # Calculate statistics\n",
    "    for feature_name in sorted(all_feature_names):\n",
    "        values = [f.get(feature_name, 0) for f in features]\n",
    "        values = np.array(values)\n",
    "        values = values[~np.isnan(values)]  # Remove NaN values\n",
    "        \n",
    "        if len(values) > 0:\n",
    "            print(f\"{feature_name:<25}: Mean={np.mean(values):.3f}, Std={np.std(values):.3f}, \"\n",
    "                  f\"Min={np.min(values):.3f}, Max={np.max(values):.3f}\")\n",
    "\n",
    "# Usage functions\n",
    "def analyze_with_pytorch_model(pt_path, respiratory_signal, timestamps):\n",
    "    \"\"\"Wrapper for PyTorch model analysis\"\"\"\n",
    "    return analyze_with_model(pt_path, respiratory_signal, timestamps, is_pickle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # For your case:\n",
    "    respiratory_signal = respeck_df['breathingSignal'].values\n",
    "    timestamps = respeck_df['timestamp'].values\n",
    "    \n",
    "    # With PyTorch model:\n",
    "    results = analyze_with_pytorch_model('/Users/hkhes/Developer/msc/dissertation/DysfunctionalBreathingCharacterisation/results/cd2046784/final_sleep_apnea_model_20250718_200301.pt', respiratory_signal, timestamps)\n",
    "    \n",
    "    # Features only:\n",
    "    results = analyze_features_only(respiratory_signal, timestamps)\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089ee04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import signal as scipy_signal\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def define_clinical_breath_signatures(features_data, model_predictions=None):\n",
    "    \"\"\"\n",
    "    Define sleep apnea events in terms of clinical breath features\n",
    "    Maps complex model patterns to interpretable clinical signatures\n",
    "    \n",
    "    Args:\n",
    "        features_data: List of feature dictionaries from your analysis\n",
    "        model_predictions: Optional model predictions for correlation analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\" CLINICAL BREATH FEATURE SIGNATURES FOR SLEEP APNEA\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Defining OSA events in terms clinicians understand\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(features_data)\n",
    "    \n",
    "    # Define clinical breath feature signatures\n",
    "    clinical_signatures = define_osa_breath_signatures()\n",
    "    \n",
    "    # Classify each segment using clinical criteria\n",
    "    segment_classifications = []\n",
    "    clinical_scores = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        classification, scores = classify_segment_clinically(row, clinical_signatures)\n",
    "        segment_classifications.append(classification)\n",
    "        clinical_scores.append(scores)\n",
    "    \n",
    "    # Add classifications to dataframe\n",
    "    df['clinical_classification'] = segment_classifications\n",
    "    \n",
    "    # Analyze signature patterns\n",
    "    signature_analysis = analyze_signature_patterns(df, clinical_scores, clinical_signatures)\n",
    "    \n",
    "    # Create clinical decision tree\n",
    "    decision_rules = create_clinical_decision_tree(df, clinical_signatures)\n",
    "    \n",
    "    # Visualize clinical signatures\n",
    "    create_clinical_signature_visualizations(df, clinical_signatures)\n",
    "    \n",
    "    return {\n",
    "        'clinical_signatures': clinical_signatures,\n",
    "        'classifications': segment_classifications,\n",
    "        'signature_analysis': signature_analysis,\n",
    "        'decision_rules': decision_rules,\n",
    "        'clinical_df': df\n",
    "    }\n",
    "\n",
    "def define_osa_breath_signatures():\n",
    "    \"\"\"\n",
    "    Define clinical breath feature signatures for each OSA event type\n",
    "    Based on clinical sleep medicine literature\n",
    "    \"\"\"\n",
    "    \n",
    "    signatures = {\n",
    "        'obstructive_apnea': {\n",
    "            'name': 'Obstructive Apnea',\n",
    "            'clinical_definition': 'Complete cessation of airflow with continued respiratory effort',\n",
    "            'primary_features': {\n",
    "                'amplitude_reduction_ratio': {'min': 0.90, 'max': 1.0},  # >90% amplitude reduction\n",
    "                'effort_flow_mismatch': {'min': 2.0, 'max': np.inf},      # High effort, low flow\n",
    "                'respiratory_rate_cv': {'min': 0.1, 'max': np.inf},       # Irregular due to obstruction\n",
    "                'envelope_cv': {'min': 0.5, 'max': np.inf}                # Variable effort\n",
    "            },\n",
    "            'secondary_features': {\n",
    "                'normal_power_ratio': {'min': 0.0, 'max': 0.6},           # Reduced normal breathing\n",
    "                'hf_power_ratio': {'min': 0.15, 'max': np.inf},           # Increased effort artifacts\n",
    "                'breath_duration_mean': {'min': 15.0, 'max': np.inf},     # Long event duration\n",
    "                'activity_level': {'min': 0.01, 'max': np.inf}            # Increased activity/struggle\n",
    "            },\n",
    "            'weight': 4,  # Highest severity\n",
    "            'duration_threshold': 10.0  # seconds\n",
    "        },\n",
    "        \n",
    "        'hypopnea': {\n",
    "            'name': 'Hypopnea',\n",
    "            'clinical_definition': 'Partial reduction in airflow (30-90%) with arousal or desaturation',\n",
    "            'primary_features': {\n",
    "                'amplitude_reduction_ratio': {'min': 0.30, 'max': 0.90},  # 30-90% reduction\n",
    "                'respiratory_rate_cv': {'min': 0.12, 'max': np.inf},       # Moderate irregularity\n",
    "                'envelope_cv': {'min': 0.35, 'max': 0.7},                  # Some effort variability\n",
    "                'normal_power_ratio': {'min': 0.4, 'max': 0.8}             # Reduced but present\n",
    "            },\n",
    "            'secondary_features': {\n",
    "                'breath_duration_cv': {'min': 0.15, 'max': np.inf},        # Variable durations\n",
    "                'hf_power_ratio': {'min': 0.10, 'max': 0.25},              # Some effort increase\n",
    "                'dominant_frequency': {'min': 0.15, 'max': 0.4},           # Normal-ish frequency\n",
    "                'activity_level': {'min': 0.005, 'max': 0.02}              # Mild activity increase\n",
    "            },\n",
    "            'weight': 2,  # Moderate severity\n",
    "            'duration_threshold': 10.0\n",
    "        },\n",
    "        \n",
    "        'central_apnea': {\n",
    "            'name': 'Central Apnea',\n",
    "            'clinical_definition': 'Cessation of both airflow and respiratory effort',\n",
    "            'primary_features': {\n",
    "                'amplitude_reduction_ratio': {'min': 0.85, 'max': 1.0},    # High amplitude reduction\n",
    "                'activity_level': {'min': 0.0, 'max': 0.008},              # Very low activity\n",
    "                'envelope_cv': {'min': 0.0, 'max': 0.3},                   # Low effort variability\n",
    "                'respiratory_rate_cv': {'min': 0.0, 'max': 0.15}           # Can be regular\n",
    "            },\n",
    "            'secondary_features': {\n",
    "                'normal_power_ratio': {'min': 0.0, 'max': 0.5},            # Very low normal breathing\n",
    "                'hf_power_ratio': {'min': 0.0, 'max': 0.12},               # Low effort artifacts\n",
    "                'rms': {'min': 0.0, 'max': 0.05},                          # Low signal energy\n",
    "                'signal_std': {'min': 0.0, 'max': 0.05}                    # Low variability\n",
    "            },\n",
    "            'weight': 3,  # High severity\n",
    "            'duration_threshold': 10.0\n",
    "        },\n",
    "        \n",
    "        'flow_limitation': {\n",
    "            'name': 'Flow Limitation',\n",
    "            'clinical_definition': 'Flattened inspiratory flow with increased effort',\n",
    "            'primary_features': {\n",
    "                'amplitude_reduction_ratio': {'min': 0.15, 'max': 0.45},   # Mild-moderate reduction\n",
    "                'effort_flow_mismatch': {'min': 1.2, 'max': 3.0},          # Moderate mismatch\n",
    "                'envelope_cv': {'min': 0.4, 'max': 0.8},                   # Effort variability\n",
    "                'respiratory_rate_cv': {'min': 0.08, 'max': 0.25}          # Some irregularity\n",
    "            },\n",
    "            'secondary_features': {\n",
    "                'normal_power_ratio': {'min': 0.6, 'max': 0.85},           # Mostly preserved\n",
    "                'hf_power_ratio': {'min': 0.08, 'max': 0.20},              # Some effort increase\n",
    "                'dominant_frequency': {'min': 0.12, 'max': 0.35},          # Slightly altered\n",
    "                'breath_duration_cv': {'min': 0.10, 'max': 0.30}           # Mild variability\n",
    "            },\n",
    "            'weight': 1,  # Mild severity\n",
    "            'duration_threshold': 5.0\n",
    "        },\n",
    "        \n",
    "        'normal_breathing': {\n",
    "            'name': 'Normal Breathing',\n",
    "            'clinical_definition': 'Regular, unobstructed breathing pattern',\n",
    "            'primary_features': {\n",
    "                'amplitude_reduction_ratio': {'min': 0.0, 'max': 0.25},    # Minimal reduction\n",
    "                'respiratory_rate_cv': {'min': 0.0, 'max': 0.12},          # Regular pattern\n",
    "                'envelope_cv': {'min': 0.1, 'max': 0.4},                   # Consistent effort\n",
    "                'normal_power_ratio': {'min': 0.7, 'max': 1.0}             # High normal breathing\n",
    "            },\n",
    "            'secondary_features': {\n",
    "                'breath_duration_cv': {'min': 0.0, 'max': 0.15},           # Regular durations\n",
    "                'hf_power_ratio': {'min': 0.0, 'max': 0.15},               # Low effort artifacts\n",
    "                'activity_level': {'min': 0.002, 'max': 0.012},            # Normal activity\n",
    "                'dominant_frequency': {'min': 0.15, 'max': 0.35}           # Normal frequency\n",
    "            },\n",
    "            'weight': 0,  # No pathology\n",
    "            'duration_threshold': 0.0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return signatures\n",
    "\n",
    "def classify_segment_clinically(segment_data, clinical_signatures):\n",
    "    \"\"\"\n",
    "    Classify a single segment using clinical breath feature criteria\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = {}\n",
    "    \n",
    "    for event_type, signature in clinical_signatures.items():\n",
    "        primary_score = 0\n",
    "        secondary_score = 0\n",
    "        primary_total = len(signature['primary_features'])\n",
    "        secondary_total = len(signature['secondary_features'])\n",
    "        \n",
    "        # Check primary features\n",
    "        for feature, criteria in signature['primary_features'].items():\n",
    "            if feature in segment_data:\n",
    "                value = segment_data[feature]\n",
    "                if pd.notna(value) and criteria['min'] <= value <= criteria['max']:\n",
    "                    primary_score += 1\n",
    "        \n",
    "        # Check secondary features\n",
    "        for feature, criteria in signature['secondary_features'].items():\n",
    "            if feature in segment_data:\n",
    "                value = segment_data[feature]\n",
    "                if pd.notna(value) and criteria['min'] <= value <= criteria['max']:\n",
    "                    secondary_score += 1\n",
    "        \n",
    "        # Calculate composite score\n",
    "        primary_ratio = primary_score / primary_total if primary_total > 0 else 0\n",
    "        secondary_ratio = secondary_score / secondary_total if secondary_total > 0 else 0\n",
    "        \n",
    "        # Weight primary features more heavily\n",
    "        composite_score = (0.7 * primary_ratio) + (0.3 * secondary_ratio)\n",
    "        \n",
    "        scores[event_type] = {\n",
    "            'primary_score': primary_score,\n",
    "            'secondary_score': secondary_score,\n",
    "            'primary_ratio': primary_ratio,\n",
    "            'secondary_ratio': secondary_ratio,\n",
    "            'composite_score': composite_score,\n",
    "            'weight': signature['weight']\n",
    "        }\n",
    "    \n",
    "    # Find best matching signature\n",
    "    best_match = max(scores.items(), key=lambda x: x[1]['composite_score'])\n",
    "    classification = best_match[0]\n",
    "    \n",
    "    # Require minimum threshold for pathological classifications\n",
    "    min_threshold = 0.4  # At least 40% of criteria must be met\n",
    "    if best_match[1]['composite_score'] < min_threshold and classification != 'normal_breathing':\n",
    "        classification = 'indeterminate'\n",
    "    \n",
    "    return classification, scores\n",
    "\n",
    "def analyze_signature_patterns(df, clinical_scores, clinical_signatures):\n",
    "    \"\"\"\n",
    "    Analyze patterns in clinical signature matching\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n CLINICAL SIGNATURE ANALYSIS:\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # Classification distribution\n",
    "    class_counts = df['clinical_classification'].value_counts()\n",
    "    total_segments = len(df)\n",
    "    \n",
    "    print(f\"Clinical Classification Distribution:\")\n",
    "    for classification, count in class_counts.items():\n",
    "        percentage = (count / total_segments) * 100\n",
    "        signature_name = clinical_signatures.get(classification, {}).get('name', classification.title())\n",
    "        print(f\"  {signature_name:<20}: {count:4d} segments ({percentage:5.1f}%)\")\n",
    "    \n",
    "    # Feature importance for each classification\n",
    "    print(f\"\\n KEY DISCRIMINATING FEATURES BY EVENT TYPE:\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    for event_type, signature in clinical_signatures.items():\n",
    "        if event_type in class_counts and class_counts[event_type] > 5:  # Only analyze if enough samples\n",
    "            event_segments = df[df['clinical_classification'] == event_type]\n",
    "            \n",
    "            print(f\"\\n{signature['name']} (n={len(event_segments)}):\")\n",
    "            print(f\"  Clinical Definition: {signature['clinical_definition']}\")\n",
    "            \n",
    "            # Show primary feature statistics\n",
    "            print(f\"  Primary Features:\")\n",
    "            for feature, criteria in signature['primary_features'].items():\n",
    "                if feature in event_segments.columns:\n",
    "                    values = event_segments[feature].dropna()\n",
    "                    if len(values) > 0:\n",
    "                        mean_val = values.mean()\n",
    "                        std_val = values.std()\n",
    "                        print(f\"    {feature:<25}: ={mean_val:.3f}{std_val:.3f} (criteria: {criteria['min']:.2f}-{criteria['max']:.2f})\")\n",
    "    \n",
    "    # Calculate signature consistency\n",
    "    signature_consistency = {}\n",
    "    for idx, scores in enumerate(clinical_scores):\n",
    "        classification = df.iloc[idx]['clinical_classification']\n",
    "        if classification in scores:\n",
    "            consistency = scores[classification]['composite_score']\n",
    "            if classification not in signature_consistency:\n",
    "                signature_consistency[classification] = []\n",
    "            signature_consistency[classification].append(consistency)\n",
    "    \n",
    "    print(f\"\\n SIGNATURE CONSISTENCY (how well segments match their assigned signature):\")\n",
    "    print(\"-\"*60)\n",
    "    for event_type, consistencies in signature_consistency.items():\n",
    "        if event_type in clinical_signatures:\n",
    "            mean_consistency = np.mean(consistencies)\n",
    "            signature_name = clinical_signatures[event_type]['name']\n",
    "            print(f\"  {signature_name:<20}: {mean_consistency:.3f} (higher = better match)\")\n",
    "    \n",
    "    return {\n",
    "        'class_counts': class_counts,\n",
    "        'signature_consistency': signature_consistency\n",
    "    }\n",
    "\n",
    "def create_clinical_decision_tree(df, clinical_signatures):\n",
    "    \"\"\"\n",
    "    Create interpretable decision rules for clinical classification\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n CLINICAL DECISION TREE RULES:\")\n",
    "    print(\"-\"*60)\n",
    "    print(\"If-then rules for clinicians to identify OSA events:\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    rules = []\n",
    "    \n",
    "    # Rule 1: Obstructive Apnea\n",
    "    rules.append({\n",
    "        'condition': \"Obstructive Apnea\",\n",
    "        'rule': \"IF amplitude_reduction_ratio > 0.90 AND effort_flow_mismatch > 2.0 AND respiratory_rate_cv > 0.1\",\n",
    "        'clinical_meaning': \"Complete airflow cessation with continued respiratory effort\"\n",
    "    })\n",
    "    \n",
    "    # Rule 2: Central Apnea  \n",
    "    rules.append({\n",
    "        'condition': \"Central Apnea\",\n",
    "        'rule': \"IF amplitude_reduction_ratio > 0.85 AND activity_level < 0.008 AND envelope_cv < 0.3\",\n",
    "        'clinical_meaning': \"Both airflow and effort cessation\"\n",
    "    })\n",
    "    \n",
    "    # Rule 3: Hypopnea\n",
    "    rules.append({\n",
    "        'condition': \"Hypopnea\", \n",
    "        'rule': \"IF 0.30 < amplitude_reduction_ratio < 0.90 AND respiratory_rate_cv > 0.12\",\n",
    "        'clinical_meaning': \"Partial airflow reduction with pattern disruption\"\n",
    "    })\n",
    "    \n",
    "    # Rule 4: Flow Limitation\n",
    "    rules.append({\n",
    "        'condition': \"Flow Limitation\",\n",
    "        'rule': \"IF 0.15 < amplitude_reduction_ratio < 0.45 AND effort_flow_mismatch > 1.2\",\n",
    "        'clinical_meaning': \"Increased effort with mild flow reduction\"\n",
    "    })\n",
    "    \n",
    "    # Rule 5: Normal\n",
    "    rules.append({\n",
    "        'condition': \"Normal Breathing\",\n",
    "        'rule': \"IF amplitude_reduction_ratio < 0.25 AND respiratory_rate_cv < 0.12 AND normal_power_ratio > 0.7\",\n",
    "        'clinical_meaning': \"Regular, unobstructed breathing\"\n",
    "    })\n",
    "    \n",
    "    for rule in rules:\n",
    "        print(f\"\\n{rule['condition']}:\")\n",
    "        print(f\"  Rule: {rule['rule']}\")\n",
    "        print(f\"  Meaning: {rule['clinical_meaning']}\")\n",
    "    \n",
    "    # Validate rules against data\n",
    "    print(f\"\\n RULE VALIDATION:\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    for rule in rules:\n",
    "        condition = rule['condition'].lower().replace(' ', '_')\n",
    "        if condition in df['clinical_classification'].values:\n",
    "            matching_segments = len(df[df['clinical_classification'] == condition])\n",
    "            print(f\"{rule['condition']:<15}: {matching_segments:4d} segments match this rule\")\n",
    "    \n",
    "    return rules\n",
    "\n",
    "def create_clinical_signature_visualizations(df, clinical_signatures):\n",
    "    \"\"\"\n",
    "    Create visualizations showing clinical breath feature signatures\n",
    "    \"\"\"\n",
    "    \n",
    "    # Key discriminating features\n",
    "    key_features = [\n",
    "        'amplitude_reduction_ratio', 'effort_flow_mismatch', 'respiratory_rate_cv',\n",
    "        'envelope_cv', 'normal_power_ratio', 'activity_level'\n",
    "    ]\n",
    "    \n",
    "    available_features = [f for f in key_features if f in df.columns]\n",
    "    \n",
    "    if len(available_features) < 3:\n",
    "        print(\"Not enough features available for visualization\")\n",
    "        return None\n",
    "    \n",
    "    # Create signature comparison plot\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Clinical Breath Feature Signatures for OSA Events', fontsize=16)\n",
    "    \n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, feature in enumerate(available_features[:6]):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Plot distributions for each clinical classification\n",
    "        classifications = df['clinical_classification'].unique()\n",
    "        colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown']\n",
    "        \n",
    "        for j, classification in enumerate(classifications):\n",
    "            if classification in clinical_signatures:\n",
    "                class_data = df[df['clinical_classification'] == classification][feature].dropna()\n",
    "                if len(class_data) > 5:  # Only plot if enough data\n",
    "                    signature_name = clinical_signatures[classification]['name']\n",
    "                    ax.hist(class_data, bins=20, alpha=0.6, \n",
    "                           label=f'{signature_name} (n={len(class_data)})', \n",
    "                           color=colors[j % len(colors)], density=True)\n",
    "        \n",
    "        ax.set_xlabel(feature.replace('_', ' ').title())\n",
    "        ax.set_ylabel('Density')\n",
    "        ax.set_title(f'{feature.replace(\"_\", \" \").title()} by Event Type')\n",
    "        ax.legend(fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(available_features), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def calculate_clinical_ahi(classifications, window_duration_sec=30):\n",
    "    \"\"\"\n",
    "    Calculate clinical AHI (Apnea-Hypopnea Index) from classifications\n",
    "    \"\"\"\n",
    "    \n",
    "    # Count apnea and hypopnea events\n",
    "    apnea_events = sum(1 for c in classifications if 'apnea' in c.lower())\n",
    "    hypopnea_events = sum(1 for c in classifications if 'hypopnea' in c.lower())\n",
    "    \n",
    "    total_events = apnea_events + hypopnea_events\n",
    "    total_time_hours = (len(classifications) * window_duration_sec) / 3600\n",
    "    \n",
    "    ahi = total_events / total_time_hours if total_time_hours > 0 else 0\n",
    "    \n",
    "    print(f\"\\n CLINICAL AHI CALCULATION:\")\n",
    "    print(\"-\"*40)\n",
    "    print(f\"Apnea Events: {apnea_events}\")\n",
    "    print(f\"Hypopnea Events: {hypopnea_events}\")\n",
    "    print(f\"Total Events: {total_events}\")\n",
    "    print(f\"Analysis Duration: {total_time_hours:.2f} hours\")\n",
    "    print(f\"Calculated AHI: {ahi:.1f} events/hour\")\n",
    "    \n",
    "    # Clinical severity classification\n",
    "    if ahi < 5:\n",
    "        severity = \"Normal (No OSA)\"\n",
    "    elif ahi < 15:\n",
    "        severity = \"Mild OSA\"\n",
    "    elif ahi < 30:\n",
    "        severity = \"Moderate OSA\"\n",
    "    else:\n",
    "        severity = \"Severe OSA\"\n",
    "    \n",
    "    print(f\"Clinical Severity: {severity}\")\n",
    "    \n",
    "    return {\n",
    "        'ahi': ahi,\n",
    "        'apnea_events': apnea_events,\n",
    "        'hypopnea_events': hypopnea_events,\n",
    "        'total_events': total_events,\n",
    "        'analysis_hours': total_time_hours,\n",
    "        'severity': severity\n",
    "    }\n",
    "\n",
    "# Main analysis function\n",
    "def run_clinical_breath_signature_analysis(features_data):\n",
    "    \"\"\"\n",
    "    Run complete clinical breath signature analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\" RUNNING CLINICAL BREATH SIGNATURE ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Defining OSA events in terms of clinical breath features\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Define clinical signatures and classify segments\n",
    "    results = define_clinical_breath_signatures(features_data)\n",
    "    \n",
    "    # Calculate clinical AHI\n",
    "    ahi_results = calculate_clinical_ahi(results['classifications'])\n",
    "    \n",
    "    # Summary recommendations\n",
    "    print(f\"\\n CLINICAL INTERPRETATION SUMMARY:\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Your sophisticated attention-CNN model likely learned these patterns:\")\n",
    "    \n",
    "    class_counts = results['signature_analysis']['class_counts']\n",
    "    for classification, count in class_counts.items():\n",
    "        if classification in results['clinical_signatures']:\n",
    "            signature = results['clinical_signatures'][classification]\n",
    "            percentage = (count / len(results['classifications'])) * 100\n",
    "            print(f\"\\n{signature['name']} ({percentage:.1f}% of segments):\")\n",
    "            print(f\"  Clinical Definition: {signature['clinical_definition']}\")\n",
    "            print(f\"  Key Pattern: Multi-scale CNN likely detects this through attention on specific frequency/temporal features\")\n",
    "    \n",
    "    results['ahi_results'] = ahi_results\n",
    "    return results\n",
    "\n",
    "# Usage:\n",
    "# Define OSA events in clinical breath feature terms\n",
    "clinical_results = run_clinical_breath_signature_analysis(results['features'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
