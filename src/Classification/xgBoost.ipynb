{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df119221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import pytz\n",
    "import os\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import xgboost\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96248335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 event files. Processing each one...\n",
      "  - Processing session: 26-04-2025\n",
      "  - Preparing and merging engineered features using Unix time intervals...\n",
      "  - Applying precise interval-based labels...\n",
      "  - Processing session: 08-05-2025\n",
      "  - Preparing and merging engineered features using Unix time intervals...\n",
      "  - Applying precise interval-based labels...\n",
      "  - Processing session: 05-04-2025\n",
      "  - Preparing and merging engineered features using Unix time intervals...\n",
      "  - Applying precise interval-based labels...\n",
      "  - Processing session: 10-05-2025\n",
      "  - Preparing and merging engineered features using Unix time intervals...\n",
      "  - Applying precise interval-based labels...\n",
      "  - Processing session: 24-04-2025\n",
      "  - Preparing and merging engineered features using Unix time intervals...\n",
      "  - Applying precise interval-based labels...\n",
      "  - Processing session: 25-04-2025\n",
      "  - Preparing and merging engineered features using Unix time intervals...\n",
      "  - Applying precise interval-based labels...\n",
      "  - Processing session: 16-04-2025\n",
      "  - Preparing and merging engineered features using Unix time intervals...\n",
      "  - Applying precise interval-based labels...\n",
      "  - Processing session: 11-05-2025\n",
      "  - Preparing and merging engineered features using Unix time intervals...\n",
      "  - Applying precise interval-based labels...\n",
      "  - Processing session: 04-04-2025\n",
      "  - Preparing and merging engineered features using Unix time intervals...\n",
      "  - Applying precise interval-based labels...\n",
      "\n",
      "----------------------------------------------------\n",
      "Data loading with PRECISE interval labeling complete.\n",
      "Final DataFrame shape: (2139235, 30)\n",
      "Final class distribution in raw data: \n",
      "Label\n",
      "0    0.988052\n",
      "1    0.011948\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load in Data\n",
    "EVENTS_FOLDER = '../../data/bishkek_csr/03_train_ready/event_exports' \n",
    "RESPECK_FOLDER = '../../data/bishkek_csr/03_train_ready/respeck'\n",
    "NASAL_FOLDER = '../../data/bishkek_csr/03_train_ready/nasal_files'\n",
    "FEATURES_FOLDER = '../../data/bishkek_csr/03_train_ready/respeck_features'\n",
    "# --- Define relevant events ---\n",
    "\n",
    "APNEA_EVENT_LABELS = [\n",
    "    'Obstructive Apnea', \n",
    "]\n",
    "\n",
    "all_sessions_df_list = []\n",
    "event_files = glob.glob(os.path.join(EVENTS_FOLDER, '*_event_export.csv'))\n",
    "\n",
    "if not event_files:\n",
    "    raise FileNotFoundError(f\"No event files found in '{EVENTS_FOLDER}'.\")\n",
    "\n",
    "print(f\"Found {len(event_files)} event files. Processing each one...\")\n",
    "\n",
    "for event_file_path in event_files:\n",
    "    # --- 1. Setup paths and IDs ---\n",
    "    base_name = os.path.basename(event_file_path)\n",
    "    session_id = base_name.split('_event_export.csv')[0]\n",
    "    respeck_file_path = os.path.join(RESPECK_FOLDER, f'{session_id}_respeck.csv')\n",
    "    nasal_file_path = os.path.join(NASAL_FOLDER, f'{session_id}_nasal.csv')\n",
    "    feature_file_path = os.path.join(FEATURES_FOLDER, f'{session_id}_respeck_features.csv')\n",
    "    \n",
    "    if not all(os.path.exists(p) for p in [respeck_file_path, nasal_file_path, feature_file_path]):\n",
    "        print(f\"  - WARNING: Skipping session '{session_id}'. A corresponding file is missing.\")\n",
    "        continue\n",
    "    print(f\"  - Processing session: {session_id}\")\n",
    "    \n",
    "    # --- 2. Load all data sources ---\n",
    "    df_events = pd.read_csv(event_file_path, decimal=',')\n",
    "    df_nasal = pd.read_csv(nasal_file_path)\n",
    "    df_respeck = pd.read_csv(respeck_file_path)\n",
    "    df_features = pd.read_csv(feature_file_path)\n",
    "\n",
    "    # --- 3. Standardize timestamp columns and types ---\n",
    "    df_events.rename(columns={'UnixTimestamp': 'timestamp_unix'}, inplace=True)\n",
    "    df_nasal.rename(columns={'UnixTimestamp': 'timestamp_unix'}, inplace=True, errors='ignore')\n",
    "    df_respeck.rename(columns={'alignedTimestamp': 'timestamp_unix'}, inplace=True)\n",
    "    \n",
    "    df_features['timestamp_unix'] = pd.to_datetime(df_features['startTimestamp'], format=\"mixed\")\n",
    "    df_features['timestamp_unix'] = df_features['timestamp_unix'].astype('int64') // 10**6\n",
    "\n",
    "    df_features['timestamp_unix_end'] = pd.to_datetime(df_features['endTimestamp'], format=\"mixed\")\n",
    "    df_features['timestamp_unix_end'] = df_features['timestamp_unix_end'].astype('int64') // 10**6\n",
    "    \n",
    "    for df_ in [df_events, df_nasal, df_respeck]:\n",
    "        df_['timestamp_unix'] = pd.to_numeric(df_['timestamp_unix'], errors='coerce')\n",
    "        df_.dropna(subset=['timestamp_unix'], inplace=True)\n",
    "        df_['timestamp_unix'] = df_['timestamp_unix'].astype('int64')\n",
    "\n",
    "    # --- 4. Calculate the true overlapping time range ---\n",
    "    start_time = max(df_nasal['timestamp_unix'].min(), df_respeck['timestamp_unix'].min())\n",
    "    end_time = min(df_nasal['timestamp_unix'].max(), df_respeck['timestamp_unix'].max())\n",
    "    \n",
    "    # --- 5. Trim Respeck data to the overlapping time range ---\n",
    "    df_respeck = df_respeck[(df_respeck['timestamp_unix'] >= start_time) & (df_respeck['timestamp_unix'] <= end_time)].copy()\n",
    "\n",
    "    if df_respeck.empty:\n",
    "        print(f\"  - WARNING: Skipping session '{session_id}'. No Respeck data in the overlapping range.\")\n",
    "        continue\n",
    "\n",
    "    print(\"  - Preparing and merging engineered features using Unix time intervals...\")\n",
    "    df_respeck = df_respeck.sort_values('timestamp_unix')\n",
    "    df_features = df_features.sort_values('timestamp_unix')\n",
    "\n",
    "    # Use merge_asof to find the correct feature window for each respeck data point\n",
    "    df_session_merged = pd.merge_asof(\n",
    "        df_respeck,\n",
    "        df_features,\n",
    "        on='timestamp_unix',\n",
    "        direction='backward' # Finds the last feature window that started <= the respeck timestamp\n",
    "    )\n",
    "\n",
    "    cols_to_drop = ['Unnamed: 0','startTimestamp', 'endTimestamp', 'timestamp_unix_end']\n",
    "    df_session_merged.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "    if df_session_merged.empty:\n",
    "        print(f\"  - WARNING: Skipping session '{session_id}'. No merge matches found.\")\n",
    "        continue\n",
    "        \n",
    "    # --- 6. **NEW: Precise Interval-Based Labeling using Duration** ---\n",
    "    print(f\"  - Applying precise interval-based labels...\")\n",
    "    \n",
    "    # ** Step 6a: Initialize the label column in the respeck data with 0 (Normal)\n",
    "    df_session_merged['Label'] = 0\n",
    "    \n",
    "    # ** Step 6b: Calculate event end times using the 'Duration' column\n",
    "    # The 'Duration' column has commas, which we handled with `decimal=','` at load time.\n",
    "    # Convert duration from seconds to milliseconds to match the Unix timestamps.\n",
    "    df_events['Duration_ms'] = (df_events['Duration'] * 1000).astype('int64')\n",
    "    df_events['end_time_unix'] = df_events['timestamp_unix'] + df_events['Duration_ms']\n",
    "    \n",
    "    # ** Step 6c: Filter for only the apnea/hypopnea events we want to label as '1'\n",
    "    df_apnea_events = df_events[df_events['Event'].isin(APNEA_EVENT_LABELS)].copy()\n",
    "\n",
    "    # ** Step 6d: Efficiently label the respeck data using event intervals\n",
    "    # This is much faster than looping. It checks which respeck timestamps fall\n",
    "    # within any of the [start, end] intervals of the apnea events.\n",
    "    for index, event in df_apnea_events.iterrows():\n",
    "        start_event = event['timestamp_unix']\n",
    "        end_event = event['end_time_unix']\n",
    "        # Set the 'Label' to 1 for all respeck rows within this event's time interval\n",
    "        df_session_merged.loc[df_session_merged['timestamp_unix'].between(start_event, end_event), 'Label'] = 1\n",
    "\n",
    "    # --- 7. Finalize session data ---\n",
    "    df_session_merged['SessionID'] = session_id\n",
    "    all_sessions_df_list.append(df_session_merged)\n",
    "\n",
    "# --- Combine all nights and perform final processing ---\n",
    "if not all_sessions_df_list:\n",
    "    raise ValueError(\"Processing failed. No data was loaded.\")\n",
    "\n",
    "df = pd.concat(all_sessions_df_list, ignore_index=True)\n",
    "\n",
    "\n",
    "df.to_csv('test.csv')\n",
    "print(\"\\n----------------------------------------------------\")\n",
    "print(\"Data loading with PRECISE interval labeling complete.\")\n",
    "print(f\"Final DataFrame shape: {df.shape}\")\n",
    "print(f\"Final class distribution in raw data: \\n{df['Label'].value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8ea2c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['timestamp_unix', 'timestamp', 'interpolatedPhoneTimestamp',\n",
      "       'respeckTimestamp', 'sequenceNumber', 'x', 'y', 'z', 'breathingSignal',\n",
      "       'breathingRate', 'activityLevel', 'activityType', 'type', 'area',\n",
      "       'extremas', 'meanActivityLevel', 'modeActivityType',\n",
      "       'peakRespiratoryFlow', 'duration', 'BR_md', 'BR_mean', 'BR_std',\n",
      "       'AL_md', 'AL_mean', 'AL_std', 'RRV', 'RRV3MA', 'breath_regularity',\n",
      "       'Label', 'SessionID'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Windowing: Creating the time-series segments.\n",
    "print(df.columns)\n",
    "\n",
    "SAMPLING_RATE_HZ = 12.5\n",
    "WINDOW_DURATION_SEC = 30\n",
    "WINDOW_SIZE = int(WINDOW_DURATION_SEC * SAMPLING_RATE_HZ)\n",
    "\n",
    "# Step size for sliding window. An 80% overlap is a good start.\n",
    "OVERLAP_PERCENTAGE = 0.80\n",
    "STEP_SIZE = int(WINDOW_SIZE * (1 - OVERLAP_PERCENTAGE))\n",
    "\n",
    "# === Data Parameters ===\n",
    "FEATURE_COLUMNS = [\n",
    "    'breathingSignal', \n",
    "    'activityLevel',\n",
    "    'peakRespiratoryFlow', \n",
    "    'area',\n",
    "]\n",
    "LABEL_COLUMN = 'Label' \n",
    "SESSION_ID_COLUMN = 'SessionID'\n",
    "\n",
    "\n",
    "TEST_NIGHTS = 2\n",
    "TOTAL_NIGHTS = 9 \n",
    "TEST_SIZE = TEST_NIGHTS / TOTAL_NIGHTS\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4e53241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for and imputing missing values (NaNs)...\n",
      "  - Found 4630 NaNs in 'breathingSignal'. Applying forward-fill and backward-fill.\n",
      "  - Found 2097749 NaNs in 'breathingRate'. Applying forward-fill and backward-fill.\n",
      "  - Found 4688 NaNs in 'type'. Applying forward-fill and backward-fill.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qj/1f20_52j2yj49hjhy3ht9wzw0000gn/T/ipykernel_66070/370400962.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].ffill(inplace=True)\n",
      "/var/folders/qj/1f20_52j2yj49hjhy3ht9wzw0000gn/T/ipykernel_66070/370400962.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].bfill(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Found 4688 NaNs in 'area'. Applying forward-fill and backward-fill.\n",
      "  - Found 4688 NaNs in 'extremas'. Applying forward-fill and backward-fill.\n",
      "  - Found 4688 NaNs in 'meanActivityLevel'. Applying forward-fill and backward-fill.\n",
      "  - Found 4688 NaNs in 'modeActivityType'. Applying forward-fill and backward-fill.\n",
      "  - Found 4688 NaNs in 'peakRespiratoryFlow'. Applying forward-fill and backward-fill.\n",
      "  - Found 4688 NaNs in 'duration'. Applying forward-fill and backward-fill.\n",
      "  - Found 7179 NaNs in 'BR_md'. Applying forward-fill and backward-fill.\n",
      "  - Found 7179 NaNs in 'BR_mean'. Applying forward-fill and backward-fill.\n",
      "  - Found 22762 NaNs in 'BR_std'. Applying forward-fill and backward-fill.\n",
      "  - Found 4688 NaNs in 'AL_md'. Applying forward-fill and backward-fill.\n",
      "  - Found 4688 NaNs in 'AL_mean'. Applying forward-fill and backward-fill.\n",
      "  - Found 4688 NaNs in 'AL_std'. Applying forward-fill and backward-fill.\n",
      "  - Found 4688 NaNs in 'RRV'. Applying forward-fill and backward-fill.\n",
      "  - Found 7378 NaNs in 'RRV3MA'. Applying forward-fill and backward-fill.\n",
      "  - Found 7179 NaNs in 'breath_regularity'. Applying forward-fill and backward-fill.\n",
      "\n",
      "Imputation complete. No NaNs remain in feature columns.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nChecking for and imputing missing values (NaNs)...\")\n",
    "for col in df:\n",
    "    if col in df.columns:\n",
    "        nan_count = df[col].isnull().sum()\n",
    "        if nan_count > 0:\n",
    "            print(f\"  - Found {nan_count} NaNs in '{col}'. Applying forward-fill and backward-fill.\")\n",
    "            \n",
    "            # Step 1: Forward-fill handles all NaNs except leading ones.\n",
    "            df[col].ffill(inplace=True) \n",
    "            \n",
    "            # Step 2: Backward-fill handles any remaining NaNs at the beginning of the file.\n",
    "            df[col].bfill(inplace=True) \n",
    "\n",
    "# Add a final check to ensure everything is clean\n",
    "final_nan_count = df[FEATURE_COLUMNS].isnull().sum().sum()\n",
    "if final_nan_count > 0:\n",
    "    print(f\"\\nWARNING: {final_nan_count} NaNs still remain in feature columns after imputation. Please investigate.\")\n",
    "else:\n",
    "    print(\"\\nImputation complete. No NaNs remain in feature columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12ed1b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the windowing process...\n",
      "\n",
      "Data windowing complete.\n",
      "----------------------------------------------------\n",
      "Shape of X (features): (28868, 375, 4) -> (Num_Windows, Window_Size, Num_Features)\n",
      "Shape of y (labels):   (28868,)\n",
      "Shape of groups (IDs): (28868,)\n",
      "Final class distribution across all windows: Counter({np.int64(0): 27958, np.int64(1): 910}) (0=Normal, 1=Apnea)\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "groups = [] \n",
    "\n",
    "print(\"Starting the windowing process...\")\n",
    "\n",
    "# --- 3. Loop through each session (night) to create windows ---\n",
    "# We group by SessionID to ensure windows do not cross over between nights.\n",
    "for session_id, session_df in df.groupby(SESSION_ID_COLUMN):\n",
    "    for i in range(0, len(session_df) - WINDOW_SIZE, STEP_SIZE):\n",
    "        \n",
    "        window_df = session_df.iloc[i : i + WINDOW_SIZE]\n",
    "        \n",
    "        features = window_df[FEATURE_COLUMNS].values\n",
    "        \n",
    "        # --- CORRECTED LABELING LOGIC ---\n",
    "        # The 'Label' column already contains 0s and 1s.\n",
    "        # If the sum of labels in the window is > 0, it means there's at least one '1' (Apnea).\n",
    "        if window_df[LABEL_COLUMN].sum() > 0:\n",
    "            label = 1 # Apnea\n",
    "        else:\n",
    "            label = 0 # Normal\n",
    "        # ------------------------------------\n",
    "            \n",
    "        X.append(features)\n",
    "        y.append(label)\n",
    "        groups.append(session_id)\n",
    "\n",
    "# --- 4. Convert the lists into efficient NumPy arrays ---\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)\n",
    "groups = np.asarray(groups)\n",
    "\n",
    "# --- 5. Print a summary of the results ---\n",
    "print(\"\\nData windowing complete.\")\n",
    "print(\"----------------------------------------------------\")\n",
    "print(f\"Shape of X (features): {X.shape} -> (Num_Windows, Window_Size, Num_Features)\")\n",
    "print(f\"Shape of y (labels):   {y.shape}\")\n",
    "print(f\"Shape of groups (IDs): {groups.shape}\")\n",
    "print(f\"Final class distribution across all windows: {Counter(y)} (0=Normal, 1=Apnea)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "475c0f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 unique sessions (nights) in the dataset: ['04-04-2025' '05-04-2025' '08-05-2025' '10-05-2025' '11-05-2025'\n",
      " '16-04-2025' '24-04-2025' '25-04-2025' '26-04-2025']\n",
      "\n",
      "Splitting data into training and testing sets...\n",
      "  - Sessions assigned to TRAINING set: ['16-04-2025' '04-04-2025' '26-04-2025' '08-05-2025' '11-05-2025'\n",
      " '10-05-2025' '24-04-2025']\n",
      "  - Sessions assigned to TESTING set:  ['25-04-2025' '05-04-2025']\n",
      "\n",
      "Train-test split complete.\n",
      "----------------------------------------------------\n",
      "Total windows in training set:   21374\n",
      "Total windows in testing set:    7494\n",
      "Shape of X_train:                (21374, 375, 4)\n",
      "Shape of X_test:                 (7494, 375, 4)\n",
      "Training set class distribution: Counter({np.int64(0): 20839, np.int64(1): 535}) (0=Normal, 1=Apnea)\n",
      "Testing set class distribution:  Counter({np.int64(0): 7119, np.int64(1): 375}) (0=Normal, 1=Apnea)\n"
     ]
    }
   ],
   "source": [
    "# Splitting dataset\n",
    "\n",
    "unique_session_ids = np.unique(groups)\n",
    "n_total_sessions = len(unique_session_ids)\n",
    "\n",
    "print(f\"Found {n_total_sessions} unique sessions (nights) in the dataset: {unique_session_ids}\")\n",
    "\n",
    "train_ids, test_ids = train_test_split(\n",
    "    unique_session_ids, \n",
    "    test_size=TEST_NIGHTS, \n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "print(f\"\\nSplitting data into training and testing sets...\")\n",
    "print(f\"  - Sessions assigned to TRAINING set: {train_ids}\")\n",
    "print(f\"  - Sessions assigned to TESTING set:  {test_ids}\")\n",
    "\n",
    "train_mask = np.isin(groups, train_ids)\n",
    "test_mask = np.isin(groups, test_ids)\n",
    "\n",
    "# --- 4. Apply the masks to create the final data sets ---\n",
    "X_train, y_train = X[train_mask], y[train_mask]\n",
    "X_test, y_test = X[test_mask], y[test_mask]\n",
    "\n",
    "# --- 5. Verify the results ---\n",
    "print(\"\\nTrain-test split complete.\")\n",
    "print(\"----------------------------------------------------\")\n",
    "print(f\"Total windows in training set:   {len(X_train)}\")\n",
    "print(f\"Total windows in testing set:    {len(X_test)}\")\n",
    "print(f\"Shape of X_train:                {X_train.shape}\")\n",
    "print(f\"Shape of X_test:                 {X_test.shape}\")\n",
    "print(f\"Training set class distribution: {Counter(y_train)} (0=Normal, 1=Apnea)\")\n",
    "print(f\"Testing set class distribution:  {Counter(y_test)} (0=Normal, 1=Apnea)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "768d4e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nsamples, n_timesteps, n_features = X_train.shape\n",
    "# X_train_reshaped = X_train.reshape((nsamples, n_timesteps * n_features))\n",
    "\n",
    "# print(\"Balancing the training data using SMOTE...\")\n",
    "# print(f\"  - Original training distribution: {Counter(y_train)}\")\n",
    "\n",
    "# # --- 2. Initialize and apply SMOTE ---\n",
    "# # `random_state` ensures that the synthetic samples are the same each time you run the code.\n",
    "# smote = SMOTE(random_state=RANDOM_STATE)\n",
    "# X_train_resampled, y_train_resampled = smote.fit_resample(X_train_reshaped, y_train)\n",
    "\n",
    "# print(f\"  - Resampled training distribution: {Counter(y_train_resampled)}\")\n",
    "\n",
    "# # --- 3. Reshape the balanced training data back to its original 3D format ---\n",
    "# # The model expects the data in the format (samples, timesteps, features).\n",
    "# X_train_resampled = X_train_resampled.reshape((X_train_resampled.shape[0], n_timesteps, n_features))\n",
    "\n",
    "# X_train_tensor = torch.from_numpy(X_train_resampled).float()\n",
    "# y_train_tensor = torch.from_numpy(y_train_resampled).long() # Use .long() for class indices\n",
    "\n",
    "# X_test_tensor = torch.from_numpy(X_test).float()\n",
    "# y_test_tensor = torch.from_numpy(y_test).long()\n",
    "\n",
    "# # Create TensorDatasets\n",
    "# train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "# test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# # Create DataLoaders to handle batching and shuffling\n",
    "# # BATCH_SIZE is from your configuration cell\n",
    "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# print(\"\\nPyTorch DataLoaders created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be772b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    ")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cfcc7432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting XGBoost Leave-One-Night-Out cross-validation with 9 folds...\n",
      "----------------------------------------------------\\n\n",
      "--- FOLD 1/9 ---\n",
      "Testing on Night: 04-04-2025\\n\n",
      "  - Data flattened for XGBoost. Train shape: (25317, 1500)\n",
      "  - Original training distribution: Counter({np.int64(0): 24608, np.int64(1): 709})\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "SMOTE.__init__() got an unexpected keyword argument 's_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# --- 6. Balance the TRAINING data for this fold using SMOTE ---\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  - Original training distribution: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCounter(y_train)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m smote = \u001b[43mSMOTE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRANDOM_STATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.33\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m X_train_resampled, y_train_resampled = smote.fit_resample(X_train_flattened, y_train)\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  - Resampled training distribution: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCounter(y_train_resampled)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: SMOTE.__init__() got an unexpected keyword argument 's_strategy'"
     ]
    }
   ],
   "source": [
    "# In the final cell (id: d9903739)\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Initialize lists to store results from all folds ---\n",
    "all_fold_predictions = []\n",
    "all_fold_true_labels = []\n",
    "\n",
    "# --- 2. Setup Leave-One-Night-Out cross-validator ---\n",
    "logo = LeaveOneGroupOut()\n",
    "n_folds = logo.get_n_splits(groups=groups)\n",
    "print(f\"Starting XGBoost Leave-One-Night-Out cross-validation with {n_folds} folds...\")\n",
    "print(\"----------------------------------------------------\\\\n\")\n",
    "\n",
    "# --- 3. Loop through each fold ---\n",
    "for fold, (train_idx, test_idx) in enumerate(logo.split(X, y, groups)):\n",
    "    \n",
    "    test_night = np.unique(groups[test_idx])[0]\n",
    "    print(f\"--- FOLD {fold + 1}/{n_folds} ---\")\n",
    "    print(f\"Testing on Night: {test_night}\\\\n\")\n",
    "\n",
    "    # --- 4. Split the data for this fold ---\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # --- 5. FLATTEN THE DATA for XGBoost ---\n",
    "    # Convert the 3D windowed data (samples, timesteps, features) to 2D\n",
    "    n_timesteps, n_features = X_train.shape[1], X_train.shape[2]\n",
    "    X_train_flattened = X_train.reshape(-1, n_timesteps * n_features)\n",
    "    X_test_flattened = X_test.reshape(-1, n_timesteps * n_features)\n",
    "    print(f\"  - Data flattened for XGBoost. Train shape: {X_train_flattened.shape}\")\n",
    "    \n",
    "    # --- 6. Balance the TRAINING data for this fold using SMOTE ---\n",
    "    print(f\"  - Original training distribution: {Counter(y_train)}\")\n",
    "    smote = SMOTE(random_state=RANDOM_STATE, s_strategy=0.33)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_flattened, y_train)\n",
    "    print(f\"  - Resampled training distribution: {Counter(y_train_resampled)}\")\n",
    "\n",
    "    # # pos_weight = Counter(y_train)[0] / Counter(y_train)[1]\n",
    "    # print(f\"  - Calculated scale_pos_weight: {pos_weight:.2f}\")\n",
    "\n",
    "    # --- 7. Initialize and Train a NEW XGBoost model for this fold ---\n",
    "    print(\"  - Training XGBoost model...\")\n",
    "    # It's crucial to re-initialize the model for each fold.\n",
    "    model = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        # scale_pos_weight=pos_weight,\n",
    "        n_estimators=300,         # Number of trees (can be tuned)\n",
    "        learning_rate=0.001,\n",
    "        max_depth=5,              # Max depth of a tree (can be tuned)\n",
    "        eval_metric='logloss',\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1                 # Use all available CPU cores for faster training\n",
    "    )\n",
    "    \n",
    "    # Train the model on the balanced data\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    print(f\"  - Training complete.\")\n",
    "            \n",
    "    # --- 8. Evaluate the fold and store results ---\n",
    "    fold_preds = model.predict(X_test_flattened)\n",
    "    \n",
    "    all_fold_predictions.extend(fold_preds)\n",
    "    all_fold_true_labels.extend(y_test)\n",
    "    print(f\"  - Evaluation complete for fold {fold + 1}.\\\\n\")\n",
    "\n",
    "\n",
    "# --- FINAL AGGREGATED EVALUATION (after all folds are complete) ---\n",
    "print(\"\\\\n====================================================\")\n",
    "print(\"XGBoost Leave-One-Night-Out Cross-Validation Complete.\")\n",
    "print(\"Aggregated Results Across All Folds:\")\n",
    "print(\"====================================================\")\n",
    "\n",
    "# --- Final Classification Report ---\n",
    "print('\\\\nAggregated Classification Report')\n",
    "print('------------------------------')\n",
    "class_names = ['Normal (0)', 'Apnea (1)']\n",
    "print(classification_report(all_fold_true_labels, all_fold_predictions, target_names=class_names))\n",
    "\n",
    "# --- Final Confusion Matrix ---\n",
    "print('Aggregated Confusion Matrix')\n",
    "print('---------------------------')\n",
    "cm = confusion_matrix(all_fold_true_labels, all_fold_predictions)\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm_norm, annot=True, fmt='.2%', cmap='Greens', # Different color for clarity\n",
    "    xticklabels=class_names, yticklabels=class_names\n",
    ")\n",
    "plt.title('XGBoost - Aggregated Normalized Confusion Matrix (LONO)')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5664a88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f99099",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
