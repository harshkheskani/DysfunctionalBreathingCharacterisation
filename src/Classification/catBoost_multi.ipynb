{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df119221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/diss/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import pytz\n",
    "import os\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.model_selection import LeaveOneGroupOut, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, balanced_accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.combine import SMOTETomek\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "import shap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='xgboost')\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96248335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading and feature generation complete. Final shape: (7796, 13)\n",
      "Final class distribution:\n",
      "Label\n",
      "0    0.908415\n",
      "4    0.061570\n",
      "2    0.026552\n",
      "1    0.003335\n",
      "3    0.000128\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Total classes for model: 5. Names: ['Normal', 'Obstructive Apnea', 'Hypopnea Events', 'Other Apnea', 'Desaturation']\n"
     ]
    }
   ],
   "source": [
    "# Load in Data\n",
    "EVENTS_FOLDER = '../../data/bishkek_csr/03_train_ready/event_exports' \n",
    "RESPECK_FOLDER = '../../data/bishkek_csr/03_train_ready/respeck'\n",
    "NASAL_FOLDER = '../../data/bishkek_csr/03_train_ready/nasal_files'\n",
    "\n",
    "# --- STRATEGIC FIX: Re-group the rarest classes ---\n",
    "# We combine Central/Mixed Apnea and RERA into a single \"Other Events\" class\n",
    "# This ensures every class has enough samples for robust modeling.\n",
    "EVENT_GROUP_TO_LABEL = {\n",
    "    1: ['Obstructive Apnea'],\n",
    "    2: ['Hypopnea', 'Central Hypopnea', 'Obstructive Hypopnea'],\n",
    "    3: ['Central Apnea', 'Mixed Apnea'], # Combined rare events\n",
    "    4: ['Desaturation'] # Note: Relabeled from 5 to 4\n",
    "}\n",
    "\n",
    "LABEL_TO_EVENT_GROUP_NAME = {\n",
    "    0: 'Normal',\n",
    "    1: 'Obstructive Apnea',\n",
    "    2: 'Hypopnea Events',\n",
    "    3: 'Other Apnea', # New combined name\n",
    "    4: 'Desaturation'\n",
    "}\n",
    "\n",
    "# --- Feature Generation Functions (Unchanged) ---\n",
    "def generate_RRV(sliced_signal):\n",
    "    sliced_signal = sliced_signal.dropna()\n",
    "    if sliced_signal.empty: return np.nan\n",
    "    breathingSignal = sliced_signal.values\n",
    "    N = len(breathingSignal)\n",
    "    if N < 2: return 0.0\n",
    "    yf = np.fft.fft(breathingSignal)\n",
    "    yff = 2.0 / N * np.abs(yf[:N//2])\n",
    "    if len(yff) < 2: return 0.0\n",
    "    dc_component_value = np.amax(yff)\n",
    "    if dc_component_value == 0: return 0.0\n",
    "    temp_dc_removed = np.delete(yff, np.argmax(yff))\n",
    "    h1_value = np.amax(temp_dc_removed)\n",
    "    rrv = 100 - (h1_value / dc_component_value) * 100\n",
    "    return rrv\n",
    "\n",
    "def extract_respeck_features(df):\n",
    "    resampled_df = pd.DataFrame()\n",
    "    br_resampler = df[\"breathingRate\"].resample('30s')\n",
    "    resampled_df[\"BR_median\"], resampled_df[\"BR_mean\"], resampled_df[\"BR_std\"] = br_resampler.median(), br_resampler.mean(), br_resampler.std()\n",
    "    resampled_df[\"BR_cov\"] = resampled_df[\"BR_std\"] / resampled_df[\"BR_mean\"]\n",
    "    al_resampler = df[\"activityLevel\"].resample('30s')\n",
    "    resampled_df[\"AL_median\"], resampled_df[\"AL_mean\"], resampled_df[\"AL_std\"] = al_resampler.median(), al_resampler.mean(), al_resampler.std()\n",
    "    resampled_df[\"AL_cov\"] = resampled_df[\"AL_std\"] / resampled_df[\"AL_mean\"]\n",
    "    resampled_df[\"RRV\"] = df[\"breathingSignal\"].resample('30s').apply(generate_RRV)\n",
    "    resampled_df[\"RRV\"] = resampled_df[\"RRV\"].replace(0, np.nan).ffill().bfill()\n",
    "    resampled_df[\"RRV3ANN\"] = resampled_df[\"RRV\"].rolling(window=3, center=True).mean()\n",
    "    resampled_df[\"RRV3ANN\"] = resampled_df[\"RRV3ANN\"] * 0.65\n",
    "    resampled_df['Label'] = df['Label'].resample('30s').apply(lambda x: stats.mode(x)[0] if not x.empty else 0)\n",
    "    return resampled_df\n",
    "\n",
    "# --- Data Preparation Loop ---\n",
    "all_sessions_df_list = []\n",
    "event_files = glob.glob(os.path.join(EVENTS_FOLDER, '*_event_export.csv'))\n",
    "for event_file_path in event_files:\n",
    "    base_name = os.path.basename(event_file_path)\n",
    "    session_id = base_name.split('_event_export.csv')[0]\n",
    "    respeck_file_path = os.path.join(RESPECK_FOLDER, f'{session_id}_respeck.csv')\n",
    "    if not os.path.exists(respeck_file_path): continue\n",
    "    \n",
    "    df_events = pd.read_csv(event_file_path, decimal=',')\n",
    "    df_respeck = pd.read_csv(respeck_file_path)\n",
    "    \n",
    "    df_events.rename(columns={'UnixTimestamp': 'timestamp_unix'}, inplace=True)\n",
    "    df_respeck.rename(columns={'alignedTimestamp': 'timestamp_unix'}, inplace=True)\n",
    "    for df_ in [df_events, df_respeck]:\n",
    "        df_['timestamp_unix'] = pd.to_numeric(df_['timestamp_unix'], errors='coerce')\n",
    "        df_.dropna(subset=['timestamp_unix'], inplace=True)\n",
    "        df_['timestamp_unix'] = df_['timestamp_unix'].astype('int64')\n",
    "\n",
    "    df_respeck['timestamp'] = pd.to_datetime(df_respeck['timestamp_unix'], unit='ms').dt.tz_localize('UTC').dt.tz_convert('Asia/Bishkek')\n",
    "    df_respeck['Label'] = 0\n",
    "    df_events['Duration_ms'] = (df_events['Duration'] * 1000).astype('int64')\n",
    "    df_events['end_time_unix'] = df_events['timestamp_unix'] + df_events['Duration_ms']\n",
    "    \n",
    "    for label_id, event_names_in_group in EVENT_GROUP_TO_LABEL.items():\n",
    "        df_filtered_events = df_events[df_events['Event'].isin(event_names_in_group)]\n",
    "        for _, event in df_filtered_events.iterrows():\n",
    "            df_respeck.loc[df_respeck['timestamp_unix'].between(event['timestamp_unix'], event['end_time_unix']), 'Label'] = label_id\n",
    "    \n",
    "    df_respeck.set_index('timestamp', inplace=True)\n",
    "    features_df = extract_respeck_features(df_respeck).dropna()\n",
    "    features_df['SessionID'] = session_id\n",
    "    all_sessions_df_list.append(features_df)\n",
    "\n",
    "final_df = pd.concat(all_sessions_df_list).reset_index().rename(columns={'index': 'timestamp'})\n",
    "print(f\"Data loading and feature generation complete. Final shape: {final_df.shape}\")\n",
    "print(f\"Final class distribution:\\n{final_df['Label'].value_counts(normalize=True)}\")\n",
    "\n",
    "# --- Prepare data for the model ---\n",
    "FEATURE_COLUMNS = ['BR_median', 'BR_mean', 'BR_std', 'BR_cov', 'AL_median', 'AL_mean', 'AL_std', 'AL_cov', 'RRV', 'RRV3ANN']\n",
    "X = final_df[FEATURE_COLUMNS].values\n",
    "y = final_df['Label'].values\n",
    "groups = final_df['SessionID'].values\n",
    "X = SimpleImputer(strategy='mean').fit_transform(X)\n",
    "\n",
    "# --- Use LabelEncoder for robust label handling ---\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "N_OUTPUTS = len(le.classes_)\n",
    "CLASS_NAMES = [LABEL_TO_EVENT_GROUP_NAME[c] for c in le.classes_]\n",
    "\n",
    "print(f\"\\nTotal classes for model: {N_OUTPUTS}. Names: {CLASS_NAMES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37189021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make sure you have these imports if they are not global\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "\n",
    "class ImprovedCatBoostClassifier:\n",
    "    def __init__(self, feature_columns, n_outputs, random_state=42):\n",
    "        self.feature_columns = feature_columns\n",
    "        self.n_outputs = n_outputs\n",
    "        self.random_state = random_state\n",
    "        self.scaler = RobustScaler()\n",
    "        self.best_params = None\n",
    "        self.model = None\n",
    "\n",
    "    def create_advanced_features(self, X_df):\n",
    "        X_advanced = X_df.copy()\n",
    "        X_advanced['BR_AL_interaction'] = X_advanced['BR_mean'] * X_advanced['AL_mean']\n",
    "        X_advanced['BR_RRV_interaction'] = X_advanced['BR_mean'] * X_advanced['RRV']\n",
    "        X_advanced['BR_median_mean_ratio'] = X_advanced['BR_median'] / (X_advanced['BR_mean'] + 1e-8)\n",
    "        X_advanced['BR_mean_squared'] = X_advanced['BR_mean'] ** 2\n",
    "        X_advanced['AL_mean_squared'] = X_advanced['AL_mean'] ** 2\n",
    "        X_advanced['RRV_squared'] = X_advanced['RRV'] ** 2\n",
    "        X_advanced['BR_stability'] = 1 / (1 + X_advanced['BR_cov'])\n",
    "        X_advanced['activity_breathing_score'] = (X_advanced['AL_mean'] * 0.6 + X_advanced['BR_mean'] * 0.4)\n",
    "        X_advanced = X_advanced.replace([np.inf, -np.inf], np.nan).fillna(X_advanced.median())\n",
    "        return X_advanced\n",
    "\n",
    "    def robust_resampling(self, X_train, y_train):\n",
    "        print(f\"  - Original distribution: {Counter(y_train)}\")\n",
    "        class_counts = Counter(y_train)\n",
    "        min_samples = min(class_counts.values()) if class_counts else 0\n",
    "\n",
    "        if min_samples >= 6:\n",
    "            k = min(5, min_samples - 1)\n",
    "            sampler = SMOTE(random_state=self.random_state, k_neighbors=k)\n",
    "            print(f\"  - Using SMOTE with k_neighbors={k}\")\n",
    "        else:\n",
    "            sampler = RandomOverSampler(random_state=self.random_state)\n",
    "            print(\"  - Using RandomOverSampler due to small class size.\")\n",
    "        \n",
    "        X_res, y_res = sampler.fit_resample(X_train, y_train)\n",
    "        print(f\"  - After resampling: {Counter(y_res)}\")\n",
    "        return X_res, y_res\n",
    "\n",
    "    def optimize_hyperparameters(self, X_train_scaled, y_train_encoded, n_trials=50):\n",
    "        print(\"  - Optimizing CatBoost hyperparameters with Optuna...\")\n",
    "        X_train_resampled, y_train_resampled = self.robust_resampling(X_train_scaled, y_train_encoded)\n",
    "        \n",
    "        optuna_le = LabelEncoder()\n",
    "        y_train_optuna = optuna_le.fit_transform(y_train_resampled)\n",
    "        \n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'objective': 'MultiClass',\n",
    "                'eval_metric': 'TotalF1',  # FIX: Use a multi-class compatible metric\n",
    "                'random_seed': self.random_state,\n",
    "                'verbose': 0,\n",
    "                'iterations': trial.suggest_int('iterations', 200, 1500, step=100),\n",
    "                'depth': trial.suggest_int('depth', 4, 10),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True),\n",
    "                'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                'bootstrap_type': 'Bernoulli'\n",
    "            }\n",
    "            skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=self.random_state)\n",
    "            scores = []\n",
    "            for tr_idx, val_idx in skf.split(X_train_resampled, y_train_optuna):\n",
    "                X_tr, X_val = X_train_resampled[tr_idx], X_train_resampled[val_idx]\n",
    "                y_tr, y_val = y_train_optuna[tr_idx], y_train_optuna[val_idx]\n",
    "\n",
    "                model = CatBoostClassifier(**params)\n",
    "                model.fit(X_tr, y_tr)\n",
    "                y_pred = model.predict(X_val)\n",
    "                scores.append(balanced_accuracy_score(y_val, y_pred))\n",
    "            return np.mean(scores)\n",
    "        \n",
    "        study = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner())\n",
    "        study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
    "        \n",
    "        self.best_params = study.best_params\n",
    "        print(f\"  - Best balanced accuracy from optimization: {study.best_value:.4f}\")\n",
    "        return self.best_params\n",
    "\n",
    "    def cross_validate(self, X, y_encoded, le, groups, use_optimization=True):\n",
    "        X_df = pd.DataFrame(X, columns=self.feature_columns)\n",
    "        X_engineered = self.create_advanced_features(X_df)\n",
    "        print(f\"  - Feature engineering applied: {X.shape[1]} -> {X_engineered.shape[1]} features\")\n",
    "\n",
    "        all_preds_encoded, all_true_encoded, all_importances = [], [], []\n",
    "        logo = LeaveOneGroupOut()\n",
    "        n_folds = logo.get_n_splits(X_engineered, y_encoded, groups=groups)\n",
    "        \n",
    "        print(f\"Starting LONO CV with {n_folds} folds...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for fold, (train_idx, test_idx) in enumerate(logo.split(X_engineered, y_encoded, groups)):\n",
    "            test_night = np.unique(groups[test_idx])[0]\n",
    "            print(f\"\\n--- FOLD {fold + 1}/{n_folds} (Testing: {test_night}) ---\")\n",
    "\n",
    "            X_train, X_test = X_engineered.iloc[train_idx].values, X_engineered.iloc[test_idx].values\n",
    "            y_train, y_test = y_encoded[train_idx], y_encoded[test_idx]\n",
    "\n",
    "            scaler = RobustScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            if use_optimization and self.best_params is None:\n",
    "                self.best_params = self.optimize_hyperparameters(X_train_scaled, y_train, n_trials=50)\n",
    "\n",
    "            params = self.best_params or {}\n",
    "            params.update({\n",
    "                'objective': 'MultiClass', 'random_seed': self.random_state, 'verbose': 0\n",
    "            })\n",
    "            \n",
    "            X_train_aug, y_train_aug = X_train_scaled, y_train\n",
    "            missing_classes = set(range(self.n_outputs)) - set(np.unique(y_train_aug))\n",
    "            if missing_classes:\n",
    "                print(f\"  - Injecting dummy samples for missing classes: {missing_classes}\")\n",
    "                dummy_sample_X = X_train_aug[0:1]\n",
    "                for mc in missing_classes:\n",
    "                    X_train_aug = np.vstack([X_train_aug, dummy_sample_X])\n",
    "                    y_train_aug = np.hstack([y_train_aug, [mc]])\n",
    "\n",
    "            X_train_resampled, y_train_resampled = self.robust_resampling(X_train_aug, y_train_aug)\n",
    "            \n",
    "            model = CatBoostClassifier(**params)\n",
    "            model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "            y_pred_encoded = model.predict(X_test_scaled).flatten()\n",
    "            all_preds_encoded.extend(y_pred_encoded)\n",
    "            all_true_encoded.extend(y_test)\n",
    "            \n",
    "            print(f\"  - Fold balanced accuracy: {balanced_accuracy_score(y_test, y_pred_encoded):.4f}\")\n",
    "            \n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': X_engineered.columns,\n",
    "                'importance': model.get_feature_importance()\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            all_importances.append(importance_df)\n",
    "\n",
    "        self.model = model\n",
    "        self._display_results(le, all_true_encoded, all_preds_encoded, all_importances)\n",
    "\n",
    "    def _display_results(self, le, true_encoded, pred_encoded, importances):\n",
    "        print(\"\\n\" + \"=\" * 60 + \"\\nAGGREGATED CATBOOST RESULTS\\n\" + \"=\" * 60)\n",
    "        \n",
    "        # This assumes your global LABEL_TO_EVENT_GROUP_NAME is available\n",
    "        class_names_ordered = [LABEL_TO_EVENT_GROUP_NAME[c] for c in le.classes_]\n",
    "        \n",
    "        print(f\"Overall Balanced Accuracy: {balanced_accuracy_score(true_encoded, pred_encoded):.4f}\\n\")\n",
    "        print(classification_report(true_encoded, pred_encoded, target_names=class_names_ordered, labels=le.classes_, zero_division=0))\n",
    "        \n",
    "        cm = confusion_matrix(true_encoded, pred_encoded, labels=le.classes_)\n",
    "        cm_norm = np.nan_to_num(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis])\n",
    "        \n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(cm_norm, annot=True, fmt='.2%', cmap='Blues', xticklabels=class_names_ordered, yticklabels=class_names_ordered)\n",
    "        plt.title('Aggregated Normalized Confusion Matrix (LONO)')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        self._plot_feature_importance(importances)\n",
    "        \n",
    "    def _plot_feature_importance(self, importances):\n",
    "        all_imp_df = pd.concat(importances)\n",
    "        mean_imp_df = all_imp_df.groupby('feature')['importance'].mean().sort_values(ascending=False).reset_index()\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.barplot(x='importance', y='feature', data=mean_imp_df.head(15), palette='viridis')\n",
    "        plt.xlabel('Mean Feature Importance (PredictionValueChange)')\n",
    "        plt.ylabel('Feature')\n",
    "        plt.title('Top 15 Feature Importances (Averaged Across Folds)')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8ea2c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data preparation complete.\n",
      "Shape of X (features): (7796, 10) -> (Num_Windows, Num_Engineered_Features)\n",
      "Shape of y (labels):   (7796,)\n",
      "Original class distribution: Counter({np.int64(0): 7082, np.int64(4): 480, np.int64(2): 207, np.int64(1): 26, np.int64(3): 1})\n",
      "Encoded class distribution: Counter({np.int64(0): 7082, np.int64(4): 480, np.int64(2): 207, np.int64(1): 26, np.int64(3): 1})\n",
      "Total number of classes detected: 5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# --- Define data for the model ---\n",
    "df = final_df.copy()\n",
    "\n",
    "FEATURE_COLUMNS = [\n",
    "    'BR_median', 'BR_mean', 'BR_std', 'BR_cov', \n",
    "    'AL_median', 'AL_mean', 'AL_std', 'AL_cov',\n",
    "    'RRV', 'RRV3ANN'\n",
    "]\n",
    "LABEL_COLUMN = 'Label' \n",
    "SESSION_ID_COLUMN = 'SessionID'\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# --- Create final X, y, and groups arrays ---\n",
    "X = df[FEATURE_COLUMNS].values\n",
    "y = df[LABEL_COLUMN].values\n",
    "groups = df[SESSION_ID_COLUMN].values\n",
    "\n",
    "# --- Impute any remaining NaNs ---\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# --- NEW: Initialize and fit LabelEncoder on the entire dataset's labels ---\n",
    "# This makes the encoder aware of ALL possible classes (0 through 5)\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "# N_OUTPUTS should now be based on the number of classes found by the encoder\n",
    "N_OUTPUTS = len(le.classes_) \n",
    "# Update CLASS_NAMES to match the order found by the encoder\n",
    "CLASS_NAMES = [LABEL_TO_EVENT_GROUP_NAME[c] for c in le.classes_]\n",
    "\n",
    "\n",
    "print(\"\\nData preparation complete.\")\n",
    "print(f\"Shape of X (features): {X.shape} -> (Num_Windows, Num_Engineered_Features)\")\n",
    "print(f\"Shape of y (labels):   {y_encoded.shape}\")\n",
    "print(f\"Original class distribution: {Counter(y)}\")\n",
    "print(f\"Encoded class distribution: {Counter(y_encoded)}\")\n",
    "print(f\"Total number of classes detected: {N_OUTPUTS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4e53241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for and imputing missing values (NaNs)...\n",
      "\n",
      "Imputation complete. No NaNs remain in feature columns.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nChecking for and imputing missing values (NaNs)...\")\n",
    "for col in df:\n",
    "    if col in df.columns:\n",
    "        nan_count = df[col].isnull().sum()\n",
    "        if nan_count > 0:\n",
    "            print(f\"  - Found {nan_count} NaNs in '{col}'. Applying forward-fill and backward-fill.\")\n",
    "            \n",
    "            # Step 1: Forward-fill handles all NaNs except leading ones.\n",
    "            df[col].ffill(inplace=True) \n",
    "            \n",
    "            # Step 2: Backward-fill handles any remaining NaNs at the beginning of the file.\n",
    "            df[col].bfill(inplace=True) \n",
    "\n",
    "# Add a final check to ensure everything is clean\n",
    "final_nan_count = df[FEATURE_COLUMNS].isnull().sum().sum()\n",
    "if final_nan_count > 0:\n",
    "    print(f\"\\nWARNING: {final_nan_count} NaNs still remain in feature columns after imputation. Please investigate.\")\n",
    "else:\n",
    "    print(\"\\nImputation complete. No NaNs remain in feature columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be772b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\n",
    "#     \"cuda\" if torch.cuda.is_available()\n",
    "#     else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# )\n",
    "# print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b7b3e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-14 23:22:32,222] A new study created in memory with name: no-name-8e7da3d0-adb1-4c5e-ab0f-3a07e4e9df67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Feature engineering applied: 10 -> 18 features\n",
      "Starting LONO CV with 9 folds...\n",
      "============================================================\n",
      "\n",
      "--- FOLD 1/9 (Testing: 04-04-2025) ---\n",
      "  - Optimizing CatBoost hyperparameters with Optuna...\n",
      "  - Original distribution: Counter({np.int64(0): 6043, np.int64(4): 414, np.int64(2): 201, np.int64(1): 24, np.int64(3): 1})\n",
      "  - Using RandomOverSampler due to small class size.\n",
      "  - After resampling: Counter({np.int64(0): 6043, np.int64(2): 6043, np.int64(4): 6043, np.int64(1): 6043, np.int64(3): 6043})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-14 23:22:46,581] Trial 0 finished with value: 0.9723976991497892 and parameters: {'iterations': 400, 'depth': 6, 'learning_rate': 0.09809543099062999, 'l2_leaf_reg': 0.8589837300232939, 'border_count': 170, 'subsample': 0.7496568878171402}. Best is trial 0 with value: 0.9723976991497892.\n",
      "[I 2025-07-14 23:23:04,058] Trial 1 finished with value: 0.9662417338350324 and parameters: {'iterations': 800, 'depth': 5, 'learning_rate': 0.05706347680830313, 'l2_leaf_reg': 3.039128554536146e-08, 'border_count': 69, 'subsample': 0.931144558706718}. Best is trial 0 with value: 0.9723976991497892.\n",
      "[I 2025-07-14 23:25:41,371] Trial 2 finished with value: 0.9920568099400146 and parameters: {'iterations': 1200, 'depth': 9, 'learning_rate': 0.041183756645177316, 'l2_leaf_reg': 0.07743788103129064, 'border_count': 242, 'subsample': 0.670105231227473}. Best is trial 2 with value: 0.9920568099400146.\n",
      "[I 2025-07-14 23:27:47,532] Trial 3 finished with value: 0.9869269620514135 and parameters: {'iterations': 600, 'depth': 10, 'learning_rate': 0.07995300461387993, 'l2_leaf_reg': 3.21356072064765, 'border_count': 217, 'subsample': 0.9708050228860831}. Best is trial 2 with value: 0.9920568099400146.\n",
      "[I 2025-07-14 23:28:55,834] Trial 4 finished with value: 0.9912955712001104 and parameters: {'iterations': 1300, 'depth': 7, 'learning_rate': 0.07717804417429704, 'l2_leaf_reg': 0.01975380227902447, 'border_count': 234, 'subsample': 0.8377195612337608}. Best is trial 2 with value: 0.9920568099400146.\n",
      "[I 2025-07-14 23:32:15,913] Trial 5 finished with value: 0.9932151367228409 and parameters: {'iterations': 1500, 'depth': 10, 'learning_rate': 0.022162320612757847, 'l2_leaf_reg': 0.0009960092290274768, 'border_count': 99, 'subsample': 0.9074150454514244}. Best is trial 5 with value: 0.9932151367228409.\n",
      "[I 2025-07-14 23:32:33,026] Trial 6 finished with value: 0.9763358557919215 and parameters: {'iterations': 1500, 'depth': 4, 'learning_rate': 0.10467305965661147, 'l2_leaf_reg': 0.7292997715277668, 'border_count': 39, 'subsample': 0.6057289845643085}. Best is trial 5 with value: 0.9932151367228409.\n",
      "[I 2025-07-14 23:32:42,533] Trial 7 finished with value: 0.9399301826831699 and parameters: {'iterations': 500, 'depth': 5, 'learning_rate': 0.050127223761617816, 'l2_leaf_reg': 2.1295545294853737e-05, 'border_count': 141, 'subsample': 0.8985633671185813}. Best is trial 5 with value: 0.9932151367228409.\n",
      "[I 2025-07-14 23:33:14,918] Trial 8 finished with value: 0.9334434639902813 and parameters: {'iterations': 1300, 'depth': 6, 'learning_rate': 0.010886841739830557, 'l2_leaf_reg': 0.011479133314331693, 'border_count': 160, 'subsample': 0.8678921485107023}. Best is trial 5 with value: 0.9932151367228409.\n",
      "[I 2025-07-14 23:37:43,675] Trial 9 finished with value: 0.9948700042629633 and parameters: {'iterations': 1300, 'depth': 10, 'learning_rate': 0.038825044790237, 'l2_leaf_reg': 0.00038018750262745293, 'border_count': 172, 'subsample': 0.6488964430490158}. Best is trial 9 with value: 0.9948700042629633.\n",
      "[I 2025-07-14 23:38:54,402] Trial 10 finished with value: 0.994572122520356 and parameters: {'iterations': 900, 'depth': 8, 'learning_rate': 0.19597140086300807, 'l2_leaf_reg': 6.9929156657477725e-06, 'border_count': 199, 'subsample': 0.7478743131460237}. Best is trial 9 with value: 0.9948700042629633.\n",
      "[I 2025-07-14 23:40:17,262] Trial 11 finished with value: 0.9949362239345261 and parameters: {'iterations': 1000, 'depth': 8, 'learning_rate': 0.19719621758465986, 'l2_leaf_reg': 5.421998313707871e-06, 'border_count': 195, 'subsample': 0.7283713606022406}. Best is trial 11 with value: 0.9949362239345261.\n",
      "[I 2025-07-14 23:41:27,194] Trial 12 finished with value: 0.985238877896084 and parameters: {'iterations': 1000, 'depth': 8, 'learning_rate': 0.027427978952553195, 'l2_leaf_reg': 5.215552652889492e-07, 'border_count': 126, 'subsample': 0.674155975559255}. Best is trial 11 with value: 0.9949362239345261.\n",
      "[I 2025-07-14 23:41:54,379] Trial 13 finished with value: 0.9891443600339395 and parameters: {'iterations': 200, 'depth': 9, 'learning_rate': 0.18747623425935647, 'l2_leaf_reg': 0.00014291049365619204, 'border_count': 188, 'subsample': 0.6925508298558357}. Best is trial 11 with value: 0.9949362239345261.\n",
      "[I 2025-07-14 23:43:38,922] Trial 14 finished with value: 0.9907330079024989 and parameters: {'iterations': 1100, 'depth': 9, 'learning_rate': 0.028601945387482224, 'l2_leaf_reg': 1.0030734873366054e-06, 'border_count': 120, 'subsample': 0.7719975063845455}. Best is trial 11 with value: 0.9949362239345261.\n",
      "[I 2025-07-14 23:44:34,350] Trial 15 finished with value: 0.9636599715966727 and parameters: {'iterations': 700, 'depth': 8, 'learning_rate': 0.015577395346089522, 'l2_leaf_reg': 0.00023220447505914254, 'border_count': 191, 'subsample': 0.6008743932335545}. Best is trial 11 with value: 0.9949362239345261.\n",
      "[I 2025-07-14 23:47:59,226] Trial 16 finished with value: 0.9968558387400686 and parameters: {'iterations': 1000, 'depth': 10, 'learning_rate': 0.12909230027167093, 'l2_leaf_reg': 3.0820261544589465e-08, 'border_count': 161, 'subsample': 0.7112034647930345}. Best is trial 16 with value: 0.9968558387400686.\n",
      "[I 2025-07-14 23:48:43,359] Trial 17 finished with value: 0.9919574557912644 and parameters: {'iterations': 900, 'depth': 7, 'learning_rate': 0.13912923981590414, 'l2_leaf_reg': 3.9915732385821786e-08, 'border_count': 254, 'subsample': 0.8082134997120864}. Best is trial 16 with value: 0.9968558387400686.\n",
      "[I 2025-07-14 23:50:52,744] Trial 18 finished with value: 0.9959953444170377 and parameters: {'iterations': 1100, 'depth': 9, 'learning_rate': 0.15003411492072946, 'l2_leaf_reg': 3.130475045577562e-07, 'border_count': 209, 'subsample': 0.7170900100973329}. Best is trial 16 with value: 0.9968558387400686.\n",
      "[I 2025-07-14 23:53:08,840] Trial 19 finished with value: 0.9964918523191258 and parameters: {'iterations': 1100, 'depth': 9, 'learning_rate': 0.14519083149356302, 'l2_leaf_reg': 2.3726518494584303e-07, 'border_count': 224, 'subsample': 0.7132665454029103}. Best is trial 16 with value: 0.9968558387400686.\n",
      "[I 2025-07-14 23:55:48,976] Trial 20 finished with value: 0.9954658006033861 and parameters: {'iterations': 700, 'depth': 10, 'learning_rate': 0.12503065087633264, 'l2_leaf_reg': 1.023084322798688e-08, 'border_count': 226, 'subsample': 0.8149935257892674}. Best is trial 16 with value: 0.9968558387400686.\n",
      "[I 2025-07-14 23:57:58,939] Trial 21 finished with value: 0.9959290754626627 and parameters: {'iterations': 1100, 'depth': 9, 'learning_rate': 0.1434570439772743, 'l2_leaf_reg': 2.2584588020979774e-07, 'border_count': 216, 'subsample': 0.7079760752702186}. Best is trial 16 with value: 0.9968558387400686.\n",
      "[I 2025-07-14 23:59:28,803] Trial 22 finished with value: 0.9947707158246287 and parameters: {'iterations': 1100, 'depth': 9, 'learning_rate': 0.07284658351663113, 'l2_leaf_reg': 1.4306957830254728e-07, 'border_count': 152, 'subsample': 0.7808979403349896}. Best is trial 16 with value: 0.9968558387400686.\n",
      "[I 2025-07-15 00:02:29,889] Trial 23 finished with value: 0.9961277016221438 and parameters: {'iterations': 1200, 'depth': 10, 'learning_rate': 0.150487887544354, 'l2_leaf_reg': 2.0330553035194296e-06, 'border_count': 218, 'subsample': 0.6313526362383474}. Best is trial 16 with value: 0.9968558387400686.\n",
      "[I 2025-07-15 00:06:28,376] Trial 24 finished with value: 0.9963263113540206 and parameters: {'iterations': 1400, 'depth': 10, 'learning_rate': 0.10383293067175019, 'l2_leaf_reg': 2.3008669851280983e-06, 'border_count': 253, 'subsample': 0.643448114333327}. Best is trial 16 with value: 0.9968558387400686.\n",
      "[I 2025-07-15 00:10:21,122] Trial 25 finished with value: 0.9956974955296382 and parameters: {'iterations': 1400, 'depth': 10, 'learning_rate': 0.09808316633316227, 'l2_leaf_reg': 2.720861538662136e-05, 'border_count': 248, 'subsample': 0.6469539229271749}. Best is trial 16 with value: 0.9968558387400686.\n",
      "[I 2025-07-15 00:12:11,682] Trial 26 finished with value: 0.9962931604492292 and parameters: {'iterations': 1400, 'depth': 10, 'learning_rate': 0.06576629995572765, 'l2_leaf_reg': 4.586935089062784e-08, 'border_count': 95, 'subsample': 0.6730733224039622}. Best is trial 16 with value: 0.9968558387400686.\n",
      "[I 2025-07-15 00:13:48,951] Trial 27 finished with value: 0.9954988693701573 and parameters: {'iterations': 1000, 'depth': 9, 'learning_rate': 0.11120043411504463, 'l2_leaf_reg': 1.2302674095838059e-08, 'border_count': 255, 'subsample': 0.741688580073945}. Best is trial 16 with value: 0.9968558387400686.\n",
      "[I 2025-07-15 00:14:12,004] Trial 28 finished with value: 0.9895414645044655 and parameters: {'iterations': 800, 'depth': 7, 'learning_rate': 0.09135127583989117, 'l2_leaf_reg': 2.600252864430316e-06, 'border_count': 180, 'subsample': 0.6955986883133434}. Best is trial 16 with value: 0.9968558387400686.\n",
      "[I 2025-07-15 00:15:18,297] Trial 29 finished with value: 0.9947706993970248 and parameters: {'iterations': 1200, 'depth': 8, 'learning_rate': 0.12528594837981905, 'l2_leaf_reg': 9.31440536929927e-08, 'border_count': 234, 'subsample': 0.765876344920306}. Best is trial 16 with value: 0.9968558387400686.\n",
      "[I 2025-07-15 00:15:50,854] Trial 30 finished with value: 0.9938109823460755 and parameters: {'iterations': 300, 'depth': 10, 'learning_rate': 0.16755562839289997, 'l2_leaf_reg': 3.313204499555805e-05, 'border_count': 137, 'subsample': 0.6282998487108966}. Best is trial 16 with value: 0.9968558387400686.\n",
      "[I 2025-07-15 00:17:27,785] Trial 31 finished with value: 0.9963262620712087 and parameters: {'iterations': 1400, 'depth': 10, 'learning_rate': 0.060031047239353756, 'l2_leaf_reg': 5.4669654132609415e-08, 'border_count': 96, 'subsample': 0.6744045485683147}. Best is trial 16 with value: 0.9968558387400686.\n",
      "[I 2025-07-15 00:18:50,459] Trial 32 finished with value: 0.9965579241422532 and parameters: {'iterations': 1400, 'depth': 10, 'learning_rate': 0.05978908850925179, 'l2_leaf_reg': 6.49617021396891e-07, 'border_count': 80, 'subsample': 0.6608157913025546}. Best is trial 16 with value: 0.9968558387400686.\n",
      "[I 2025-07-15 00:19:30,676] Trial 33 finished with value: 0.9950685811396323 and parameters: {'iterations': 1500, 'depth': 9, 'learning_rate': 0.04945795073733453, 'l2_leaf_reg': 5.690212644712107e-07, 'border_count': 44, 'subsample': 0.654965948839809}. Best is trial 16 with value: 0.9968558387400686.\n",
      "[I 2025-07-15 00:20:41,660] Trial 34 finished with value: 0.9968227535456929 and parameters: {'iterations': 1300, 'depth': 10, 'learning_rate': 0.08115173727828193, 'l2_leaf_reg': 1.4432189224734296e-06, 'border_count': 67, 'subsample': 0.6998225563092326}. Best is trial 16 with value: 0.9968558387400686.\n",
      "[I 2025-07-15 00:21:47,727] Trial 35 finished with value: 0.9962931768768333 and parameters: {'iterations': 1200, 'depth': 10, 'learning_rate': 0.08172598474587144, 'l2_leaf_reg': 2.4129609979677965e-07, 'border_count': 69, 'subsample': 0.721774899427247}. Best is trial 16 with value: 0.9968558387400686.\n",
      "[I 2025-07-15 00:22:33,638] Trial 36 finished with value: 0.9953995973594271 and parameters: {'iterations': 1300, 'depth': 9, 'learning_rate': 0.06635281902472207, 'l2_leaf_reg': 9.61501603203225e-06, 'border_count': 73, 'subsample': 0.6918117259486605}. Best is trial 16 with value: 0.9968558387400686.\n",
      "[I 2025-07-15 00:22:47,216] Trial 37 finished with value: 0.9770972259526571 and parameters: {'iterations': 1200, 'depth': 6, 'learning_rate': 0.03551184927758926, 'l2_leaf_reg': 1.0147458232575134e-06, 'border_count': 55, 'subsample': 0.9934586377514671}. Best is trial 16 with value: 0.9968558387400686.\n",
      "[I 2025-07-15 00:23:44,708] Trial 38 finished with value: 0.9961607539613113 and parameters: {'iterations': 900, 'depth': 10, 'learning_rate': 0.08960826177779412, 'l2_leaf_reg': 2.0284237017679938e-08, 'border_count': 82, 'subsample': 0.7449418173499093}. Best is trial 16 with value: 0.9968558387400686.\n",
      "[I 2025-07-15 00:24:30,883] Trial 39 finished with value: 0.994241139155769 and parameters: {'iterations': 1000, 'depth': 9, 'learning_rate': 0.11846337829104636, 'l2_leaf_reg': 0.002749347390868833, 'border_count': 111, 'subsample': 0.8342552252509408}. Best is trial 16 with value: 0.9968558387400686.\n",
      "[I 2025-07-15 00:25:34,160] Trial 40 finished with value: 0.9959622263674545 and parameters: {'iterations': 1300, 'depth': 10, 'learning_rate': 0.07958857278641228, 'l2_leaf_reg': 4.847468011577316e-05, 'border_count': 54, 'subsample': 0.7837005868119415}. Best is trial 16 with value: 0.9968558387400686.\n",
      "[I 2025-07-15 00:28:44,905] Trial 41 finished with value: 0.9963594294036041 and parameters: {'iterations': 1400, 'depth': 10, 'learning_rate': 0.10447669622791463, 'l2_leaf_reg': 1.9092483286167347e-06, 'border_count': 237, 'subsample': 0.6251783311661492}. Best is trial 16 with value: 0.9968558387400686.\n",
      "[I 2025-07-15 00:30:13,218] Trial 42 finished with value: 0.9964586028487107 and parameters: {'iterations': 1500, 'depth': 10, 'learning_rate': 0.05549661453541478, 'l2_leaf_reg': 1.0061153078026731e-07, 'border_count': 82, 'subsample': 0.6215279150788098}. Best is trial 16 with value: 0.9968558387400686.\n",
      "[I 2025-07-15 00:30:23,778] Trial 43 finished with value: 0.9614096855510188 and parameters: {'iterations': 1500, 'depth': 4, 'learning_rate': 0.047208982746666885, 'l2_leaf_reg': 8.13547426211182e-08, 'border_count': 83, 'subsample': 0.6171682218187854}. Best is trial 16 with value: 0.9968558387400686.\n",
      "[I 2025-07-15 00:31:28,643] Trial 44 finished with value: 0.9960283803286014 and parameters: {'iterations': 1300, 'depth': 10, 'learning_rate': 0.05415539516788266, 'l2_leaf_reg': 1.0323345035427832e-07, 'border_count': 61, 'subsample': 0.6677759634649305}. Best is trial 16 with value: 0.9968558387400686.\n",
      "[I 2025-07-15 00:31:42,288] Trial 45 finished with value: 0.9758395778762887 and parameters: {'iterations': 1500, 'depth': 5, 'learning_rate': 0.04360440775782828, 'l2_leaf_reg': 4.7674818171865033e-07, 'border_count': 111, 'subsample': 0.69358486769594}. Best is trial 16 with value: 0.9968558387400686.\n",
      "[I 2025-07-15 00:32:12,946] Trial 46 finished with value: 0.9925201012268955 and parameters: {'iterations': 1200, 'depth': 9, 'learning_rate': 0.05815617237154031, 'l2_leaf_reg': 0.1458459133304352, 'border_count': 33, 'subsample': 0.7055017528922379}. Best is trial 16 with value: 0.9968558387400686.\n",
      "[I 2025-07-15 00:33:36,152] Trial 47 finished with value: 0.9931821172388812 and parameters: {'iterations': 1400, 'depth': 9, 'learning_rate': 0.03553019093385171, 'l2_leaf_reg': 1.8013338836023785e-08, 'border_count': 162, 'subsample': 0.7354781391219859}. Best is trial 16 with value: 0.9968558387400686.\n",
      "[I 2025-07-15 00:35:05,801] Trial 48 finished with value: 0.9963593472655843 and parameters: {'iterations': 1500, 'depth': 10, 'learning_rate': 0.0697824690906429, 'l2_leaf_reg': 9.695360151885087e-06, 'border_count': 79, 'subsample': 0.7607755206473149}. Best is trial 16 with value: 0.9968558387400686.\n",
      "[I 2025-07-15 00:35:44,810] Trial 49 finished with value: 0.995531970992137 and parameters: {'iterations': 1300, 'depth': 8, 'learning_rate': 0.16867386292291914, 'l2_leaf_reg': 3.142574149746122e-08, 'border_count': 129, 'subsample': 0.6627127933927278}. Best is trial 16 with value: 0.9968558387400686.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Best balanced accuracy from optimization: 0.9969\n",
      "  - Original distribution: Counter({np.int64(0): 6043, np.int64(4): 414, np.int64(2): 201, np.int64(1): 24, np.int64(3): 1})\n",
      "  - Using RandomOverSampler due to small class size.\n",
      "  - After resampling: Counter({np.int64(0): 6043, np.int64(2): 6043, np.int64(4): 6043, np.int64(1): 6043, np.int64(3): 6043})\n"
     ]
    },
    {
     "ename": "CatBoostError",
     "evalue": "catboost/private/libs/options/catboost_options.cpp:794: Error: default bootstrap type (bayesian) doesn't support 'subsample' option",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCatBoostError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m improved_classifier\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Execute the pipeline\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# The base FEATURE_COLUMNS from the data prep cell is passed here\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m improved_model = \u001b[43mrun_improved_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_encoded\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFEATURE_COLUMNS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_OUTPUTS\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mrun_improved_model\u001b[39m\u001b[34m(X, y_encoded, le, groups, feature_columns, n_outputs)\u001b[39m\n\u001b[32m      3\u001b[39m improved_classifier =  ImprovedCatBoostClassifier(\n\u001b[32m      4\u001b[39m     feature_columns=feature_columns,\n\u001b[32m      5\u001b[39m     n_outputs=n_outputs,\n\u001b[32m      6\u001b[39m     random_state=\u001b[32m42\u001b[39m\n\u001b[32m      7\u001b[39m )\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Run the full cross-validation and training pipeline\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Set use_optimization=False for much faster runs during debugging.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mimproved_classifier\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_encoded\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_encoded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_optimization\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Set to False for speed, True for best performance\u001b[39;49;00m\n\u001b[32m     17\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m improved_classifier\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 136\u001b[39m, in \u001b[36mImprovedCatBoostClassifier.cross_validate\u001b[39m\u001b[34m(self, X, y_encoded, le, groups, use_optimization)\u001b[39m\n\u001b[32m    133\u001b[39m X_train_resampled, y_train_resampled = \u001b[38;5;28mself\u001b[39m.robust_resampling(X_train_aug, y_train_aug)\n\u001b[32m    135\u001b[39m model = CatBoostClassifier(**params)\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_resampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_resampled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m y_pred_encoded = model.predict(X_test_scaled).flatten()\n\u001b[32m    139\u001b[39m all_preds_encoded.extend(y_pred_encoded)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/diss/lib/python3.11/site-packages/catboost/core.py:5245\u001b[39m, in \u001b[36mCatBoostClassifier.fit\u001b[39m\u001b[34m(self, X, y, cat_features, text_features, embedding_features, graph, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[39m\n\u001b[32m   5242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mloss_function\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[32m   5243\u001b[39m     CatBoostClassifier._check_is_compatible_loss(params[\u001b[33m'\u001b[39m\u001b[33mloss_function\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m5245\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_best_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5246\u001b[39m \u001b[43m          \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5247\u001b[39m \u001b[43m          \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_snapshot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cerr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5248\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/diss/lib/python3.11/site-packages/catboost/core.py:2395\u001b[39m, in \u001b[36mCatBoost._fit\u001b[39m\u001b[34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[39m\n\u001b[32m   2392\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, PATH_TYPES + (Pool,)):\n\u001b[32m   2393\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\u001b[33m\"\u001b[39m\u001b[33my may be None only when X is an instance of catboost.Pool or string\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2395\u001b[39m train_params = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_train_params\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2396\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2398\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubgroup_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubgroup_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpairs_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_best_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_best_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2399\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplot_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumn_description\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumn_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2401\u001b[39m \u001b[43m    \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m=\u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_snapshot\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_snapshot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2402\u001b[39m \u001b[43m    \u001b[49m\u001b[43msnapshot_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43msnapshot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\n\u001b[32m   2404\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2405\u001b[39m params = train_params[\u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   2406\u001b[39m train_pool = train_params[\u001b[33m\"\u001b[39m\u001b[33mtrain_pool\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/diss/lib/python3.11/site-packages/catboost/core.py:2321\u001b[39m, in \u001b[36mCatBoost._prepare_train_params\u001b[39m\u001b[34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks)\u001b[39m\n\u001b[32m   2319\u001b[39m _check_param_types(params)\n\u001b[32m   2320\u001b[39m params = _params_type_cast(params)\n\u001b[32m-> \u001b[39m\u001b[32m2321\u001b[39m \u001b[43m_check_train_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2323\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m params.get(\u001b[33m'\u001b[39m\u001b[33meval_fraction\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0.0\u001b[39m) != \u001b[32m0.0\u001b[39m:\n\u001b[32m   2324\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m eval_set \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_catboost.pyx:6601\u001b[39m, in \u001b[36m_catboost._check_train_params\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_catboost.pyx:6623\u001b[39m, in \u001b[36m_catboost._check_train_params\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mCatBoostError\u001b[39m: catboost/private/libs/options/catboost_options.cpp:794: Error: default bootstrap type (bayesian) doesn't support 'subsample' option"
     ]
    }
   ],
   "source": [
    "# --- DRIVER SCRIPT to run the improved XGBoost model ---\n",
    "def run_improved_model(X, y_encoded, le, groups, feature_columns, n_outputs):\n",
    "    improved_classifier =  ImprovedCatBoostClassifier(\n",
    "        feature_columns=feature_columns,\n",
    "        n_outputs=n_outputs,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Run the full cross-validation and training pipeline\n",
    "    # Set use_optimization=False for much faster runs during debugging.\n",
    "    improved_classifier.cross_validate(\n",
    "        X=X, \n",
    "        y_encoded=y_encoded,\n",
    "        le=le,\n",
    "        groups=groups,\n",
    "        use_optimization=True # Set to False for speed, True for best performance\n",
    "    )\n",
    "    \n",
    "    return improved_classifier\n",
    "\n",
    "# Execute the pipeline\n",
    "# The base FEATURE_COLUMNS from the data prep cell is passed here\n",
    "improved_model = run_improved_model(\n",
    "    X=X, \n",
    "    y_encoded=y_encoded, \n",
    "    le=le,\n",
    "    groups=groups, \n",
    "    feature_columns=FEATURE_COLUMNS,\n",
    "    n_outputs=N_OUTPUTS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcc7432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xgboost as xgb\n",
    "# from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "# from sklearn.model_selection import LeaveOneGroupOut\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# from collections import Counter\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.inspection import permutation_importance\n",
    "\n",
    "# # --- 1. Initialize lists to store results from all folds ---\n",
    "# all_fold_predictions = []\n",
    "# all_fold_true_labels = []\n",
    "# all_fold_importances = []\n",
    "\n",
    "# # --- 2. Setup Leave-One-Night-Out cross-validator ---\n",
    "# logo = LeaveOneGroupOut()\n",
    "# n_folds = logo.get_n_splits(X, y_encoded, groups=groups)\n",
    "# print(f\"Starting XGBoost Leave-One-Night-Out cross-validation with {n_folds} folds...\")\n",
    "# print(\"----------------------------------------------------\\n\")\n",
    "\n",
    "# # --- 3. Loop through each fold ---\n",
    "# for fold, (train_idx, test_idx) in enumerate(logo.split(X, y_encoded, groups)):\n",
    "    \n",
    "#     test_night = np.unique(groups[test_idx])[0]\n",
    "#     print(f\"--- FOLD {fold + 1}/{n_folds} (Testing on Night: {test_night}) ---\")\n",
    "\n",
    "#     X_train, X_test = X[train_idx], X[test_idx]\n",
    "#     y_train_fold_encoded, y_test_fold = y_encoded[train_idx], y_encoded[test_idx]\n",
    "    \n",
    "#     # --- FINAL FIX: Robust Resampling with Dummy Sample Injection ---\n",
    "#     print(f\"  - Original training distribution (encoded): {Counter(y_train_fold_encoded)}\")\n",
    "    \n",
    "#     # Find classes missing from this fold's training data\n",
    "#     all_possible_classes = set(range(N_OUTPUTS))\n",
    "#     present_classes = set(np.unique(y_train_fold_encoded))\n",
    "#     missing_classes = all_possible_classes - present_classes\n",
    "    \n",
    "#     X_train_augmented = X_train\n",
    "#     y_train_augmented = y_train_fold_encoded\n",
    "    \n",
    "#     # If any classes are missing, inject a dummy sample for each\n",
    "#     if missing_classes:\n",
    "#         print(f\"  - Injecting dummy samples for missing classes: {missing_classes}\")\n",
    "#         dummy_sample_X = X_train[0:1] # Take the first sample as a template\n",
    "#         for mc in missing_classes:\n",
    "#             X_train_augmented = np.vstack([X_train_augmented, dummy_sample_X])\n",
    "#             y_train_augmented = np.hstack([y_train_augmented, [mc]])\n",
    "\n",
    "#     # Now, use RandomOverSampler which is safe and guaranteed to work\n",
    "#     ros = RandomOverSampler(random_state=RANDOM_STATE)\n",
    "#     X_res, y_res = ros.fit_resample(X_train_augmented, y_train_augmented)\n",
    "    \n",
    "#     print(f\"  - Resampled training distribution: {Counter(y_res)}\")\n",
    "\n",
    "#     # --- Initialize and Train the XGBoost model ---\n",
    "#     model = xgb.XGBClassifier(\n",
    "#         objective='multi:softmax',\n",
    "#         num_class=N_OUTPUTS,\n",
    "#         n_estimators=500,\n",
    "#         learning_rate=0.0001,\n",
    "#         max_depth=4,\n",
    "#         eval_metric='mlogloss',\n",
    "#         random_state=RANDOM_STATE,\n",
    "#         n_jobs=-1\n",
    "#     )\n",
    "    \n",
    "#     model.fit(X_res, y_res)\n",
    "#     print(f\"  - Training complete.\")\n",
    "\n",
    "#     # --- Evaluate and store results ---\n",
    "#     fold_preds_encoded = model.predict(X_test)\n",
    "    \n",
    "#     # Inverse transform predictions and true labels to their original values for reporting\n",
    "#     fold_preds_original = le.inverse_transform(fold_preds_encoded)\n",
    "#     fold_true_original = le.inverse_transform(y_test_fold)\n",
    "\n",
    "#     all_fold_predictions.extend(fold_preds_original)\n",
    "#     all_fold_true_labels.extend(fold_true_original)\n",
    "    \n",
    "#     # --- Calculate and store feature importance ---\n",
    "#     result = permutation_importance(\n",
    "#         model, X_test, y_test_fold, n_repeats=10, random_state=RANDOM_STATE, n_jobs=-1, scoring='f1_weighted'\n",
    "#     )\n",
    "#     perm_importance = pd.DataFrame({'feature': FEATURE_COLUMNS, 'importance': result.importances_mean})\n",
    "#     all_fold_importances.append(perm_importance)\n",
    "    \n",
    "#     print(f\"  - Evaluation complete for fold {fold + 1}.\\n\")\n",
    "\n",
    "# # --- FINAL AGGREGATED EVALUATION (after all folds are complete) ---\n",
    "# # This part of the code remains the same as the previous correct version\n",
    "# print(\"\\n====================================================\")\n",
    "# print(\"XGBoost Leave-One-Night-Out Cross-Validation Complete.\")\n",
    "# print(\"Aggregated Results Across All Folds:\")\n",
    "# print(\"====================================================\\n\")\n",
    "\n",
    "# print('Aggregated Classification Report')\n",
    "# print('------------------------------')\n",
    "# report_labels = le.classes_\n",
    "# print(classification_report(\n",
    "#     all_fold_true_labels, \n",
    "#     all_fold_predictions, \n",
    "#     labels=report_labels,\n",
    "#     target_names=CLASS_NAMES,\n",
    "#     zero_division=0\n",
    "# ))\n",
    "\n",
    "# print('\\nAggregated Confusion Matrix')\n",
    "# print('---------------------------')\n",
    "# cm = confusion_matrix(all_fold_true_labels, all_fold_predictions, labels=report_labels)\n",
    "# with np.errstate(divide='ignore', invalid='ignore'):\n",
    "#     cm_norm = np.where(cm.sum(axis=1)[:, np.newaxis] > 0, cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], 0)\n",
    "\n",
    "# plt.figure(figsize=(12, 10))\n",
    "# sns.heatmap(\n",
    "#     cm_norm, annot=True, fmt='.2%', cmap='Greens',\n",
    "#     xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES\n",
    "# )\n",
    "# plt.title('XGBoost - Aggregated Normalized Confusion Matrix (LONO)')\n",
    "# plt.ylabel('True Label')\n",
    "# plt.xlabel('Predicted Label')\n",
    "# plt.xticks(rotation=45, ha=\"right\")\n",
    "# plt.show()\n",
    "\n",
    "# # --- SHAP Value Analysis ---\n",
    "# print(\"\\n--- SHAP Value Analysis (from last fold) ---\")\n",
    "# explainer = shap.TreeExplainer(model)\n",
    "# shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# for i, class_label in enumerate(le.classes_):\n",
    "#     class_name = LABEL_TO_EVENT_GROUP_NAME[class_label]\n",
    "#     print(f\"\\nSHAP Summary Plot for: {class_name} (Encoded as {i})\")\n",
    "#     try:\n",
    "#         shap.summary_plot(shap_values[i], X_test, feature_names=FEATURE_COLUMNS, show=False)\n",
    "#         plt.title(f\"SHAP Values for {class_name}\")\n",
    "#         plt.show()\n",
    "#     except Exception as e:\n",
    "#         print(f\"Could not plot SHAP for class {i}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d882df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- AGGREGATE AND PLOT FEATURE IMPORTANCES ---\n",
    "# print(\"\\n====================================================\")\n",
    "# print(\"Feature Importance Analysis (Averaged Across All Folds)\")\n",
    "# print(\"====================================================\\n\")\n",
    "\n",
    "# # --- FIX: Aggregate Data from All Folds ---\n",
    "# all_importances_df = pd.concat(all_fold_importances)\n",
    "# mean_importance = all_importances_df.groupby('feature')['importance'].mean()\n",
    "# std_importance = all_importances_df.groupby('feature')['importance'].std()\n",
    "\n",
    "# # --- Prepare the DataFrame for Plotting ---\n",
    "# final_importance_df = pd.DataFrame({\n",
    "#     'mean_importance': mean_importance,\n",
    "#     'std_importance': std_importance\n",
    "# }).sort_values(by='mean_importance', ascending=False)\n",
    "\n",
    "# # --- Create the Plot ---\n",
    "# fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# plot_data = final_importance_df.head(10) # Plot top 10 features\n",
    "\n",
    "# ax.barh(\n",
    "#     y=plot_data.index,\n",
    "#     width=plot_data['mean_importance'],\n",
    "#     xerr=plot_data['std_importance'],\n",
    "#     align='center',\n",
    "#     ecolor='black',\n",
    "#     capsize=5\n",
    "# )\n",
    "\n",
    "# ax.invert_yaxis() \n",
    "# ax.set_xlabel('Mean Permutation Importance (Weighted F1)')\n",
    "# ax.set_ylabel('Feature')\n",
    "# ax.set_title('Top 10 Feature Importances (Averaged Across LONO Folds)')\n",
    "# ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293ec763",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
